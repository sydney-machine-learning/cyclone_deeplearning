{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from numpy import hstack\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import datetime\n",
    "import time\n",
    "import joblib\n",
    "from datetime import timedelta, date\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from numpy import array\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns; sns.set_theme()\n",
    "import errno\n",
    "\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import ConvLSTM2D\n",
    "from keras.models import load_model\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc, classification_report, confusion_matrix\n",
    "from scipy import interp\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import imblearn\n",
    "import collections\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_data_south_indian(url):\n",
    "    \"\"\"\n",
    "    Load and preprocess data for South Indian region.\n",
    "\n",
    "    Parameters:\n",
    "    - url (str): URL or file path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Processed DataFrame with a 'category' column.\n",
    "    \"\"\"\n",
    "    # Load data from CSV\n",
    "    df = pd.read_csv(url)\n",
    "\n",
    "    # Add a 'category' column based on speed ranges\n",
    "    df['category'] = df['Speed(knots)'].apply(lambda x:\n",
    "        0 if x <= 33 else\n",
    "        1 if 34 <= x <= 47 else\n",
    "        2 if 48 <= x <= 63 else\n",
    "        3 if 64 <= x <= 89 else\n",
    "        4 if 90 <= x <= 115 else\n",
    "        5\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "def load_data_south_pacific(url):\n",
    "    \"\"\"\n",
    "    Load and preprocess data for South Pacific region.\n",
    "\n",
    "    Parameters:\n",
    "    - url (str): URL or file path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Processed DataFrame with a 'category' column.\n",
    "    \"\"\"\n",
    "    # Load data from CSV\n",
    "    df = pd.read_csv(url)\n",
    "\n",
    "    # Add a 'category' column based on speed ranges\n",
    "    df['category'] = df['Speed(knots)'].apply(lambda x:\n",
    "        0 if x <= 33 else\n",
    "        1 if 34 <= x <= 47 else\n",
    "        2 if 48 <= x <= 63 else\n",
    "        3 if 64 <= x <= 85 else\n",
    "        4 if 86 <= x <= 107 else\n",
    "        5\n",
    "    )\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocean = 'south_indian'\n",
    "\n",
    "# Display the selected ocean\n",
    "print(f\"Selected ocean: {ocean}\")\n",
    "\n",
    "# Set the data URL and function based on the selected ocean\n",
    "if ocean == 'south_indian':\n",
    "    url_data = 'https://raw.githubusercontent.com/sydney-machine-learning/cyclonedatasets/main/SouthIndian-SouthPacific-Ocean/South_indian_hurricane.csv'\n",
    "    data_loading_function = load_data_south_indian\n",
    "    hot_encoded_result_file_name = 'south_indian'\n",
    "    category_result_file_name = 'roc_data_south_indian'\n",
    "else:\n",
    "    url_data = 'https://raw.githubusercontent.com/sydney-machine-learning/cyclonedatasets/main/SouthIndian-SouthPacific-Ocean/South_pacific_hurricane.csv'\n",
    "    data_loading_function = load_data_south_pacific\n",
    "    hot_encoded_result_file_name = 'south_pacific'\n",
    "    category_result_file_name = 'roc_data_south_pacific'\n",
    "\n",
    "# Display the data URL for verification\n",
    "print(f\"Data URL: {url_data}\")\n",
    "\n",
    "# Display the result file names\n",
    "print(f\"Hot-encoded result file name: {hot_encoded_result_file_name}\")\n",
    "print(f\"Category result file name: {category_result_file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data using the specified function and URL\n",
    "df = data_loading_function(url_data)\n",
    "\n",
    "# Latitude and longitude values into int\n",
    "df['Lat'] = df['Lat'].apply(lambda x: -int(x[:-1]) * 0.1 if x.endswith('N') else int(x[:-1]) * 0.1 if x.endswith('S') else 0)\n",
    "df['Lon'] = df['Lon'].apply(lambda x: -int(x[:-1]) * 0.1 if x.endswith('W') else int(x[:-1]) * 0.1 if x.endswith('E') else 0)\n",
    "\n",
    "# Extract 'Speed(knots)' and 'category' columns as lists\n",
    "speed = df['Speed(knots)'].tolist()\n",
    "categories = df['category'].tolist()\n",
    "latitude = df['Lat'].tolist()\n",
    "longitude = df['Lon'].tolist()\n",
    "\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df_head = df.head()\n",
    "print(\"DataFrame Head:\")\n",
    "print(df_head)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def split_sequence(sequences, n_steps_in, n_steps_out):\n",
    "    \"\"\"\n",
    "    Split multivariate sequences into input and output parts.\n",
    "\n",
    "    Parameters:\n",
    "    - sequences (numpy.ndarray): Multivariate time series data.\n",
    "    - n_steps_in (int): Number of input time steps.\n",
    "    - n_steps_out (int): Number of output time steps.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Input sequences (X) and output sequences (y) as numpy arrays.\n",
    "    \"\"\"\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out\n",
    "        if out_end_ix > len(sequences):\n",
    "            break\n",
    "        seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix:out_end_ix, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def uni_split_sequence(sequence, n_steps):\n",
    "    \"\"\"\n",
    "    Split univariate sequence into input and output parts.\n",
    "\n",
    "    Parameters:\n",
    "    - sequence (list): Univariate time series data.\n",
    "    - n_steps (int): Number of input time steps.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Input sequences (X) and output sequences (y) as numpy arrays.\n",
    "    \"\"\"\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        end_ix = i + n_steps\n",
    "        if end_ix > len(sequence) - 1:\n",
    "            break\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "def multivariate_split_sequence(sequence, n_steps):\n",
    "    \"\"\"\n",
    "    Split multivariate sequence into input and output parts.\n",
    "\n",
    "    Parameters:\n",
    "    - sequence (numpy.ndarray): Multivariate time series data.\n",
    "    - n_steps (int): Number of input time steps.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Input sequences (X) and output sequences (y) as numpy arrays.\n",
    "    \"\"\"\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        end_ix = i + n_steps\n",
    "        if end_ix > len(sequence) - 1:\n",
    "            break\n",
    "        seq_x, seq_y = sequence[i:end_ix, :], sequence[end_ix, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Example usage:\n",
    "# Assuming df is your DataFrame with 'Latitude', 'Longitude', 'Speed(knots)', and 'category' columns\n",
    "features_columns = ['Lat', 'Lon']\n",
    "target_column = 'category'\n",
    "\n",
    "# Extract features and target\n",
    "features = df[features_columns].values\n",
    "target = df[target_column].values\n",
    "\n",
    "n_steps = 5  # Number of input time steps\n",
    "\n",
    "X, y = multivariate_split_sequence(features, n_steps)\n",
    "\n",
    "# Now X contains sequences of 'Latitude', 'Longitude', and 'Speed(knots)' for each time step,\n",
    "# and y contains the corresponding 'category' values.\n",
    "\n",
    "def rmse(pred, actual):\n",
    "    \"\"\"\n",
    "    Calculate the Root Mean Squared Error (RMSE) between two arrays.\n",
    "\n",
    "    Parameters:\n",
    "    - pred (numpy.ndarray): Predicted values.\n",
    "    - actual (numpy.ndarray): Actual values.\n",
    "\n",
    "    Returns:\n",
    "    - float: Root Mean Squared Error.\n",
    "    \"\"\"\n",
    "    return np.sqrt(((pred - actual) ** 2).mean())\n",
    "\n",
    "def categorical(pred, actual):\n",
    "    \"\"\"\n",
    "    Compute classification metrics for categorical values.\n",
    "\n",
    "    Parameters:\n",
    "    - pred (numpy.ndarray): Predicted categorical values.\n",
    "    - actual (numpy.ndarray): Actual categorical values.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Accuracy, AUC, Confusion Matrix, Precision, Recall, F1 Score.\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(pred, actual)\n",
    "    acc = accuracy_score(actual, pred, normalize=True, sample_weight=None)\n",
    "    ps1 = precision_score(pred, actual, average='micro')\n",
    "    rs1 = recall_score(pred, actual, average='micro')\n",
    "    f11 = f1_score(pred, actual, average='micro')\n",
    "    auc = roc_auc_score(actual, pred)\n",
    "    return acc, auc, cm, ps1, rs1, f11\n",
    "\n",
    "def make_confusion_matrix_chart(cf_matrix_test, cmap='Blues', annot_kws=None):\n",
    "    \"\"\"\n",
    "    Generate and display a heatmap-style confusion matrix chart.\n",
    "\n",
    "    Parameters:\n",
    "    - cf_matrix_test (numpy.ndarray): Confusion matrix.\n",
    "    - cmap (str): Colormap for the heatmap.\n",
    "    - annot_kws (dict): Additional keyword arguments for annotation customization.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Set up the figure and axes\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Customize the heatmap using seaborn\n",
    "    sns.heatmap(cf_matrix_test, annot=True, cmap=cmap,\n",
    "                yticklabels=['0', '1'], xticklabels=['0', '1'],\n",
    "                fmt='g', annot_kws=annot_kws)\n",
    "\n",
    "    # Customize axis labels and title\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.title('Confusion Matrix - Test Data')\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define whether it's a univariate or multivariate case\n",
    "univariate_1 = False  # If True, it's a multivariate case\n",
    "\n",
    "# Define sequence and time step parameters\n",
    "n_steps_in_1 = 8 # Adjust based on the number of time steps you want to consider for input\n",
    "n_seq_1= 2  # Adjust based on your specific needs\n",
    "n_steps_out_1 = 1  # Number of output time steps\n",
    "\n",
    "# Define the number of features for input and output\n",
    "n_features_in_1 = 2  # Latitude, Longitude\n",
    "n_features_out_1 = 2  # 'category'\n",
    "\n",
    "# Define whether it's a univariate or multivariate case\n",
    "univariate_2 = True  # If not True, it's a multivariate case\n",
    "\n",
    "# Define sequence and time step parameters\n",
    "n_steps_in_2  = 8  # Adjust based on the number of time steps you want to consider for input\n",
    "n_seq_2 = 2  # Adjust based on your specific needs\n",
    "n_steps_out_2 = 1  # Number of output time steps\n",
    "\n",
    "# Define the number of features for input and output\n",
    "n_features_in_2 = 1  # Latitude, Longitude, Speed\n",
    "n_features_out_2 = 2  # 'category'\n",
    "\n",
    "# Define the number of hidden layers in the model\n",
    "hidden_layers = 50\n",
    "\n",
    "# Define training parameters\n",
    "epochs = 100\n",
    "No_exp = 10 # Number of experiments\n",
    "\n",
    "# Display the configuration\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize variables\n",
    "cyclone_id = df['No. of Cycl'][0]\n",
    "X1= []\n",
    "Y1 = []\n",
    "start_index = 0\n",
    "end_index = 0\n",
    "\n",
    "# Iterate through the DataFrame\n",
    "for i in range(1, df.shape[0]):\n",
    "    # Check if the cyclone ID is the same as the previous row\n",
    "    if df['No. of Cycl'][i] == cyclone_id:\n",
    "        end_index += 1\n",
    "    else:\n",
    "        # Split the sequence and append to X and Y\n",
    "        x, y = multivariate_split_sequence(features[start_index:end_index + 1, :], n_steps_in_1)\n",
    "        X1.append(x)\n",
    "        Y1.append(y)\n",
    "\n",
    "        # Update start and end indices for the new cyclone\n",
    "        cyclone_id = df['No. of Cycl'][i]\n",
    "        start_index = i\n",
    "        end_index = i\n",
    "\n",
    "    # Check if it's the last row of the DataFrame\n",
    "    if i == df.shape[0] - 1:\n",
    "        # Split the sequence and append to X and Y\n",
    "        x, y = multivariate_split_sequence(features[start_index:end_index + 1, :], n_steps_in_1)\n",
    "        X1.append(x)\n",
    "        Y1.append(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the initial lengths of X and Y\n",
    "print(\"Initial Lengths - X:\", len(X1), \"Y:\", len(Y1))\n",
    "\n",
    "# Flattening X and Y\n",
    "X1 = [item for sublist in X1 for item in sublist]\n",
    "Y1 = [item for sublist in Y1 for item in sublist]\n",
    "\n",
    "# Print lengths of X and Y after flattening\n",
    "print(\"Flattened Lengths - X:\", len(X1), \"Y:\", len(Y1))\n",
    "\n",
    "# Print some initial values from X and Y\n",
    "print(Y1[0])\n",
    "\n",
    "# Print some initial values from the 'speed' column\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize variables\n",
    "cyclone_id = df['No. of Cycl'][0]\n",
    "X = []\n",
    "Y = []\n",
    "start_index = 0\n",
    "end_index = 0\n",
    "\n",
    "# Iterate through the DataFrame\n",
    "for i in range(1, df.shape[0]):\n",
    "    # Check if the cyclone ID is the same as the previous row\n",
    "    if df['No. of Cycl'][i] == cyclone_id:\n",
    "        end_index += 1\n",
    "    else:\n",
    "        # Split the sequence and append to X and Y\n",
    "        x, y = uni_split_sequence(speed[start_index:end_index + 1], n_steps_in_1)\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "\n",
    "        # Update start and end indices for the new cyclone\n",
    "        cyclone_id = df['No. of Cycl'][i]\n",
    "        start_index = i\n",
    "        end_index = i\n",
    "\n",
    "    # Check if it's the last row of the DataFrame\n",
    "    if i == df.shape[0] - 1:\n",
    "        # Split the sequence and append to X and Y\n",
    "        x, y = uni_split_sequence(speed[start_index:end_index + 1], n_steps_in_1)\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the initial lengths of X and Y\n",
    "print(len(X), len(Y))\n",
    "# Flattening X and Y\n",
    "X = [item for sublist in X for item in sublist]\n",
    "Y = [item for sublist in Y for item in sublist]\n",
    "# Print lengths of X and Y after flattening\n",
    "print(len(X), len(Y))\n",
    "print(type(X), Y[0])\n",
    "print(speed[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating a list for RI cyclone classification\n",
    "intensify_y = []\n",
    "for i in range(len(X)):\n",
    "  intensify_y.append(0)\n",
    "\n",
    "\n",
    "    # Iterate through each column in X[i]\n",
    "for i in range(len(X)):\n",
    "   for j in range(len(X[0])):\n",
    "        # Iterate through k from 1 to 4\n",
    "        for k in range(1, 5):\n",
    "            # Check if the difference is greater than 30\n",
    "            if (j - k) >= 0 and (X[i][j] - X[i][j-k]) >= 30:\n",
    "                # Set Y[i] to 1 and break the loop\n",
    "                intensify_y[i] = 1\n",
    "                break\n",
    "\n",
    "\n",
    "print(len(intensify_y))\n",
    "\n",
    "# Use 'intensify_y' as the updated target variable\n",
    "Y = intensify_y\n",
    "Y1 = Y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the training limit as 75% of the total length of X\n",
    "train_limit = int(len(X) * 0.75)\n",
    "\n",
    "# Display the calculated training limit\n",
    "print(\"Training set size:\", train_limit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X1[0], X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract test data for evaluation\n",
    "test_X_original_1 = X1[train_limit + 1:]\n",
    "test_Y_original_1 = Y1[train_limit + 1:]\n",
    "\n",
    "# Display the lengths of the datasets\n",
    "print(\"Length of X:\", len(X1))\n",
    "print(\"Length of Y:\", len(Y1))\n",
    "print(\"Length of Test X (for evaluation):\", (test_X_original_1[0]))\n",
    "print(\"Length of Test Y (for evaluation):\", (test_Y_original_1[0]))\n",
    "\n",
    "# Extract test data for evaluation\n",
    "test_X_original = X[train_limit + 1:]\n",
    "test_Y_original = Y[train_limit + 1:]\n",
    "\n",
    "# Display the lengths of the datasets\n",
    "print(\"Length of X:\", len(X))\n",
    "print(\"Length of Y:\", len(Y))\n",
    "print(\"Length of Test X (for evaluation):\", test_X_original)\n",
    "print(\"Length of Test Y (for evaluation):\", test_Y_original)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X1 = np.array(X1)\n",
    "scalers = {}\n",
    "for i in range(X1.shape[1]):\n",
    "    scalers[i] = MinMaxScaler()\n",
    "    X1[:, i, :] = scalers[i].fit_transform(X1[:, i, :])\n",
    "print(X1[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the sequences in X using Min-Max scaling\n",
    "X = MinMaxScaler().fit_transform(np.asarray(X))\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_x = X1\n",
    "test_X_1 = X1[train_limit+1:]\n",
    "test_X_1 = np.asarray(test_X_1).astype(float)\n",
    "test_Y_1 = Y1[train_limit+1:]\n",
    "X1 = X1[:train_limit]\n",
    "X1 = np.asarray(X1).astype(float)\n",
    "Y1 = Y1[:train_limit]\n",
    "print(test_Y_1)\n",
    "c = 0\n",
    "for i in test_Y_1:\n",
    "  if(i==1):\n",
    "    c = c+1\n",
    "\n",
    "print(c)\n",
    "\n",
    "\n",
    "len(X1), len(Y1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Similarly, you can do the same for the training set (X and Y) before splitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_x = X\n",
    "test_X = X[train_limit+1:]\n",
    "test_X = np.asarray(test_X).astype(float)\n",
    "test_Y = Y[train_limit+1:]\n",
    "X = X[:train_limit]\n",
    "X = np.asarray(X).astype(float)\n",
    "Y = Y[:train_limit]\n",
    "print(test_Y)\n",
    "c = 0\n",
    "for i in test_Y:\n",
    "  if(i==1):\n",
    "    c = c+1\n",
    "\n",
    "print(c)\n",
    "len(X), len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "for i in Y1:\n",
    "  if(i==1):\n",
    "    c = c+1\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Calculate the class distribution for training and test data\n",
    "counter_train_1 = Counter(Y1)\n",
    "counter_test_1 = Counter(test_Y_1)\n",
    "\n",
    "# Display the class distribution for training data\n",
    "print(\"Class Distribution - Training Data:\")\n",
    "print(counter_train_1)\n",
    "\n",
    "# Display the class distribution for test data\n",
    "print(\"\\nClass Distribution - Test Data:\")\n",
    "print(counter_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming counter_train is already defined\n",
    "\n",
    "# Set\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "# Bar plot with red bars\n",
    "bars = plt.bar(range(len(counter_train)), list(counter_train.values()), color='lightcoral', edgecolor='black', align='center')\n",
    "\n",
    "# Customizing individual bars\n",
    "for bar in bars:\n",
    "    yvar = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, yvar + 50, round(yvar, 2), ha='center', va='bottom', fontsize=12, color='#2E4053')\n",
    "\n",
    "# X-axis and Y-axis labels\n",
    "plt.xlabel('Category', size=16, labelpad=15)\n",
    "plt.ylabel('Number of cyclones', size=16, labelpad=15)\n",
    "\n",
    "# Title\n",
    "plt.title('Majority and Minority class distribution in Training Data', size=20, pad=20)\n",
    "\n",
    "# Tick parameters with only '0' and '1' on the x-axis\n",
    "plt.xticks(range(2), ['0', '1'], fontsize=12)\n",
    "\n",
    "# Tick parameters for y-axis\n",
    "ax.tick_params(axis='y', which='major', labelsize=12)\n",
    "\n",
    "# Adding grid lines\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Save figure\n",
    "plt.savefig('class_distribution_red.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, concatenate\n",
    "\n",
    "def stacked_lstm(n_steps_in_1, n_features_in_1, n_steps_in_2, n_features_in_2):\n",
    "    input_1 = Input(shape=(n_steps_in_1, n_features_in_1))\n",
    "    lstm_1 = LSTM(50, activation='relu')(input_1)\n",
    "\n",
    "    input_2 = Input(shape=(n_steps_in_2, n_features_in_2))\n",
    "    lstm_2 = LSTM(50, activation='relu')(input_2)\n",
    "\n",
    "    concat = concatenate([lstm_1, lstm_2])\n",
    "    output = Dense(2, activation='softmax')(concat)\n",
    "\n",
    "    model = Model(inputs=[input_1, input_2], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import time\n",
    "\n",
    "def lstm(x_train, x_train_1, y_train, x_test_1, x_test, y_test, Num_Exp, n_steps_in_1, n_steps_in_2, n_steps_out_1, n_steps_out_2, n_features_in_1, n_features_in_2, n_features_out_1, n_features_out_2, Epochs, hidden_layers):\n",
    "    # Initialize arrays to store accuracy metrics\n",
    "    train_acc = np.zeros(Num_Exp)\n",
    "    test_acc = np.zeros(Num_Exp)\n",
    "\n",
    "    stacked_model = stacked_lstm(8, 2, 8, 1)\n",
    "    stacked_model.summary()\n",
    "    # Initialize arrays to store predictions and classification reports\n",
    "\n",
    "    y_predicttest_allruns = np.zeros([Num_Exp, x_test.shape[0], y_test.shape[1]])\n",
    "    Best_f1 = 0  # Initialize the best F1 score\n",
    "    Best_Predict_Test = 0\n",
    "\n",
    "    # Extract the actual classes from one-hot encoded vectors for both test and train sets\n",
    "    act_test = [y_test[i].argmax() for i in range(y_test.shape[0])]\n",
    "    act_train = [y_train[i].argmax() for i in range(y_train.shape[0])]\n",
    "\n",
    "    # Initialize dictionaries to store classification reports\n",
    "    Best_report_train = dict()\n",
    "    Best_report_test = dict()\n",
    "    all_report_train = dict()\n",
    "    all_report_test = dict()\n",
    "\n",
    "    # Loop through experiment runs\n",
    "    start_time = time.time()\n",
    "    for run in range(Num_Exp):\n",
    "        print(\"Experiment\", run + 1, \"in progress\")\n",
    "\n",
    "        # Fit the stacked model\n",
    "\n",
    "        stacked_model.fit([x_train_1, x_train], y_train, epochs=Epochs, batch_size=10, verbose=0, shuffle=False)\n",
    "        y_predicttrain = stacked_model.predict([x_train_1, x_train])\n",
    "        y_predicttest = stacked_model.predict([x_test_1, x_test])\n",
    "        print(y_predicttrain[0])\n",
    "        print(y_predicttest[0])\n",
    "\n",
    "        # Extract predicted classes from one-hot encoded vectors\n",
    "        pred_test = [y_predicttest[i].argmax() for i in range(y_predicttest.shape[0])]\n",
    "        pred_train = [y_predicttrain[i].argmax() for i in range(y_predicttrain.shape[0])]\n",
    "\n",
    "        c = 0\n",
    "        for i in pred_test:\n",
    "           if i==1:\n",
    "             c = c+1\n",
    "\n",
    "        print(c)\n",
    "        d = 0\n",
    "        for i in pred_train:\n",
    "           if i==1:\n",
    "             d = d+1\n",
    "\n",
    "        print(d)\n",
    "\n",
    "        # Generate classification reports\n",
    "        report_train = classification_report(act_train, pred_train, labels=[0, 1], output_dict=True)\n",
    "        report_test = classification_report(act_test, pred_test, labels=[0, 1], output_dict=True)\n",
    "\n",
    "        # Store classification reports in dictionaries\n",
    "        all_report_train[run] = report_train\n",
    "        all_report_test[run] = report_test\n",
    "\n",
    "        # Calculate F1-score for the test set\n",
    "        test_acc[run] = report_test['1']['f1-score']\n",
    "        print(\"train acc: \", report_train['1']['f1-score'])\n",
    "        print(\"test acc: \", test_acc[run])\n",
    "\n",
    "    # Update the best F1 score and associated predictions and reports\n",
    "    if test_acc[run] > Best_f1:\n",
    "        Best_f1 = test_acc[run]\n",
    "        Best_Predict_Test = y_predicttest\n",
    "        Best_report_train, Best_report_test = report_train, report_test\n",
    "\n",
    "    # Save the trained model (assuming you have the 'ocean' variable)\n",
    "    stacked_model.save(\"model_\" + ocean + \"_stacked_lstm.h5\")\n",
    "\n",
    "    # Calculate standard deviations of train and test accuracies\n",
    "    train_std = np.std(train_acc)\n",
    "    test_std = np.std(test_acc)\n",
    "\n",
    "    # Display experiment summary\n",
    "    print(\"Total time for\", Num_Exp, \"experiments\", time.time() - start_time)\n",
    "    print(\"F1 scores for test data: \", test_acc)\n",
    "    print(\"Mean: \", np.mean(test_acc), \"Std Dev: \", test_std)\n",
    "\n",
    "    # Return relevant information\n",
    "    return train_acc, test_acc, train_std, test_std, Best_Predict_Test, y_predicttrain, y_predicttest, all_report_train, all_report_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random permutation of indices for shuffling\n",
    "# idx = np.random.permutation(len(X_smote))\n",
    "idx = np.random.permutation(len(X))\n",
    "print(len(idx))\n",
    "\n",
    "# Initialize lists to store shuffled data\n",
    "x_shuffled = []\n",
    "y_shuffled = []\n",
    "\n",
    "# Iterate through the shuffled indices\n",
    "for i in idx:\n",
    "    # Append shuffled data to the lists\n",
    "    # x_shuffled.append(X_smote[i])\n",
    "    # y_shuffled.append(Y_smote[i])\n",
    "    x_shuffled.append(X[i])\n",
    "    y_shuffled.append(Y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random permutation of indices for shuffling\n",
    "# idx = np.random.permutation(len(X_smote))\n",
    "idx = np.random.permutation(len(X1))\n",
    "print(len(idx))\n",
    "\n",
    "# Initialize lists to store shuffled data\n",
    "x_shuffled_1 = []\n",
    "y_shuffled_1  = []\n",
    "\n",
    "# Iterate through the shuffled indices\n",
    "for i in idx:\n",
    "    # Append shuffled data to the lists\n",
    "    # x_shuffled.append(X_smote[i])\n",
    "    # y_shuffled.append(Y_smote[i])\n",
    "    x_shuffled_1.append(X1[i])\n",
    "    y_shuffled_1.append(Y1[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the shuffled labels to one-hot encoded format for training data\n",
    "Y_hot_encoded_train = np.asarray(to_categorical(y_shuffled))\n",
    "\n",
    "# Convert the test labels to one-hot encoded format\n",
    "Y_hot_encoded_test = np.asarray(to_categorical(test_Y))\n",
    "\n",
    "# Print the shapes of the one-hot encoded training and test labels\n",
    "print(Y_hot_encoded_train.shape, Y_hot_encoded_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the shuffled labels to one-hot encoded format for training data\n",
    "Y_hot_encoded_train_1 = np.asarray(to_categorical(y_shuffled_1))\n",
    "\n",
    "# Convert the test labels to one-hot encoded format\n",
    "Y_hot_encoded_test_1 = np.asarray(to_categorical(test_Y_1))\n",
    "\n",
    "# Print the shapes of the one-hot encoded training and test labels\n",
    "print(Y_hot_encoded_train_1.shape, Y_hot_encoded_test_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train = dict()\n",
    "actual_train = dict()\n",
    "predictions_test = dict()\n",
    "actual_test = dict()\n",
    "metrics_train = dict()\n",
    "metrics_test = dict()\n",
    "test_acc_all = dict()\n",
    "test_stddev = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over different values of n_steps_out (in this case, only 1)\n",
    "for j in range(1):\n",
    "    # Initialize dictionaries to store results for the ensemble model\n",
    "    predictions_train_per_step = dict()\n",
    "    actual_train_per_step = dict()\n",
    "    predictions_test_per_step = dict()\n",
    "    actual_test_per_step = dict()\n",
    "    metrics_train_per_step = dict()\n",
    "    metrics_test_per_step = dict()\n",
    "    test_acc_per_step = dict()\n",
    "    test_stddev_per_step = dict()\n",
    "    n_steps_out = j + 1\n",
    "\n",
    "    print('---------------------------------------------------------')\n",
    "    print('Number of steps out:', n_steps_out)\n",
    "\n",
    "    # Reshape data based on the ensemble model type\n",
    "    x_train_1, y_train_1 = np.asarray(x_shuffled_1), np.asarray(Y_hot_encoded_train_1)\n",
    "    x_test_1, y_test_1 = np.asarray(test_X_1), np.asarray(Y_hot_encoded_test_1)\n",
    "\n",
    "    x_train, y_train = np.asarray(x_shuffled), np.asarray(Y_hot_encoded_train)\n",
    "    x_test, y_test = np.asarray(test_X), np.asarray(Y_hot_encoded_test)\n",
    "\n",
    "    # Call ensemble_lstm to create the ensemble model\n",
    "\n",
    "\n",
    "    # Call the lstm function with the ensemble model and retrieve results\n",
    "    train_acc, test_acc, train_std_dev, test_std_dev, Best_Predict_Test, y_predicttrain, y_predicttest, report_train, report_test = lstm(x_train, x_train_1, y_train, x_test_1, x_test,  y_test, No_exp, n_steps_in_1, n_steps_in_2, n_steps_out_1, n_steps_out_2, n_features_in_1, n_features_in_2, n_features_out_1, n_features_out_2, epochs, hidden_layers)\n",
    "    # Store results in respective dictionaries\n",
    "    predictions_train_per_step['ensemble'] = Best_Predict_Test\n",
    "    actual_train_per_step['ensemble'] = y_train\n",
    "    predictions_test_per_step['ensemble'] = Best_Predict_Test\n",
    "    actual_test_per_step['ensemble'] = y_test\n",
    "    metrics_train_per_step['ensemble'] = report_train\n",
    "    metrics_test_per_step['ensemble'] = report_test\n",
    "    test_acc_per_step['ensemble'] = test_acc\n",
    "    test_stddev_per_step['ensemble'] = test_std_dev\n",
    "\n",
    "    # Store results for the current n_steps_out in the overall dictionaries\n",
    "    predictions_train[str(j + 1)] = predictions_train_per_step\n",
    "    actual_train[str(j + 1)] = actual_train_per_step\n",
    "    predictions_test[str(j + 1)] = predictions_test_per_step\n",
    "    actual_test[str(j + 1)] = actual_test_per_step\n",
    "    metrics_train[str(j + 1)] = metrics_train_per_step\n",
    "    metrics_test[str(j + 1)] = metrics_test_per_step\n",
    "    test_acc_all[str(j + 1)] = test_acc_per_step\n",
    "    test_stddev[str(j + 1)] = test_stddev_per_step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"predictions_\" + ocean + '_original' + '.pkl', 'wb') as f:\n",
    "    pickle.dump([predictions_train,actual_train,predictions_test,actual_test,metrics_train,metrics_test,test_acc,test_stddev], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def make_confusion_matrix_chart(cf_matrix, name):\n",
    "    \"\"\"\n",
    "    Create and save a well-formatted confusion matrix heatmap.\n",
    "\n",
    "    Parameters:\n",
    "    - cf_matrix: Confusion matrix\n",
    "    - name: Name of the file to save the plot\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "\n",
    "    # Set a Seaborn style with dark grid\n",
    "    sns.set(style=\"darkgrid\", font_scale=1.5)\n",
    "\n",
    "    # Create a figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "    # Customize the appearance of the heatmap with green colors\n",
    "    sns.heatmap(cf_matrix, annot=True, fmt='d', cmap='Greens',\n",
    "                linewidths=.5, square=True, cbar=False,\n",
    "                annot_kws={\"size\": 16}, ax=ax)\n",
    "\n",
    "    # Set labels and title\n",
    "    plt.ylabel(\"Actual\", size=18)\n",
    "    plt.xlabel(\"Predicted\", size=18)\n",
    "    plt.title(\"Confusion Matrix\", size=20)\n",
    "\n",
    "    # Set tick parameters\n",
    "    ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "    # Save the plot as an image\n",
    "    plt.savefig(name + '.png', dpi=300, transparent=False, bbox_inches='tight')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "    # Return None\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [i.argmax() for i in actual_test_per_step['ensemble']]\n",
    "pred = [i.argmax() for i in predictions_test_per_step['ensemble']]\n",
    "cf_matrix_test = confusion_matrix(y, pred)\n",
    "make_confusion_matrix_chart(cf_matrix_test, ocean + '_ensemble_cm_original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store precision, recall, and F1-score metrics for class 0, class 1, overall accuracy,\n",
    "# macro average, and weighted average across multiple experiments\n",
    "precision0 = []\n",
    "precision1 = []\n",
    "precisionacc = []\n",
    "precisionmacavg = []\n",
    "precisionweighavg = []\n",
    "recall0 = []\n",
    "recall1 = []\n",
    "recallacc = []\n",
    "recallmacavg = []\n",
    "recallweighavg = []\n",
    "f10 = []\n",
    "f11 = []\n",
    "f1acc = []\n",
    "f1macavg = []\n",
    "f1weighavg = []\n",
    "\n",
    "# Loop through the results of multiple experiments\n",
    "for i in range(No_exp):\n",
    "    # Append precision, recall, and F1-score metrics for class 0, class 1, overall accuracy,\n",
    "    # macro average, and weighted average from the 'vanilla' model results\n",
    "    precision0.append(metrics_test_per_step['ensemble'][i]['0']['precision'])\n",
    "    precision1.append(metrics_test_per_step['ensemble'][i]['1']['precision'])\n",
    "    precisionacc.append(metrics_test_per_step['ensemble'][i]['accuracy'])\n",
    "    precisionmacavg.append(metrics_test_per_step['ensemble'][i]['macro avg']['precision'])\n",
    "    precisionweighavg.append(metrics_test_per_step['ensemble'][i]['weighted avg']['precision'])\n",
    "\n",
    "    recall0.append(metrics_test_per_step['ensemble'][i]['0']['recall'])\n",
    "    recall1.append(metrics_test_per_step['ensemble'][i]['1']['recall'])\n",
    "    recallacc.append(metrics_test_per_step['ensemble'][i]['accuracy'])\n",
    "    recallmacavg.append(metrics_test_per_step['ensemble'][i]['macro avg']['recall'])\n",
    "    recallweighavg.append(metrics_test_per_step['ensemble'][i]['weighted avg']['recall'])\n",
    "\n",
    "    f10.append(metrics_test_per_step['ensemble'][i]['0']['f1-score'])\n",
    "    f11.append(metrics_test_per_step['ensemble'][i]['1']['f1-score'])\n",
    "    f1acc.append(metrics_test_per_step['ensemble'][i]['accuracy'])\n",
    "    f1macavg.append(metrics_test_per_step['ensemble'][i]['macro avg']['f1-score'])\n",
    "    f1weighavg.append(metrics_test_per_step['ensemble'][i]['weighted avg']['f1-score'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(round(np.mean(precision0),4)) + \"±\" + str(round(np.std(precision0),4)),\" & \" + str(round(np.mean(recall0),4)) + \"±\" + str(round(np.std(recall0),4)), \" & \" + str(round(np.mean(f10),4)) + \"±\" + str(round(np.std(f10),4)))\n",
    "print(str(round(np.mean(precision1),4)) + \"±\" + str(round(np.std(precision1),4)),\" & \" + str(round(np.mean(recall1),4)) + \"±\" + str(round(np.std(recall1),4)), \" & \" + str(round(np.mean(f11),4)) + \"±\" + str(round(np.std(f11),4)))\n",
    "print(str(round(np.mean(precisionacc),4)) + \"±\" + str(round(np.std(precisionacc),4)),\" & \" + str(round(np.mean(recallacc),4)) + \"±\" + str(round(np.std(recallacc),4)), \" & \" + str(round(np.mean(f1acc),4)) + \"±\" + str(round(np.std(f1acc),4)))\n",
    "print(str(round(np.mean(precisionmacavg),4)) + \"±\" + str(round(np.std(precisionmacavg),4)),\" & \" + str(round(np.mean(recallmacavg),4)) + \"±\" + str(round(np.std(recallmacavg),4)), \" & \" + str(round(np.mean(f1macavg),4)) + \"±\" + str(round(np.std(f1macavg),4)))\n",
    "print(str(round(np.mean(precisionweighavg),4)) + \"±\" + str(round(np.std(precisionweighavg),4)),\" & \" + str(round(np.mean(recallweighavg),4)) + \"±\" + str(round(np.std(recallweighavg),4)), \" & \" + str(round(np.mean(f1weighavg),4)) + \"±\" + str(round(np.std(f1weighavg),4)))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
