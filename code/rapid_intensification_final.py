# -*- coding: utf-8 -*-
"""ri_sp (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RsPzUeVzUIZeY_z07s-KdfdoyxHRSqJN
"""

import copy
import numpy as np
import sklearn
import pandas as pd
import matplotlib.pyplot as plt
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Activation
from keras.layers import LSTM
from keras.layers import Bidirectional
from keras.layers import RepeatVector
from keras.layers import TimeDistributed
from numpy import hstack
from sklearn.preprocessing import StandardScaler
import datetime
import time
import joblib
from datetime import timedelta, date
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler
from numpy import array
from tensorflow.keras.utils import to_categorical
from sklearn.metrics import precision_score, recall_score
from sklearn.metrics import f1_score
import matplotlib.pyplot as plt
import os
import seaborn as sns; sns.set_theme() 
import errno
from keras.layers.convolutional import Conv1D
from keras.layers.convolutional import MaxPooling1D
from keras.layers import Flatten
from keras.layers import ConvLSTM2D
from keras.models import load_model
import pickle
from sklearn.metrics import accuracy_score, roc_curve, auc, classification_report, confusion_matrix
from scipy import interp
from imblearn.over_sampling import SMOTE
from collections import Counter
import imblearn
import collections
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import f1_score
from matplotlib import pyplot

def load_data_south_indian(url):
  df = pd.read_csv(url)
  #df.columns = ['id','date','longitude','latitude','speed']
  #df = df.drop(['date'], axis = 1)
  df['category'] = df['Speed(knots)'].apply(lambda x: 
  0 if x<=33 else 1  if x<=47 and x>=34 else 2 if x<=63 and x>=48 else 3 if x<=89 and x>=64 else 4 if x<=115 and x>=90 else 5 )
  return df

def load_data_south_pacific(url):
  df = pd.read_csv(url)
  #df.columns = ['id','date','longitude','latitude','speed']
  #df = df.drop(['date'], axis = 1)
  df['category'] = df['Speed(knots)'].apply(lambda x: 
  0 if x<=33 else 1  if x<=47 and x>=34 else 2 if x<=63 and x>=48 else 3 if x<=85 and x>=64 else 4 if x<=107 and x>=86 else 5 )
  return df

ocean = 'south_pacific'  #south_indian or south_pacific
print(ocean)

if ocean == 'south_indian':
    url_data = 'https://raw.githubusercontent.com/sydney-machine-learning/cyclonedatasets/main/SouthIndian-SouthPacific-Ocean/South_indian_hurricane.csv'
    function = load_data_south_indian
    hot_encoded_result_file_name = 'south_indian'
    category_result_file_name = 'roc_data_south_indian'
else:
    url_data = 'https://raw.githubusercontent.com/sydney-machine-learning/cyclonedatasets/main/SouthIndian-SouthPacific-Ocean/South_pacific_hurricane.csv'
    function = load_data_south_pacific
    hot_encoded_result_file_name = 'south_pacific'
    category_result_file_name = 'roc_data_south_pacific'

df = function(url_data)
speed = df['Speed(knots)'].tolist()
categories = df['category'].tolist()
df.head()

def split_sequence(sequences, n_steps_in, n_steps_out):
	X, y = list(), list()
	for i in range(len(sequences)):
		# find the end of this pattern
		end_ix = i + n_steps_in
		out_end_ix = end_ix + n_steps_out
		# check if we are beyond the dataset
		if out_end_ix > len(sequences):
			break
		# gather input and output parts of the pattern
		seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix:out_end_ix, -1 ]
		X.append(seq_x)
		y.append(seq_y)
	return array(X), array(y)
 
# split a univariate sequence into samples
def uni_split_sequence(sequence, n_steps):
	X, y = list(), list()
	for i in range(len(sequence)):
		# find the end of this pattern
		end_ix = i + n_steps
		# check if we are beyond the sequence
		if end_ix > len(sequence)-1:
			break
		# gather input and output parts of the pattern
		seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]
		X.append(seq_x)
		y.append(seq_y)
	return array(X), array(y) 
 
def rmse(pred, actual):
    return np.sqrt(((pred-actual) ** 2).mean())

def categorical(pred, actual):
  cm = confusion_matrix(pred,actual)
  ps = precision_score(pred,actual,average='micro')
  rs = recall_score(pred,actual,average='micro')
  f1 = f1_score(pred,actual, average = 'micro')
  return cm,ps,rs,f1

def make_confusion_matrix_chart(cf_matrix_test):
    plt.figure(1, figsize=(10,5))
    sns.heatmap(cf_matrix_test, annot=True, yticklabels=['0','1'], 
                                xticklabels=['0','1'], fmt='g')
    plt.ylabel("Actual")
    plt.xlabel("Pred")
    plt.title('Test data')
    return None

univariate = True # if false, its multivariate case
n_steps_in = 4
n_seq = 2
n_steps_out = 1
n_features_in = 1 #speed
n_features_out = 2 # one hot encoding of category
Hidden = 10
Epochs = 100
Num_Exp = 30

id = df['No. of Cycl'][0]
count = 0
X = []
Y = []
start_index=0
end_index=0
for i in range(1, df.shape[0]):
  if df['No. of Cycl'][i] == id :
    end_index+=1
  else:
    x,y = uni_split_sequence(speed[start_index:end_index+1], n_steps_in)
    X.append(x)
    Y.append(y)
    id = df['No. of Cycl'][i]
    start_index=i
    end_index=i
  if i == df.shape[0]-1:   
    x,y = uni_split_sequence(speed[start_index:end_index+1], n_steps_in)
    X.append(x)
    Y.append(y)

print(len(X), len(Y))
X = [item for sublist in X for item in sublist]
Y = [item for sublist in Y for item in sublist]
print(len(X), len(Y))
print(X[0], Y[0], X[1], Y[1])
print(speed[:10])

intensify_y = []
for i in range(len(X)):
  if Y[i]-X[i][0]>=30:
    intensify_y.append(1)
  else:
    intensify_y.append(0)
print(len(intensify_y))
speed_y = Y
Y=intensify_y

train_limit = int(len(X)*70/100)
train_limit

test_X_original = X[train_limit+1:]
#X_original = X[:train_limit]
#X_original = np.asarray(X_original).astype(float)
test_Y_original = Y[train_limit+1:]
#Y_original = Y[:train_limit]
len(X), len(Y), len(test_X_original), len(test_Y_original)

X = MinMaxScaler().fit_transform(np.asarray(X))

speed_x = X
test_X = X[train_limit+1:]
test_X = np.asarray(test_X).astype(float)
test_Y = Y[train_limit+1:]
X = X[:train_limit]
X = np.asarray(X).astype(float)
Y = Y[:train_limit]
print(len(test_X), len(test_Y))
len(X), len(Y)

counter_train = Counter(Y)
counter_test = Counter(test_Y)
#counter = collections.OrderedDict(sorted(counter.items()))
print("train data: ", counter_train)
print("test data: ", counter_test)

plt.figure()
#fig = plt.subplots(figsize =(20, 12))
fig, ax = plt.subplots(figsize = (12,8))
plt.bar(range(len(counter_train)), list(counter_train.values()), align='center')
plt.xticks(range(len(counter_train)), list(counter_train.keys()))
plt.xlabel('Category', size=40)
plt.ylabel('Number', size=40)
#plt.title('ROC' + ' (' + str(no_of_output_steps) + ' steps ahead, ' + model.capitalize() + ', Time step: ' + str(time_step) + ') - ' + train_or_test.capitalize() + ' (' + ocean + '_ocean)')
ax.tick_params(axis='both', which='major', labelsize=30)
plt.savefig(ocean + '_class_dist.png', dpi=300, transparent=False, bbox_inches='tight')
plt.show()

def vanilla(n_steps_in,n_steps_out,n_features_in, n_features_out):
  model = Sequential()
  model.add(LSTM(50, activation='relu', input_shape=(n_steps_in, n_features_in)))
  model.add(Dense(n_features_out, activation = "softmax"))
  model.compile(optimizer='adam', loss='binary_crossentropy')
  return model
 
  #model = Sequential()
  #model.add(LSTM(Hidden, activation='relu', input_shape=(n_steps_in, n_features_in)))
  #model.add(RepeatVector(n_steps_out))
  #model.add(LSTM(Hidden, activation='relu', return_sequences=True))
  #model.add(TimeDistributed(Dense(n_features_out, activation = "softmax")))
  #model.compile(optimizer='adam', loss='categorical_crossentropy')
  #return model

def bidirectional(n_steps_in,n_steps_out,n_features_in, n_features_out):
  model = Sequential()
  model.add(Bidirectional(LSTM(Hidden, activation='relu', input_shape=(n_steps_in, n_features_in))))
  model.add(Dense(n_features_out, activation = "softmax"))
  model.compile(optimizer='adam', loss='binary_crossentropy')
  return model

def cnn_lstm(n_steps_in,n_steps_out,n_features_in, n_features_out, n_seq):
  model = Sequential()
  model.add(TimeDistributed(Conv1D(filters=64, kernel_size=1, activation='relu'), input_shape=(None, int(n_steps_in/n_seq), n_features_in)))
  model.add(TimeDistributed(MaxPooling1D(pool_size=2)))
  model.add(TimeDistributed(Flatten()))
  model.add(LSTM(Hidden, activation='relu'))
  model.add(Dense(n_features_out, activation = "softmax"))
  model.compile(optimizer='adam', loss='binary_crossentropy')
  return model

def conv_lstm(n_steps_in,n_steps_out,n_features_in, n_features_out, n_seq):
  model = Sequential()
  model.add(ConvLSTM2D(filters=64, kernel_size=(1,2), activation='relu', input_shape=(n_seq, 1, int(n_steps_in/n_seq), n_features_in)))
  model.add(Flatten())
  model.add(Dense(n_features_out, activation = "softmax"))
  model.compile(optimizer='adam', loss='binary_crossentropy')
  return model

#all models
def MODEL_LSTM(model_name, method, univariate, x_train, x_test, y_train, y_test, Num_Exp, n_steps_in, n_steps_out, Epochs, Hidden):

    train_acc = np.zeros(Num_Exp)
    test_acc = np.zeros(Num_Exp)

    if model_name == 'vanilla':
      model = vanilla(n_steps_in,n_steps_out,n_features_in, n_features_out)
    elif model_name == 'bidirectional':
      model = bidirectional(n_steps_in,n_steps_out,n_features_in, n_features_out)
    elif model_name == 'cnn-lstm':
      model = cnn_lstm(n_steps_in,n_steps_out,n_features_in,n_features_out,n_seq)
    elif model_name == 'conv-lstm':
      model = conv_lstm(n_steps_in,n_steps_out,n_features_in,n_features_out,n_seq)
    
    model.summary()

    y_predicttest_allruns = np.zeros([Num_Exp, x_test.shape[0], y_test.shape[1]])

    #print(y_predicttest_allruns.shape, ' shape ')


    Best_f1 = 0  # Assigning a small number
    act_test = [y_test[i].argmax() for i in range(y_test.shape[0])]
    act_train = [y_train[i].argmax() for i in range(y_train.shape[0])]
    start_time = time.time()
    Best_report_train = dict()
    Best_report_test = dict()
    all_report_train=dict()
    all_report_test=dict()
    for run in range(Num_Exp):
        print("Experiment", run + 1, "in progress")
        # fit model
        model.fit(x_train, y_train, epochs=Epochs, batch_size=10, verbose=0, shuffle=False)
        #scores = model.predict_proba(x_test)
        y_predicttrain = model.predict(x_train)
        y_predicttest = model.predict(x_test)
        #y_predicttest_allruns[run,:,:] = y_predicttest
        #train_acc[run] = rmse(y_predicttrain, y_train)
        #print(train_acc[run], 'train accuracy')
        #test_acc[run] = rmse(y_predicttest, y_test)
        pred_test = [y_predicttest[i].argmax() for i in range(y_predicttest.shape[0])]
        pred_train = [y_predicttrain[i].argmax() for i in range(y_predicttrain.shape[0])]
        report_train = classification_report(act_train, pred_train, labels=[0,1], output_dict=True)
        report_test = classification_report(act_test, pred_test, labels=[0,1], output_dict=True)
        #test_acc[run] = f1_score(pred,act, average = 'binary')
        all_report_train[run] = report_train
        all_report_test[run] = report_test
        test_acc[run] = report_test['1']['f1-score']
        print("train acc: ", report_train['1']['f1-score'])        
        print("test acc: ", test_acc[run])
        if test_acc[run] > Best_f1:
            Best_f1 = test_acc[run]
            Best_Predict_Test = y_predicttest
            Best_report_train, Best_report_test = report_train, report_test
    model.save("model_" + ocean+"_"+model_name+"_"+method+'.h5')  
    train_std = np.std(train_acc)
    test_std = np.std(test_acc)
    print("Total time for", Num_Exp, "experiments", time.time() - start_time)
    print("f1 scores for test data: ", test_acc)
    print("mean: ", np.mean(test_acc), "std dev: ", test_std)
    return train_acc, test_acc, train_std, test_std, Best_Predict_Test, y_predicttrain, y_predicttest, all_report_train, all_report_test

#idx = np.random.permutation(len(X_smote))
idx = np.random.permutation(len(X))
print(len(idx))
x_shuffled = []
y_shuffled = []
for i in idx:
  #x_shuffled.append(X_smote[i])
  #y_shuffled.append(Y_smote[i])
  x_shuffled.append(X[i])
  y_shuffled.append(Y[i])

Y_hot_encoded_train =  np.asarray(to_categorical(y_shuffled))
#Y_hot_encoded_train = Y_hot_encoded_train.reshape(len(y_shuffled), n_steps_out, n_features_out)

Y_hot_encoded_test =  np.asarray(to_categorical(test_Y))
#Y_hot_encoded_test = Y_hot_encoded_test.reshape(len(test_Y), n_steps_out, n_features_out)

print(Y_hot_encoded_train.shape, Y_hot_encoded_test.shape)

x_shuffled[0], test_X[0]

#models = ['vanilla', 'bidirectional', 'cnn-lstm', 'conv-lstm']
models = ['vanilla']
predictions_train = dict()
actual_train = dict()
predictions_test = dict()
actual_test = dict()
metrics_train = dict()
metrics_test = dict()
test_acc_all = dict()
test_stddev = dict()

for j in range(1):
    predictions_train_per_step = dict()
    actual_train_per_step = dict()  
    predictions_test_per_step = dict()
    actual_test_per_step = dict()
    metrics_train_per_step = dict()
    metrics_test_per_step = dict()
    test_acc_per_step = dict()
    test_stddev_per_step = dict()
    n_steps_out = j+1
    print('---------------------------------------------------------')
    print('no of steps out: ', n_steps_out)

    for i in models:
        print("for " + i + ":")

        if i == 'vanilla' or i=='bidirectional':
            x_train, y_train = np.asarray(x_shuffled), np.asarray(Y_hot_encoded_train)
            x_test, y_test = np.asarray(test_X), np.asarray(Y_hot_encoded_test)
            x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], n_features_in))
            x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], n_features_in))
        elif i == 'cnn-lstm':
            x_train, y_train = np.asarray(x_shuffled), np.asarray(Y_hot_encoded_train)
            x_test, y_test = np.asarray(test_X), np.asarray(Y_hot_encoded_test)
            x_train = x_train.reshape((x_train.shape[0], n_seq, int(n_steps_in/n_seq), n_features_in))
            x_test = x_test.reshape((x_test.shape[0], n_seq, int(n_steps_in/n_seq), n_features_in))
        elif i=='conv-lstm':
            x_train, y_train = np.asarray(x_shuffled), np.asarray(Y_hot_encoded_train)
            x_test, y_test = np.asarray(test_X), np.asarray(Y_hot_encoded_test)
            x_train = x_train.reshape((x_train.shape[0], n_seq, 1, int(n_steps_in/n_seq), n_features_in))
            x_test = x_test.reshape((x_test.shape[0], n_seq, 1, int(n_steps_in/n_seq), n_features_in))
        
        #print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)
        train_acc, test_acc, train_std_dev, test_std_dev, Best_Predict_Test, y_predicttrain, y_predicttest, report_train, report_test = MODEL_LSTM(i,'original', univariate,x_train,x_test,y_train,y_test,Num_Exp,n_steps_in,n_steps_out,Epochs, Hidden)
        predictions_train_per_step[i] = y_predicttrain
        actual_train_per_step[i] = y_train
        predictions_test_per_step[i] = Best_Predict_Test
        actual_test_per_step[i] = y_test
        metrics_train_per_step[i] = report_train
        metrics_test_per_step[i] = report_test 
        test_acc_per_step[i] = test_acc
        test_stddev_per_step[i] = test_std_dev
    predictions_train[str(j+1)] = predictions_train_per_step 
    actual_train[str(j+1)] = actual_train_per_step
    predictions_test[str(j+1)] = predictions_test_per_step 
    actual_test[str(j+1)] = actual_test_per_step
    metrics_train[str(j+1)] = metrics_train_per_step
    metrics_test[str(j+1)] = metrics_test_per_step
    test_acc_all[str(j+1)] = test_acc_per_step
    test_stddev[str(j+1)] = test_stddev_per_step

with open("predictions_" + ocean + '_original' + '.pkl', 'wb') as f: 
    pickle.dump([predictions_train,actual_train,predictions_test,actual_test,metrics_train,metrics_test,test_acc,test_stddev], f)

def make_confusion_matrix_chart2(cf_matrix_test, name):
    #plt.figure(figsize=(20,12))
    sns.set(font_scale=2.5)
    fig, ax = plt.subplots(figsize = (20,12))
    sns.heatmap(cf_matrix_test, annot=True, yticklabels=['0','1'], 
                                xticklabels=['0','1'], fmt='g')
    plt.ylabel("Actual", size=30)
    plt.xlabel("Pred", size=30)
    ax.tick_params(axis='both', which='major', labelsize=25)
    plt.savefig(name + '.png', dpi=300, transparent=False, bbox_inches='tight')
    return None

y = [i.argmax() for i in actual_test_per_step['vanilla']]
pred = [i.argmax() for i in predictions_test_per_step['vanilla']]
cf_matrix_test = confusion_matrix(y, pred)
make_confusion_matrix_chart2(cf_matrix_test, ocean + '_vanilla_cm_original')

precision0=[]
precision1=[]
precisionacc=[]
precisionmacavg=[]
precisionweighavg=[]
recall0=[]
recall1=[]
recallacc=[]
recallmacavg=[]
recallweighavg=[]
f10=[]
f11=[]
f1acc=[]
f1macavg=[]
f1weighavg=[]
for i in range(Num_Exp):
  precision0.append(metrics_test_per_step['vanilla'][i]['0']['precision'])
  precision1.append(metrics_test_per_step['vanilla'][i]['1']['precision'])
  precisionacc.append(metrics_test_per_step['vanilla'][i]['accuracy'])
  precisionmacavg.append(metrics_test_per_step['vanilla'][i]['macro avg']['precision'])
  precisionweighavg.append(metrics_test_per_step['vanilla'][i]['weighted avg']['precision'])
  recall0.append(metrics_test_per_step['vanilla'][i]['0']['recall'])
  recall1.append(metrics_test_per_step['vanilla'][i]['1']['recall'])
  recallacc.append(metrics_test_per_step['vanilla'][i]['accuracy'])
  recallmacavg.append(metrics_test_per_step['vanilla'][i]['macro avg']['recall'])
  recallweighavg.append(metrics_test_per_step['vanilla'][i]['weighted avg']['recall'])  
  f10.append(metrics_test_per_step['vanilla'][i]['0']['f1-score'])
  f11.append(metrics_test_per_step['vanilla'][i]['1']['f1-score'])
  f1acc.append(metrics_test_per_step['vanilla'][i]['accuracy'])
  f1macavg.append(metrics_test_per_step['vanilla'][i]['macro avg']['f1-score'])
  f1weighavg.append(metrics_test_per_step['vanilla'][i]['weighted avg']['f1-score'])

print(str(round(np.mean(precision0),4)) + "±" + str(round(np.std(precision0),4))," & " + str(round(np.mean(recall0),4)) + "±" + str(round(np.std(recall0),4)), " & " + str(round(np.mean(f10),4)) + "±" + str(round(np.std(f10),4)))
print(str(round(np.mean(precision1),4)) + "±" + str(round(np.std(precision1),4))," & " + str(round(np.mean(recall1),4)) + "±" + str(round(np.std(recall1),4)), " & " + str(round(np.mean(f11),4)) + "±" + str(round(np.std(f11),4)))
print(str(round(np.mean(precisionacc),4)) + "±" + str(round(np.std(precisionacc),4))," & " + str(round(np.mean(recallacc),4)) + "±" + str(round(np.std(recallacc),4)), " & " + str(round(np.mean(f1acc),4)) + "±" + str(round(np.std(f1acc),4)))
print(str(round(np.mean(precisionmacavg),4)) + "±" + str(round(np.std(precisionmacavg),4))," & " + str(round(np.mean(recallmacavg),4)) + "±" + str(round(np.std(recallmacavg),4)), " & " + str(round(np.mean(f1macavg),4)) + "±" + str(round(np.std(f1macavg),4)))
print(str(round(np.mean(precisionweighavg),4)) + "±" + str(round(np.std(precisionweighavg),4))," & " + str(round(np.mean(recallweighavg),4)) + "±" + str(round(np.std(recallweighavg),4)), " & " + str(round(np.mean(f1weighavg),4)) + "±" + str(round(np.std(f1weighavg),4)))

oversample = SMOTE()
X_smote, Y_smote = oversample.fit_resample(X, Y)
X_smote = X_smote.astype(np.float)

X.shape, test_X.shape, X_smote.shape

counter = Counter(Y_smote)
print(counter)
print(len(X_smote), len(X))
print("old smote data after 10000 index: ", Counter(Y_smote[10000:]))

idx = np.random.permutation(len(X_smote))
print(len(idx))

x_shuffled = []
y_shuffled = []
for i in idx:
  x_shuffled.append(X_smote[i])
  y_shuffled.append(Y_smote[i])

print("shuffled smote data after 10000 index: ", Counter(y_shuffled[10000:]))
Y_hot_encoded_train =  np.asarray(to_categorical(y_shuffled))
#Y_hot_encoded_train = Y_hot_encoded_train.reshape(len(y_shuffled), n_steps_out, n_features_out)

Y_hot_encoded_test =  np.asarray(to_categorical(test_Y))
#Y_hot_encoded_test = Y_hot_encoded_test.reshape(len(test_Y), n_steps_out, n_features_out)

Y_hot_encoded_train.shape, Y_hot_encoded_test.shape

x_train, y_train = np.asarray(x_shuffled), np.asarray(Y_hot_encoded_train)
            x_test, y_test = np.asarray(test_X), np.asarray(Y_hot_encoded_test)
            x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], n_features_in))
            x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], n_features_in))

train_acc, test_acc, train_std_dev, test_std_dev, Best_Predict_Test, y_predicttrain, y_predicttest, report_train, report_test = MODEL_LSTM('vanilla', 'smote', univariate,x_train,x_test,y_train,y_test,Num_Exp,n_steps_in,n_steps_out,Epochs, Hidden)

"""**SMOTE Results**"""

y = [i.argmax() for i in y_test]
pred = [i.argmax() for i in Best_Predict_Test]
cf_matrix_test = confusion_matrix(y, pred)
make_confusion_matrix_chart2(cf_matrix_test, ocean + '_vanilla_cm_smote')

with open("predictions_" + ocean + '_smote' + '.pkl', 'wb') as f: 
    pickle.dump([y_predicttrain, y_train, Best_Predict_Test, y_test, report_train, report_test, test_acc, test_std_dev], f)

precision0=[]
precision1=[]
precisionacc=[]
precisionmacavg=[]
precisionweighavg=[]
recall0=[]
recall1=[]
recallacc=[]
recallmacavg=[]
recallweighavg=[]
f10=[]
f11=[]
f1acc=[]
f1macavg=[]
f1weighavg=[]
for i in range(Num_Exp):
  precision0.append(report_test[i]['0']['precision'])
  precision1.append(report_test[i]['1']['precision'])
  precisionacc.append(report_test[i]['accuracy'])
  precisionmacavg.append(report_test[i]['macro avg']['precision'])
  precisionweighavg.append(report_test[i]['weighted avg']['precision'])
  recall0.append(report_test[i]['0']['recall'])
  recall1.append(report_test[i]['1']['recall'])
  recallacc.append(report_test[i]['accuracy'])
  recallmacavg.append(report_test[i]['macro avg']['recall'])
  recallweighavg.append(report_test[i]['weighted avg']['recall'])  
  f10.append(report_test[i]['0']['f1-score'])
  f11.append(report_test[i]['1']['f1-score'])
  f1acc.append(report_test[i]['accuracy'])
  f1macavg.append(report_test[i]['macro avg']['f1-score'])
  f1weighavg.append(report_test[i]['weighted avg']['f1-score'])    
print(str(round(np.mean(precision0),4)) + "±" + str(round(np.std(precision0),4))," & " + str(round(np.mean(recall0),4)) + "±" + str(round(np.std(recall0),4)), " & " + str(round(np.mean(f10),4)) + "±" + str(round(np.std(f10),4)))
print(str(round(np.mean(precision1),4)) + "±" + str(round(np.std(precision1),4))," & " + str(round(np.mean(recall1),4)) + "±" + str(round(np.std(recall1),4)), " & " + str(round(np.mean(f11),4)) + "±" + str(round(np.std(f11),4)))
print(str(round(np.mean(precisionacc),4)) + "±" + str(round(np.std(precisionacc),4))," & " + str(round(np.mean(recallacc),4)) + "±" + str(round(np.std(recallacc),4)), " & " + str(round(np.mean(f1acc),4)) + "±" + str(round(np.std(f1acc),4)))
print(str(round(np.mean(precisionmacavg),4)) + "±" + str(round(np.std(precisionmacavg),4))," & " + str(round(np.mean(recallmacavg),4)) + "±" + str(round(np.std(recallmacavg),4)), " & " + str(round(np.mean(f1macavg),4)) + "±" + str(round(np.std(f1macavg),4)))
print(str(round(np.mean(precisionweighavg),4)) + "±" + str(round(np.std(precisionweighavg),4))," & " + str(round(np.mean(recallweighavg),4)) + "±" + str(round(np.std(recallweighavg),4)), " & " + str(round(np.mean(f1weighavg),4)) + "±" + str(round(np.std(f1weighavg),4)))

# report_df = pd.DataFrame(report_test).transpose()
# report_df = report_df.reset_index()
# model_eval  = report_df[report_df['index'].str.contains('1')][['precision','recall','f1-score']]
# model_eval['accuracy']  = list(report_df[report_df['index'].str.contains('accuracy')]['support'])
# report_df

# for i in range(report_df.shape[0]):
#   tmp=""
#   for j in ['precision', 'recall', 'f1-score', 'support']:
#     tmp=tmp+ str(round(report_df[j][i],4)) + " & "
#   print(tmp)

"""**Simple GAN**"""

import torch
from torch import nn
from tqdm.auto import tqdm
from torchvision import transforms
from torchvision.utils import make_grid
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
torch.cuda.empty_cache()

x_shuffled = X
y_shuffled = Y

len(x_shuffled), len(X), len(y_shuffled)

t2 = np.asarray(x_shuffled).shape
X_oversampled = torch.from_numpy(np.asarray(x_shuffled))

def get_generator_block(input_dim, output_dim):
    return nn.Sequential(
        nn.Linear(input_dim, output_dim),
        nn.BatchNorm1d(output_dim),
        nn.ReLU(inplace=True),
    )
class Generator(nn.Module):

    def __init__(self, z_dim=t2[1], im_dim=t2[1], hidden_dim=128):
        super(Generator, self).__init__()
        self.gen = nn.Sequential(
            get_generator_block(z_dim, hidden_dim),
            get_generator_block(hidden_dim, hidden_dim * 2),
            get_generator_block(hidden_dim * 2, hidden_dim * 4),
            get_generator_block(hidden_dim * 4, hidden_dim * 8),
            nn.Linear(hidden_dim * 8, im_dim),
            nn.Sigmoid()
        )
    def forward(self, noise):
        return self.gen(noise)
    
    
    def get_gen(self):

        return self.gen
def get_discriminator_block(input_dim, output_dim):
    return nn.Sequential(
        nn.Linear(input_dim, output_dim),
        nn.LeakyReLU(0.2, inplace=True)        
    )
class Discriminator(nn.Module):
    def __init__(self, im_dim=t2[1], hidden_dim=128):
        super(Discriminator, self).__init__()
        self.disc = nn.Sequential(
            get_discriminator_block(im_dim, hidden_dim * 4),
            get_discriminator_block(hidden_dim * 4, hidden_dim * 2),
            get_discriminator_block(hidden_dim * 2, hidden_dim),
            nn.Linear(hidden_dim, 1)
        )

    def forward(self, image):

        return self.disc(image)
    
    def get_disc(self):

        return self.dis
def get_noise(n_samples, z_dim, device='cuda'):

    return torch.randn(n_samples,z_dim,device=device)

criterion = nn.BCEWithLogitsLoss()
n_epochs = 1000
z_dim = t2[1]
batch_size = 128
lr = 0.00001
display_step = 1
device = 'cuda'
gen = Generator(z_dim).to(device)
gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)
disc = Discriminator().to(device) 
disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)

def get_disc_loss(gen, disc, criterion, real, num_images, z_dim, device):

    fake_noise = get_noise(num_images, z_dim, device=device)
    fake = gen(fake_noise)
    disc_fake_pred = disc(fake.detach())
    disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))
    disc_real_pred = disc(real)
    disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))
    disc_loss = (disc_fake_loss + disc_real_loss) / 2

    return disc_loss

def get_gen_loss(gen, disc, criterion, num_images, z_dim, device):

    fake_noise = get_noise(num_images, z_dim, device=device)
    fake = gen(fake_noise)
    disc_fake_pred = disc(fake)
    gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))
    return gen_loss

li=[]
for i in range(len(y_shuffled)):
    if int(y_shuffled[i])==1:
        li.append(x_shuffled[i])
        
len(y_shuffled), len(li)

X_real=np.array(li)
t3=X_real.shape
li2=[1]*(t3[0])
y_real=np.array(li2)
y_real.shape

from torch.utils.data import TensorDataset, DataLoader
tensor_x = torch.Tensor(X_real) 
tensor_y = torch.Tensor(y_real)
my_dataset = TensorDataset(tensor_x,tensor_y)
dataloader = DataLoader(
    my_dataset,
    batch_size=batch_size,
    shuffle=True)

cur_step = 0
mean_generator_loss = 0
mean_discriminator_loss = 0
test_generator = True 
gen_loss = False
error = False

samples_to_generate = X_oversampled.shape[0]-X_real.shape[0]
print(samples_to_generate)

#epochs = [100,500,1000,2000,3000,4000,5000]
epochs = [10000]

original_gan_data = dict()

for no_epoch in epochs:
  print("no of epochs: ", no_epoch)
  for epoch in range(no_epoch):
  
   
    for real, _ in tqdm(dataloader, disable=True):
        cur_batch_size = len(real)

       
        real = real.view(cur_batch_size, -1).to(device)

      
       
        disc_opt.zero_grad()

       
        disc_loss = get_disc_loss(gen, disc, criterion, real, cur_batch_size, z_dim, device)

        
        disc_loss.backward(retain_graph=True)

       
        disc_opt.step()

      
        if test_generator:
            old_generator_weights = gen.gen[0][0].weight.detach().clone()

        gen_opt.zero_grad()
        gen_loss = get_gen_loss(gen, disc, criterion, cur_batch_size, z_dim, device)
        gen_loss.backward()
        gen_opt.step()

        if test_generator:
            try:
                assert lr > 0.0000002 or (gen.gen[0][0].weight.grad.abs().max() < 0.0005 and epoch == 0)
                assert torch.any(gen.gen[0][0].weight.detach().clone() != old_generator_weights)
            except:
                error = True
                print("Runtime tests have failed")

     
        mean_discriminator_loss += disc_loss.item() / display_step

  
        mean_generator_loss += gen_loss.item() / display_step

        if epoch%500==0:
            print(f"Epoch {epoch}: Step {cur_step}: Generator loss: {mean_generator_loss}, discriminator loss: {mean_discriminator_loss}")

        if cur_step % display_step == 0 and cur_step > 0:
            mean_generator_loss = 0
            mean_discriminator_loss = 0
        cur_step += 1
  fake_noise = get_noise(samples_to_generate, z_dim, device=device)
  res=gen(fake_noise)
  fres=res.cpu().detach().numpy()
  X_old=X
  finX=np.concatenate((X_old, fres), axis=0)
  y_fake = np.ones(samples_to_generate)
  Y_old=np.asarray(Y)
  finY = np.append(Y_old, y_fake, axis=0)
  print(finX.shape, finY.shape)
  #idx = np.random.permutation(len(X_smote))
  idx = np.random.permutation(finX.shape[0])
  print(len(idx))
  x_shuffled = []
  y_shuffled = []
  xy=dict()
  for i in idx:
    #x_shuffled.append(X_smote[i])
    #y_shuffled.append(Y_smote[i])
    x_shuffled.append(finX[i])
    y_shuffled.append(finY[i])
  xy['x_shuffled']=x_shuffled
  xy['y_shuffled']=y_shuffled
  original_gan_data[no_epoch]=xy

with open(ocean + '_original_gan_data' + '.pkl', 'wb') as f: 
    pickle.dump([original_gan_data], f)

len(original_gan_data[10000]['x_shuffled']), len(original_gan_data[10000]['y_shuffled'])

x_shuffled = original_gan_data[10000]['x_shuffled']
y_shuffled = original_gan_data[10000]['y_shuffled']

len(test_Y), test_X.shape

Y_hot_encoded_train =  np.asarray(to_categorical(y_shuffled))
#Y_hot_encoded_train = Y_hot_encoded_train.reshape(len(y_shuffled), n_steps_out, n_features_out)

Y_hot_encoded_test =  np.asarray(to_categorical(test_Y))
#Y_hot_encoded_test = Y_hot_encoded_test.reshape(len(test_Y), n_steps_out, n_features_out)

print(Y_hot_encoded_train.shape, Y_hot_encoded_test.shape)

test_X

x_shuffled[0], y_shuffled[0]

x_train, y_train = np.asarray(x_shuffled), np.asarray(Y_hot_encoded_train)
            x_test, y_test = np.asarray(test_X), np.asarray(Y_hot_encoded_test)
            x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], n_features_in))
            x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], n_features_in))

train_acc, test_acc, train_std_dev, test_std_dev, Best_Predict_Test, y_predicttrain, y_predicttest, report_train, report_test = MODEL_LSTM('vanilla', 'gan', univariate,x_train,x_test,y_train,y_test,Num_Exp,n_steps_in,n_steps_out,Epochs, Hidden)

with open("predictions_" + ocean + '_gan' + '.pkl', 'wb') as f: 
    pickle.dump([y_predicttrain, y_train, Best_Predict_Test, y_test, report_train, report_test, test_acc, test_std_dev], f)

"""**GAN Results**"""

precision0=[]
precision1=[]
precisionacc=[]
precisionmacavg=[]
precisionweighavg=[]
recall0=[]
recall1=[]
recallacc=[]
recallmacavg=[]
recallweighavg=[]
f10=[]
f11=[]
f1acc=[]
f1macavg=[]
f1weighavg=[]
for i in range(Num_Exp):
  precision0.append(report_test[i]['0']['precision'])
  precision1.append(report_test[i]['1']['precision'])
  precisionacc.append(report_test[i]['accuracy'])
  precisionmacavg.append(report_test[i]['macro avg']['precision'])
  precisionweighavg.append(report_test[i]['weighted avg']['precision'])
  recall0.append(report_test[i]['0']['recall'])
  recall1.append(report_test[i]['1']['recall'])
  recallacc.append(report_test[i]['accuracy'])
  recallmacavg.append(report_test[i]['macro avg']['recall'])
  recallweighavg.append(report_test[i]['weighted avg']['recall'])  
  f10.append(report_test[i]['0']['f1-score'])
  f11.append(report_test[i]['1']['f1-score'])
  f1acc.append(report_test[i]['accuracy'])
  f1macavg.append(report_test[i]['macro avg']['f1-score'])
  f1weighavg.append(report_test[i]['weighted avg']['f1-score'])    
print(str(round(np.mean(precision0),4)) + "±" + str(round(np.std(precision0),4))," & " + str(round(np.mean(recall0),4)) + "±" + str(round(np.std(recall0),4)), " & " + str(round(np.mean(f10),4)) + "±" + str(round(np.std(f10),4)))
print(str(round(np.mean(precision1),4)) + "±" + str(round(np.std(precision1),4))," & " + str(round(np.mean(recall1),4)) + "±" + str(round(np.std(recall1),4)), " & " + str(round(np.mean(f11),4)) + "±" + str(round(np.std(f11),4)))
print(str(round(np.mean(precisionacc),4)) + "±" + str(round(np.std(precisionacc),4))," & " + str(round(np.mean(recallacc),4)) + "±" + str(round(np.std(recallacc),4)), " & " + str(round(np.mean(f1acc),4)) + "±" + str(round(np.std(f1acc),4)))
print(str(round(np.mean(precisionmacavg),4)) + "±" + str(round(np.std(precisionmacavg),4))," & " + str(round(np.mean(recallmacavg),4)) + "±" + str(round(np.std(recallmacavg),4)), " & " + str(round(np.mean(f1macavg),4)) + "±" + str(round(np.std(f1macavg),4)))
print(str(round(np.mean(precisionweighavg),4)) + "±" + str(round(np.std(precisionweighavg),4))," & " + str(round(np.mean(recallweighavg),4)) + "±" + str(round(np.std(recallweighavg),4)), " & " + str(round(np.mean(f1weighavg),4)) + "±" + str(round(np.std(f1weighavg),4)))

y = [i.argmax() for i in y_test]
pred = [i.argmax() for i in Best_Predict_Test]
cf_matrix_test = confusion_matrix(y, pred)
make_confusion_matrix_chart2(cf_matrix_test, ocean + 'vanilla_cm_gan')

torch.cuda.empty_cache()

x_shuffled = X_smote
y_shuffled = Y_smote
print(len(x_shuffled), len(X), len(y_shuffled)) 
t2 = np.asarray(x_shuffled).shape
X_oversampled = torch.from_numpy(np.asarray(x_shuffled))

def get_generator_block(input_dim, output_dim):
    return nn.Sequential(
        nn.Linear(input_dim, output_dim),
        nn.BatchNorm1d(output_dim),
        nn.ReLU(inplace=True),
    )
class Generator(nn.Module):

    def __init__(self, z_dim=t2[1], im_dim=t2[1], hidden_dim=128):
        super(Generator, self).__init__()
        self.gen = nn.Sequential(
            get_generator_block(z_dim, hidden_dim),
            get_generator_block(hidden_dim, hidden_dim * 2),
            get_generator_block(hidden_dim * 2, hidden_dim * 4),
            get_generator_block(hidden_dim * 4, hidden_dim * 8),
            nn.Linear(hidden_dim * 8, im_dim),
            nn.Sigmoid()
        )
    def forward(self, noise):
        return self.gen(noise)
    
    
    def get_gen(self):

        return self.gen
def get_discriminator_block(input_dim, output_dim):
    return nn.Sequential(
        nn.Linear(input_dim, output_dim),
        nn.LeakyReLU(0.2, inplace=True)        
    )
class Discriminator(nn.Module):
    def __init__(self, im_dim=t2[1], hidden_dim=128):
        super(Discriminator, self).__init__()
        self.disc = nn.Sequential(
            get_discriminator_block(im_dim, hidden_dim * 4),
            get_discriminator_block(hidden_dim * 4, hidden_dim * 2),
            get_discriminator_block(hidden_dim * 2, hidden_dim),
            nn.Linear(hidden_dim, 1)
        )

    def forward(self, image):

        return self.disc(image)
    
    def get_disc(self):

        return self.dis
def get_noise(n_samples, z_dim, device='cuda'):

    return torch.randn(n_samples,z_dim,device=device)

criterion = nn.BCEWithLogitsLoss()
n_epochs = 1000
z_dim = t2[1]
batch_size = 128
lr = 0.00001
display_step = 1
device = 'cuda'
gen = Generator(z_dim).to(device)
gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)
disc = Discriminator().to(device) 
disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)

def get_disc_loss(gen, disc, criterion, real, num_images, z_dim, device):

    fake_noise = get_noise(num_images, z_dim, device=device)
    fake = gen(fake_noise)
    disc_fake_pred = disc(fake.detach())
    disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))
    disc_real_pred = disc(real)
    disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))
    disc_loss = (disc_fake_loss + disc_real_loss) / 2

    return disc_loss

def get_gen_loss(gen, disc, criterion, num_images, z_dim, device):

    fake_noise = get_noise(num_images, z_dim, device=device)
    fake = gen(fake_noise)
    disc_fake_pred = disc(fake)
    gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))
    return gen_loss

li=[]
for i in range(len(y_shuffled)):
    if int(y_shuffled[i])==1:
        li.append(x_shuffled[i])
        
print(len(y_shuffled), len(li))

X_real=np.array(li)
t3=X_real.shape
li2=[1]*(t3[0])
y_real=np.array(li2)
y_real.shape

from torch.utils.data import TensorDataset, DataLoader
tensor_x = torch.Tensor(X_real) 
tensor_y = torch.Tensor(y_real)
my_dataset = TensorDataset(tensor_x,tensor_y)
dataloader = DataLoader(
    my_dataset,
    batch_size=batch_size,
    shuffle=True)

cur_step = 0
mean_generator_loss = 0
mean_discriminator_loss = 0
test_generator = True 
gen_loss = False
error = False

samples_to_generate = X_oversampled.shape[0]-X_real.shape[0]
print(samples_to_generate)

#epochs = [100,500,1000,2000,3000,4000,5000]
epochs = [10000]

smote_gan_data = dict()

for no_epoch in epochs:
  print("no of epochs: ", no_epoch)
  for epoch in range(no_epoch):
  
   
    for real, _ in tqdm(dataloader, disable=True):
        cur_batch_size = len(real)

       
        real = real.view(cur_batch_size, -1).to(device)

      
       
        disc_opt.zero_grad()

       
        disc_loss = get_disc_loss(gen, disc, criterion, real, cur_batch_size, z_dim, device)

        
        disc_loss.backward(retain_graph=True)

       
        disc_opt.step()

      
        if test_generator:
            old_generator_weights = gen.gen[0][0].weight.detach().clone()

        gen_opt.zero_grad()
        gen_loss = get_gen_loss(gen, disc, criterion, cur_batch_size, z_dim, device)
        gen_loss.backward()
        gen_opt.step()

        if test_generator:
            try:
                assert lr > 0.0000002 or (gen.gen[0][0].weight.grad.abs().max() < 0.0005 and epoch == 0)
                assert torch.any(gen.gen[0][0].weight.detach().clone() != old_generator_weights)
            except:
                error = True
                print("Runtime tests have failed")

     
        mean_discriminator_loss += disc_loss.item() / display_step

  
        mean_generator_loss += gen_loss.item() / display_step

        if epoch%500==0:
            print(f"Epoch {epoch}: Step {cur_step}: Generator loss: {mean_generator_loss}, discriminator loss: {mean_discriminator_loss}")

        if cur_step % display_step == 0 and cur_step > 0:
            mean_generator_loss = 0
            mean_discriminator_loss = 0
        cur_step += 1
  fake_noise = get_noise(samples_to_generate, z_dim, device=device)
  res=gen(fake_noise)
  fres=res.cpu().detach().numpy()
  X_old=X
  finX=np.concatenate((X_old, fres), axis=0)
  y_fake = np.ones(samples_to_generate)
  Y_old=np.asarray(Y)
  finY = np.append(Y_old, y_fake, axis=0)
  print(finX.shape, finY.shape)
  #idx = np.random.permutation(len(X_smote))
  idx = np.random.permutation(finX.shape[0])
  print(len(idx))
  x_shuffled = []
  y_shuffled = []
  xy=dict()
  for i in idx:
    #x_shuffled.append(X_smote[i])
    #y_shuffled.append(Y_smote[i])
    x_shuffled.append(finX[i])
    y_shuffled.append(finY[i])
  xy['x_shuffled']=x_shuffled
  xy['y_shuffled']=y_shuffled
  smote_gan_data[no_epoch]=xy

with open(ocean + '_smote_gan_data' + '.pkl', 'wb') as f: 
    pickle.dump([smote_gan_data], f)

print(len(smote_gan_data[10000]['x_shuffled']), len(smote_gan_data[10000]['y_shuffled']))
x_shuffled = smote_gan_data[10000]['x_shuffled']
y_shuffled = smote_gan_data[10000]['y_shuffled']
print(len(test_Y), test_X.shape)

Y_hot_encoded_train =  np.asarray(to_categorical(y_shuffled))
#Y_hot_encoded_train = Y_hot_encoded_train.reshape(len(y_shuffled), n_steps_out, n_features_out)

Y_hot_encoded_test =  np.asarray(to_categorical(test_Y))
#Y_hot_encoded_test = Y_hot_encoded_test.reshape(len(test_Y), n_steps_out, n_features_out)

print(Y_hot_encoded_train.shape, Y_hot_encoded_test.shape)

test_X

x_shuffled[0], y_shuffled[0]

x_train, y_train = np.asarray(x_shuffled), np.asarray(Y_hot_encoded_train)
            x_test, y_test = np.asarray(test_X), np.asarray(Y_hot_encoded_test)
            x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], n_features_in))
            x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], n_features_in))

train_acc, test_acc, train_std_dev, test_std_dev, Best_Predict_Test, y_predicttrain, y_predicttest, report_train, report_test = MODEL_LSTM('vanilla', 'smote_gan', univariate,x_train,x_test,y_train,y_test,Num_Exp,n_steps_in,n_steps_out,Epochs, Hidden)

with open("predictions_" + ocean + '_smote_gan' + '.pkl', 'wb') as f: 
    pickle.dump([y_predicttrain, y_train, Best_Predict_Test, y_test, report_train, report_test, test_acc, test_std_dev], f)

"""**SMOTE GAN results**"""

precision0=[]
precision1=[]
precisionacc=[]
precisionmacavg=[]
precisionweighavg=[]
recall0=[]
recall1=[]
recallacc=[]
recallmacavg=[]
recallweighavg=[]
f10=[]
f11=[]
f1acc=[]
f1macavg=[]
f1weighavg=[]
for i in range(Num_Exp):
  precision0.append(report_test[i]['0']['precision'])
  precision1.append(report_test[i]['1']['precision'])
  precisionacc.append(report_test[i]['accuracy'])
  precisionmacavg.append(report_test[i]['macro avg']['precision'])
  precisionweighavg.append(report_test[i]['weighted avg']['precision'])
  recall0.append(report_test[i]['0']['recall'])
  recall1.append(report_test[i]['1']['recall'])
  recallacc.append(report_test[i]['accuracy'])
  recallmacavg.append(report_test[i]['macro avg']['recall'])
  recallweighavg.append(report_test[i]['weighted avg']['recall'])  
  f10.append(report_test[i]['0']['f1-score'])
  f11.append(report_test[i]['1']['f1-score'])
  f1acc.append(report_test[i]['accuracy'])
  f1macavg.append(report_test[i]['macro avg']['f1-score'])
  f1weighavg.append(report_test[i]['weighted avg']['f1-score'])    
print(str(round(np.mean(precision0),4)) + "±" + str(round(np.std(precision0),4))," & " + str(round(np.mean(recall0),4)) + "±" + str(round(np.std(recall0),4)), " & " + str(round(np.mean(f10),4)) + "±" + str(round(np.std(f10),4)))
print(str(round(np.mean(precision1),4)) + "±" + str(round(np.std(precision1),4))," & " + str(round(np.mean(recall1),4)) + "±" + str(round(np.std(recall1),4)), " & " + str(round(np.mean(f11),4)) + "±" + str(round(np.std(f11),4)))
print(str(round(np.mean(precisionacc),4)) + "±" + str(round(np.std(precisionacc),4))," & " + str(round(np.mean(recallacc),4)) + "±" + str(round(np.std(recallacc),4)), " & " + str(round(np.mean(f1acc),4)) + "±" + str(round(np.std(f1acc),4)))
print(str(round(np.mean(precisionmacavg),4)) + "±" + str(round(np.std(precisionmacavg),4))," & " + str(round(np.mean(recallmacavg),4)) + "±" + str(round(np.std(recallmacavg),4)), " & " + str(round(np.mean(f1macavg),4)) + "±" + str(round(np.std(f1macavg),4)))
print(str(round(np.mean(precisionweighavg),4)) + "±" + str(round(np.std(precisionweighavg),4))," & " + str(round(np.mean(recallweighavg),4)) + "±" + str(round(np.std(recallweighavg),4)), " & " + str(round(np.mean(f1weighavg),4)) + "±" + str(round(np.std(f1weighavg),4)))

y = [i.argmax() for i in y_test]
pred = [i.argmax() for i in Best_Predict_Test]
cf_matrix_test = confusion_matrix(y, pred)
make_confusion_matrix_chart2(cf_matrix_test, ocean + 'vanilla_cm_smote_gan')

for i in range(report_df.shape[0]):
  tmp=""
  for j in ['precision', 'recall', 'f1-score', 'support']:
    tmp=tmp+ str(round(report_df[j][i],4)) + " & "
  print(tmp)

"""**Recall Precision curves**"""

with open(ocean + '_original' + '.pkl', 'rb') as f: 
    predictions_train,actual_train,predictions_test,actual_test,metrics_train,metrics_test,test_acc,test_stddev=pickle.load(f)

lr_probs = predictions_test['1']['vanilla']
lr_probs = lr_probs[:, 1]
print(lr_probs)
testy= [i.argmax() for i in actual_test['1']['vanilla']]
yhat= [i.argmax() for i in predictions_test['1']['vanilla']]
lr_precision, lr_recall, _ = precision_recall_curve(testy, lr_probs)
lr_f1, lr_auc = f1_score(testy, yhat), auc(lr_recall, lr_precision)
# summarize scores
print('Logistic: f1=%.3f auc=%.3f' % (lr_f1, lr_auc))

with open(ocean + '_smote' + '.pkl', 'rb') as f: 
    y_predicttrain, y_train, Best_Predict_Test, y_test, report_train, report_test, test_acc, test_stddev = pickle.load(f)

lr_probs = Best_Predict_Test
lr_probs = lr_probs[:, 1]
print(lr_probs)
testy= [i.argmax() for i in y_test]
yhat= [i.argmax() for i in Best_Predict_Test]
lr_precision_smote, lr_recall_smote, _ = precision_recall_curve(testy, lr_probs)
lr_f1_smote, lr_auc_smote = f1_score(testy, yhat), auc(lr_recall_smote, lr_precision_smote)
# summarize scores
print('Logistic: f1=%.3f auc=%.3f' % (lr_f1_smote, lr_auc_smote))

with open(ocean + '_gan' + '.pkl', 'rb') as f: 
    y_predicttrain, y_train, Best_Predict_Test, y_test, report_train, report_test, test_acc, test_stddev = pickle.load(f)

lr_probs = Best_Predict_Test
lr_probs = lr_probs[:, 1]
print(lr_probs)
testy= [i.argmax() for i in y_test]
yhat= [i.argmax() for i in Best_Predict_Test]
lr_precision_gan, lr_recall_gan, _ = precision_recall_curve(testy, lr_probs)
lr_f1_gan, lr_auc_gan = f1_score(testy, yhat), auc(lr_recall_gan, lr_precision_gan)
# summarize scores
print('Logistic: f1=%.3f auc=%.3f' % (lr_f1_gan, lr_auc_gan))

with open(ocean + '_smote_gan' + '.pkl', 'rb') as f: 
    y_predicttrain, y_train, Best_Predict_Test, y_test, report_train, report_test, test_acc, test_stddev = pickle.load(f)

lr_probs = Best_Predict_Test
lr_probs = lr_probs[:, 1]
print(lr_probs)
testy= [i.argmax() for i in y_test]
yhat= [i.argmax() for i in Best_Predict_Test]
lr_precision_smote_gan, lr_recall_smote_gan, _ = precision_recall_curve(testy, lr_probs)
lr_f1_smote_gan, lr_auc_smote_gan = f1_score(testy, yhat), auc(lr_recall_smote_gan, lr_precision_smote_gan)
# summarize scores
print('Logistic: f1=%.3f auc=%.3f' % (lr_f1_smote_gan, lr_auc_smote_gan))

# plot the precision-recall curves
fig, ax = pyplot.subplots(figsize = (20,12))
no_skill = 135 / len(testy)    #no of one/total
pyplot.plot(lr_recall, lr_precision, marker='.',lw=4, label='Original data: ' + str(round(lr_f1,3)))
pyplot.plot(lr_recall_smote, lr_precision_smote,lw=4, marker='.', label='SMOTE: ' + str(round(lr_f1_smote,3)))
pyplot.plot(lr_recall_gan, lr_precision_gan,lw=4, marker='.', label='GAN: ' + str(round(lr_f1_gan,3)))
pyplot.plot(lr_recall_smote_gan, lr_precision_smote_gan,lw=4, marker='.', label='SMOTE-GAN: ' + str(round(lr_f1_smote_gan,3)))
# axis labels
pyplot.xlabel('Recall', size=40)
pyplot.ylabel('Precision', size=40)
ax.tick_params(axis='both', which='major', labelsize=30)
# show the legend
pyplot.legend(loc="upper right", fontsize=30)
#save the plot
plt.savefig(ocean + '_precision_recall_curve.png', dpi=300, transparent=False, bbox_inches='tight')
plt.show()
# show the plot
pyplot.show()

