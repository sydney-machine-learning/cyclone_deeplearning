{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from numpy import hstack\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import datetime\n",
    "import time\n",
    "import joblib\n",
    "from datetime import timedelta, date\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from numpy import array\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns; sns.set_theme()\n",
    "import errno\n",
    "\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import ConvLSTM2D\n",
    "from keras.models import load_model\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc, classification_report, confusion_matrix\n",
    "from scipy import interp\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import imblearn\n",
    "import collections\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_data_south_indian(url):\n",
    "    \"\"\"\n",
    "    Load and preprocess data for South Indian region.\n",
    "\n",
    "    Parameters:\n",
    "    - url (str): URL or file path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Processed DataFrame with a 'category' column.\n",
    "    \"\"\"\n",
    "    # Load data from CSV\n",
    "    df = pd.read_csv(url)\n",
    "\n",
    "    # Add a 'category' column based on speed ranges\n",
    "    df['category'] = df['Speed(knots)'].apply(lambda x:\n",
    "        0 if x <= 33 else\n",
    "        1 if 34 <= x <= 47 else\n",
    "        2 if 48 <= x <= 63 else\n",
    "        3 if 64 <= x <= 89 else\n",
    "        4 if 90 <= x <= 115 else\n",
    "        5\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "def load_data_south_pacific(url):\n",
    "    \"\"\"\n",
    "    Load and preprocess data for South Pacific region.\n",
    "\n",
    "    Parameters:\n",
    "    - url (str): URL or file path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Processed DataFrame with a 'category' column.\n",
    "    \"\"\"\n",
    "    # Load data from CSV\n",
    "    df = pd.read_csv(url)\n",
    "\n",
    "    # Add a 'category' column based on speed ranges\n",
    "    df['category'] = df['Speed(knots)'].apply(lambda x:\n",
    "        0 if x <= 33 else\n",
    "        1 if 34 <= x <= 47 else\n",
    "        2 if 48 <= x <= 63 else\n",
    "        3 if 64 <= x <= 85 else\n",
    "        4 if 86 <= x <= 107 else\n",
    "        5\n",
    "    )\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocean = 'south_pacific'\n",
    "\n",
    "# Display the selected ocean\n",
    "print(f\"Selected ocean: {ocean}\")\n",
    "\n",
    "# Set the data URL and function based on the selected ocean\n",
    "if ocean == 'south_indian':\n",
    "    url_data = 'https://raw.githubusercontent.com/sydney-machine-learning/cyclonedatasets/main/SouthIndian-SouthPacific-Ocean/South_indian_hurricane.csv'\n",
    "    data_loading_function = load_data_south_indian\n",
    "    hot_encoded_result_file_name = 'south_indian'\n",
    "    category_result_file_name = 'roc_data_south_indian'\n",
    "else:\n",
    "    url_data = 'https://raw.githubusercontent.com/sydney-machine-learning/cyclonedatasets/main/SouthIndian-SouthPacific-Ocean/South_pacific_hurricane.csv'\n",
    "    data_loading_function = load_data_south_pacific\n",
    "    hot_encoded_result_file_name = 'south_pacific'\n",
    "    category_result_file_name = 'roc_data_south_pacific'\n",
    "\n",
    "# Display the data URL for verification\n",
    "print(f\"Data URL: {url_data}\")\n",
    "\n",
    "# Display the result file names\n",
    "print(f\"Hot-encoded result file name: {hot_encoded_result_file_name}\")\n",
    "print(f\"Category result file name: {category_result_file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data using the specified function and URL\n",
    "df = data_loading_function(url_data)\n",
    "\n",
    "# Latitude and longitude values into int\n",
    "df['Lat'] = df['Lat'].apply(lambda x: -int(x[:-1]) * 0.1 if x.endswith('N') else int(x[:-1]) * 0.1 if x.endswith('S') else 0)\n",
    "df['Lon'] = df['Lon'].apply(lambda x: -int(x[:-1]) * 0.1 if x.endswith('W') else int(x[:-1]) * 0.1 if x.endswith('E') else 0)\n",
    "\n",
    "# Extract 'Speed(knots)' and 'category' columns as lists\n",
    "speed = df['Speed(knots)'].tolist()\n",
    "categories = df['category'].tolist()\n",
    "latitude = df['Lat'].tolist()\n",
    "longitude = df['Lon'].tolist()\n",
    "\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df_head = df.head()\n",
    "print(\"DataFrame Head:\")\n",
    "print(df_head)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def multivariate_split_sequence(sequence, n_steps):\n",
    "    \"\"\"\n",
    "    Split multivariate sequence into input and output parts.\n",
    "\n",
    "    Parameters:\n",
    "    - sequence (numpy.ndarray): Multivariate time series data.\n",
    "    - n_steps (int): Number of input time steps.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Input sequences (X) and output sequences (y) as numpy arrays.\n",
    "    \"\"\"\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        end_ix = i + n_steps\n",
    "        if end_ix > len(sequence) - 1:\n",
    "            break\n",
    "        seq_x, seq_y = sequence[i:end_ix, :], sequence[end_ix, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Example usage:\n",
    "# Assuming df is your DataFrame with 'Latitude', 'Longitude', 'Speed(knots)', and 'category' columns\n",
    "features_columns = ['Lat', 'Lon','Speed(knots)']\n",
    "target_column = 'category'\n",
    "\n",
    "# Extract features and target\n",
    "features = df[features_columns].values\n",
    "target = df[target_column].values\n",
    "\n",
    "n_steps = 5  # Number of input time steps\n",
    "\n",
    "X, y = multivariate_split_sequence(features, n_steps)\n",
    "\n",
    "# Now X contains sequences of 'Latitude', 'Longitude', and 'Speed(knots)' for each time step,\n",
    "# and y contains the corresponding 'category' values.\n",
    "\n",
    "def rmse(pred, actual):\n",
    "    \"\"\"\n",
    "    Calculate the Root Mean Squared Error (RMSE) between two arrays.\n",
    "\n",
    "    Parameters:\n",
    "    - pred (numpy.ndarray): Predicted values.\n",
    "    - actual (numpy.ndarray): Actual values.\n",
    "\n",
    "    Returns:\n",
    "    - float: Root Mean Squared Error.\n",
    "    \"\"\"\n",
    "    return np.sqrt(((pred - actual) ** 2).mean())\n",
    "\n",
    "def categorical(pred, actual):\n",
    "    \"\"\"\n",
    "    Compute classification metrics for categorical values.\n",
    "\n",
    "    Parameters:\n",
    "    - pred (numpy.ndarray): Predicted categorical values.\n",
    "    - actual (numpy.ndarray): Actual categorical values.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Accuracy, AUC, Confusion Matrix, Precision, Recall, F1 Score.\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(pred, actual)\n",
    "    acc = accuracy_score(actual, pred, normalize=True, sample_weight=None)\n",
    "    ps1 = precision_score(pred, actual, average='micro')\n",
    "    rs1 = recall_score(pred, actual, average='micro')\n",
    "    f11 = f1_score(pred, actual, average='micro')\n",
    "    auc = roc_auc_score(actual, pred)\n",
    "    return acc, auc, cm, ps1, rs1, f11\n",
    "\n",
    "def make_confusion_matrix_chart(cf_matrix_test, cmap='Blues', annot_kws=None):\n",
    "    \"\"\"\n",
    "    Generate and display a heatmap-style confusion matrix chart.\n",
    "\n",
    "    Parameters:\n",
    "    - cf_matrix_test (numpy.ndarray): Confusion matrix.\n",
    "    - cmap (str): Colormap for the heatmap.\n",
    "    - annot_kws (dict): Additional keyword arguments for annotation customization.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Set up the figure and axes\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Customize the heatmap using seaborn\n",
    "    sns.heatmap(cf_matrix_test, annot=True, cmap=cmap,\n",
    "                yticklabels=['0', '1'], xticklabels=['0', '1'],\n",
    "                fmt='g', annot_kws=annot_kws)\n",
    "\n",
    "    # Customize axis labels and title\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.title('Confusion Matrix - Test Data')\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define whether it's a univariate or multivariate case\n",
    "univariate = False  # If True, it's a multivariate case\n",
    "\n",
    "# Define sequence and time step parameters\n",
    "n_steps_in = 8 # Adjust based on the number of time steps you want to consider for input\n",
    "n_seq = 3 # Adjust based on your specific needs\n",
    "n_steps_out = 1  # Number of output time steps\n",
    "\n",
    "# Define the number of features for input and output\n",
    "n_features_in = 3  # Latitude, Longitude, Speed\n",
    "n_features_out = 2  # 'category'\n",
    "\n",
    "# Define the number of hidden layers in the model\n",
    "hidden_layers = 50\n",
    "\n",
    "# Define training parameters\n",
    "epochs = 100\n",
    "No_exp = 30 # Number of experiments\n",
    "\n",
    "# Display the configuration\n",
    "\n",
    "print(f\"Univariate: {univariate}\")\n",
    "print(f\"Number of Input Time Steps: {n_steps_in}\")\n",
    "print(f\"Number of Input Sequences: {n_seq}\")\n",
    "print(f\"Number of Output Time Steps: {n_steps_out}\")\n",
    "print(f\"Number of Input Features: {n_features_in}\")\n",
    "print(f\"Number of Output Features: {n_features_out}\")\n",
    "print(f\"Number of Hidden Layers: {hidden_layers}\")\n",
    "print(f\"Epochs: {epochs}\")\n",
    "print(f\"Number of Experiments: {No_exp}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize variables\n",
    "cyclone_id = df['No. of Cycl'][0]\n",
    "X = []\n",
    "Y = []\n",
    "start_index = 0\n",
    "end_index = 0\n",
    "\n",
    "# Iterate through the DataFrame\n",
    "for i in range(1, df.shape[0]):\n",
    "    # Check if the cyclone ID is the same as the previous row\n",
    "    if df['No. of Cycl'][i] == cyclone_id:\n",
    "        end_index += 1\n",
    "    else:\n",
    "        # Split the sequence and append to X and Y\n",
    "        x, y = multivariate_split_sequence(features[start_index:end_index + 1, :], n_steps_in)\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "\n",
    "        # Update start and end indices for the new cyclone\n",
    "        cyclone_id = df['No. of Cycl'][i]\n",
    "        start_index = i\n",
    "        end_index = i\n",
    "\n",
    "    # Check if it's the last row of the DataFrame\n",
    "    if i == df.shape[0] - 1:\n",
    "        # Split the sequence and append to X and Y\n",
    "        x, y = multivariate_split_sequence(features[start_index:end_index + 1, :], n_steps_in)\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "print(Y[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the initial lengths of X and Y\n",
    "print(\"Initial Lengths - X:\", len(X), \"Y:\", len(Y))\n",
    "\n",
    "# Flattening X and Y\n",
    "X = [item for sublist in X for item in sublist]\n",
    "Y = [item for sublist in Y for item in sublist]\n",
    "\n",
    "# Print lengths of X and Y after flattening\n",
    "print(\"Flattened Lengths - X:\", len(X), \"Y:\", len(Y))\n",
    "\n",
    "# Print some initial values from X and Y\n",
    "print(X[0], Y[0])\n",
    "\n",
    "# Print some initial values from the 'speed' column\n",
    "print(\"Sample Values - Speed:\", speed[:10])\n",
    "X1 = X\n",
    "Y1 = Y\n",
    "X2 = X\n",
    "Y2 = Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating a list for RI cyclone classification\n",
    "intensify_y = []\n",
    "for i in range(len(X)):\n",
    "  intensify_y.append(0)\n",
    "\n",
    "for i in range(len(X)):\n",
    "    # Iterate through each column in X[i]\n",
    "    for j in range(len(X[0])):\n",
    "        # Iterate through k from 1 to 4\n",
    "        for k in range(1, 5):\n",
    "            # Check if the difference is greater than 30\n",
    "            if (j - k) >= 0 and (X[i][j][2] - X[i][j-k][2]) >= 30:\n",
    "                # Set Y[i] to 1 and break the loop\n",
    "                intensify_y[i] = 1\n",
    "                break\n",
    "\n",
    "\n",
    "print(len(intensify_y))\n",
    "\n",
    "# Use 'intensify_y' as the updated target variable\n",
    "Y = intensify_y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the training limit as 75% of the total length of X\n",
    "train_limit = int(len(X) * 0.75)\n",
    "\n",
    "# Display the calculated training limit\n",
    "print(\"Training set size:\", train_limit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract test data for evaluation\n",
    "test_X_original = X[train_limit + 1:]\n",
    "test_Y_original = Y[train_limit + 1:]\n",
    "\n",
    "# Display the lengths of the datasets\n",
    "print(\"Length of X:\", len(X))\n",
    "print(\"Length of Y:\", len(Y))\n",
    "print(\"Length of Test X (for evaluation):\", len(test_X_original))\n",
    "print(\"Length of Test Y (for evaluation):\", len(test_Y_original))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X = np.array(X)\n",
    "scalers = {}\n",
    "for i in range(X.shape[1]):\n",
    "    scalers[i] = MinMaxScaler()\n",
    "    X[:, i, :] = scalers[i].fit_transform(X[:, i, :])\n",
    "print(X[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_x = X\n",
    "test_X = X[train_limit+1:]\n",
    "test_X = np.asarray(test_X).astype(float)\n",
    "test_Y = Y[train_limit+1:]\n",
    "X = X[:train_limit]\n",
    "X = np.asarray(X).astype(float)\n",
    "Y = Y[:train_limit]\n",
    "print(len(test_X), len(test_Y))\n",
    "len(X), len(Y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Similarly, you can do the same for the training set (X and Y) before splitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Calculate the class distribution for training and test data\n",
    "counter_train = Counter(Y)\n",
    "counter_test = Counter(test_Y)\n",
    "\n",
    "# Display the class distribution for training data\n",
    "print(\"Class Distribution - Training Data:\")\n",
    "print(counter_train)\n",
    "\n",
    "# Display the class distribution for test data\n",
    "print(\"\\nClass Distribution - Test Data:\")\n",
    "print(counter_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming counter_train is already defined\n",
    "\n",
    "# Set\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "# Bar plot with red bars\n",
    "bars = plt.bar(range(len(counter_train)), list(counter_train.values()), color='lightcoral', edgecolor='black', align='center')\n",
    "\n",
    "# Customizing individual bars\n",
    "for bar in bars:\n",
    "    yvar = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, yvar + 50, round(yvar, 2), ha='center', va='bottom', fontsize=12, color='#2E4053')\n",
    "\n",
    "# X-axis and Y-axis labels\n",
    "plt.xlabel('Category', size=16, labelpad=15)\n",
    "plt.ylabel('Number of cyclones', size=16, labelpad=15)\n",
    "\n",
    "# Title\n",
    "plt.title('Majority and Minority class distribution in Training Data', size=20, pad=20)\n",
    "\n",
    "# Tick parameters with only '0' and '1' on the x-axis\n",
    "plt.xticks(range(2), ['0', '1'], fontsize=12)\n",
    "\n",
    "# Tick parameters for y-axis\n",
    "ax.tick_params(axis='y', which='major', labelsize=12)\n",
    "\n",
    "# Adding grid lines\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Save figure\n",
    "plt.savefig('class_distribution_red.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def multivariate_vanilla_lstm(n_steps_in, n_steps_out, n_features_in, n_features_out):\n",
    "    \"\"\"\n",
    "    Create a Multivariate LSTM model for sequence prediction.\n",
    "\n",
    "    Parameters:\n",
    "    - n_steps_in: Number of time steps in the input sequence.\n",
    "    - n_steps_out: Number of time steps in the output sequence.\n",
    "    - n_features_in: Number of input features.\n",
    "    - n_features_out: Number of output features.\n",
    "\n",
    "    Returns:\n",
    "    - model: Compiled Multivariate LSTM model.\n",
    "    \"\"\"\n",
    "    # Initialize a sequential model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add an LSTM layer with 50 units and ReLU activation\n",
    "    model.add(LSTM(50, activation='relu', input_shape=(n_steps_in, n_features_in)))\n",
    "\n",
    "    # Add a Dense layer with a single unit and sigmoid activation for binary classification\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    # Compile the model using Adam optimizer and binary crossentropy loss\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def bidirectional_lstm(n_steps_in, n_steps_out, n_features_in, n_features_out):\n",
    "    \"\"\"\n",
    "    Create a Bidirectional LSTM model for sequence prediction.\n",
    "\n",
    "    Parameters:\n",
    "    - n_steps_in: Number of time steps in the input sequence.\n",
    "    - n_steps_out: Number of time steps in the output sequence.\n",
    "    - n_features_in: Number of input features.\n",
    "    - n_features_out: Number of output features.\n",
    "\n",
    "    Returns:\n",
    "    - model: Compiled Bidirectional LSTM model.\n",
    "    \"\"\"\n",
    "    # Initialize a sequential model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add a Bidirectional LSTM layer with 50 units and ReLU activation\n",
    "    model.add(Bidirectional(LSTM(50, activation='relu', input_shape=(n_steps_in, n_features_in))))\n",
    "\n",
    "    # Add a Dense output layer with softmax activation for classification\n",
    "    # Adjust the activation based on the nature of the task (e.g., 'linear' for regression)\n",
    "    model.add(Dense(n_features_out, activation='softmax'))\n",
    "\n",
    "    # Compile the model using Adam optimizer and binary crossentropy loss\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "    # Build the model\n",
    "    model.build(input_shape=(None, n_steps_in, n_features_in))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import TimeDistributed, Conv1D, MaxPooling1D, Flatten, LSTM, Dense\n",
    "\n",
    "def cnn_lstm(n_steps_in, n_steps_out, n_features_in, n_features_out, n_seq):\n",
    "    \"\"\"\n",
    "    Create a CNN-LSTM model for sequence prediction.\n",
    "\n",
    "    Parameters:\n",
    "    - n_steps_in: Number of time steps in the input sequence.\n",
    "    - n_steps_out: Number of time steps in the output sequence.\n",
    "    - n_features_in: Number of input features.\n",
    "    - n_features_out: Number of output features.\n",
    "    - n_seq: Number of sequences.\n",
    "\n",
    "    Returns:\n",
    "    - model: Compiled CNN-LSTM model.\n",
    "    \"\"\"\n",
    "    # Initialize a sequential model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add a 1D Convolutional layer with ReLU activation, followed by MaxPooling and Flatten\n",
    "    model.add(TimeDistributed(Conv1D(filters=64, kernel_size=1, activation='relu'), input_shape=(None, int(n_steps_in/n_seq), n_features_in)))\n",
    "    model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "    # Add an LSTM layer with 50 units and ReLU activation\n",
    "    model.add(LSTM(50, activation='relu'))\n",
    "\n",
    "    # Add a Dense output layer with softmax activation for classification\n",
    "    # Adjust the activation based on the nature of the task (e.g., 'linear' for regression)\n",
    "    model.add(Dense(n_features_out, activation='softmax'))\n",
    "\n",
    "    # Compile the model using Adam optimizer and binary crossentropy loss\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    model.build(input_shape=(None, n_seq, int(n_steps_in/n_seq), n_features_in))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def conv_lstm(n_steps_in, n_steps_out, n_features_in, n_features_out, n_seq):\n",
    "    \"\"\"\n",
    "    Create a ConvLSTM model for sequence prediction.\n",
    "\n",
    "    Parameters:\n",
    "    - n_steps_in: Number of time steps in the input sequence.\n",
    "    - n_steps_out: Number of time steps in the output sequence.\n",
    "    - n_features_in: Number of input features.\n",
    "    - n_features_out: Number of output features.\n",
    "    - n_seq: Number of sequences.\n",
    "\n",
    "    Returns:\n",
    "    - model: Compiled ConvLSTM model.\n",
    "    \"\"\"\n",
    "    # Initialize a sequential model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add a ConvLSTM2D layer with 64 filters, (1,2) kernel size, and ReLU activation\n",
    "    model.add(ConvLSTM2D(filters=64, kernel_size=(1, 2), activation='relu', input_shape=(n_seq, 1, int(n_steps_in/n_seq), n_features_in)))\n",
    "\n",
    "    # Flatten the output\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Add a Dense output layer with softmax activation for classification\n",
    "    # Adjust the activation based on the nature of the task (e.g., 'linear' for regression)\n",
    "    model.add(Dense(n_features_out, activation='softmax'))\n",
    "\n",
    "    # Compile the model using Adam optimizer and binary crossentropy loss\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# all models\n",
    "def lstm(model_name, method, univariate, x_train, x_test, y_train, y_test, Num_Exp, n_steps_in, n_steps_out, Epochs, Hidden):\n",
    "    # Initialize arrays to store accuracy metrics\n",
    "    train_acc = np.zeros(Num_Exp)\n",
    "    test_acc = np.zeros(Num_Exp)\n",
    "\n",
    "    # Choose the appropriate model based on the specified model_name\n",
    "    if model_name == 'vanilla':\n",
    "        model = multivariate_vanilla_lstm(n_steps_in, n_steps_out, n_features_in, n_features_out)\n",
    "    elif model_name == 'bidirectional':\n",
    "        model = bidirectional_lstm(n_steps_in, n_steps_out, n_features_in, n_features_out)\n",
    "    elif model_name == 'cnn-lstm':\n",
    "        model = cnn_lstm(n_steps_in, n_steps_out, n_features_in, n_features_out, n_seq)\n",
    "    elif model_name == 'conv-lstm':\n",
    "        model = conv_lstm(n_steps_in, n_steps_out, n_features_in, n_features_out, n_seq)\n",
    "    else:\n",
    "        # Handle the case where model_name is not recognized (add appropriate logic or raise an error)\n",
    "        raise ValueError(f\"Unsupported model_name: {model_name}\")\n",
    "\n",
    "    # Display the model summary\n",
    "    model.summary()\n",
    "\n",
    "    # Initialize arrays to store predictions and classification reports\n",
    "    y_predicttest_allruns = np.zeros([Num_Exp, x_test.shape[0], y_test.shape[1]])\n",
    "    Best_f1 = 0  # Initialize the best F1 score\n",
    "\n",
    "    # Extract the actual classes from one-hot encoded vectors for both test and train sets\n",
    "    act_test = [y_test[i].argmax() for i in range(y_test.shape[0])]\n",
    "    act_train = [y_train[i].argmax() for i in range(y_train.shape[0])]\n",
    "\n",
    "    # Initialize dictionaries to store classification reports\n",
    "    Best_report_train = dict()\n",
    "    Best_report_test = dict()\n",
    "    all_report_train = dict()\n",
    "    all_report_test = dict()\n",
    "\n",
    "    # Loop through experiment runs\n",
    "    start_time = time.time()\n",
    "    for run in range(Num_Exp):\n",
    "        print(\"Experiment\", run + 1, \"in progress\")\n",
    "\n",
    "        # Fit the model\n",
    "        model.fit(x_train, y_train, epochs=Epochs, batch_size=10, verbose=0, shuffle=False)\n",
    "        y_predicttrain = model.predict(x_train)\n",
    "        y_predicttest = model.predict(x_test)\n",
    "\n",
    "        # Extract predicted classes from one-hot encoded vectors\n",
    "        pred_test = [y_predicttest[i].argmax() for i in range(y_predicttest.shape[0])]\n",
    "        pred_train = [y_predicttrain[i].argmax() for i in range(y_predicttrain.shape[0])]\n",
    "\n",
    "        # Generate classification reports\n",
    "        report_train = classification_report(act_train, pred_train, labels=[0, 1], output_dict=True)\n",
    "        report_test = classification_report(act_test, pred_test, labels=[0, 1], output_dict=True)\n",
    "\n",
    "        # Store classification reports in dictionaries\n",
    "        all_report_train[run] = report_train\n",
    "        all_report_test[run] = report_test\n",
    "\n",
    "        # Calculate F1-score for the test set\n",
    "        test_acc[run] = report_test['1']['f1-score']\n",
    "        print(\"train acc: \", report_train['1']['f1-score'])\n",
    "        print(\"test acc: \", test_acc[run])\n",
    "\n",
    "        precision_minority = report_test['1']['precision']\n",
    "        recall_minority = report_test['1']['recall']\n",
    "        print(\"Precision (Minority Class):\", precision_minority)\n",
    "        print(\"Recall (Minority Class):\", recall_minority)\n",
    "\n",
    "        macro_avg_precision = report_test['macro avg']['precision']\n",
    "        macro_avg_recall = report_test['macro avg']['recall']\n",
    "        macro_avg_f1 = report_test['macro avg']['f1-score']\n",
    "\n",
    "        weighted_avg_precision = report_test['weighted avg']['precision']\n",
    "        weighted_avg_recall = report_test['weighted avg']['recall']\n",
    "        weighted_avg_f1 = report_test['weighted avg']['f1-score']\n",
    "\n",
    "        # Print or store Macro Avg and Weighted Avg values\n",
    "        print(\"Macro Avg Precision:\", macro_avg_precision)\n",
    "        print(\"Macro Avg Recall:\", macro_avg_recall)\n",
    "        print(\"Macro Avg F1-Score:\", macro_avg_f1)\n",
    "\n",
    "        print(\"Weighted Avg Precision:\", weighted_avg_precision)\n",
    "        print(\"Weighted Avg Recall:\", weighted_avg_recall)\n",
    "        print(\"Weighted Avg F1-Score:\", weighted_avg_f1)\n",
    "\n",
    "\n",
    "\n",
    "        # Update the best F1 score and associated predictions and reports\n",
    "        if test_acc[run] > Best_f1:\n",
    "            Best_f1 = test_acc[run]\n",
    "            Best_Predict_Test = y_predicttest\n",
    "            Best_report_train, Best_report_test = report_train, report_test\n",
    "\n",
    "    # Save the trained model\n",
    "    model.save(\"model_\" + ocean + \"_\" + model_name + \"_\" + method + '.h5')\n",
    "\n",
    "    # Calculate standard deviations of train and test accuracies\n",
    "    train_std = np.std(train_acc)\n",
    "    test_std = np.std(test_acc)\n",
    "\n",
    "    # Display experiment summary\n",
    "    print(\"Total time for\", Num_Exp, \"experiments\", time.time() - start_time)\n",
    "    print(\"F1 scores for test data: \", test_acc)\n",
    "    print(\"Mean: \", np.mean(test_acc), \"Std Dev: \", test_std)\n",
    "\n",
    "    # Return relevant information\n",
    "    return train_acc, test_acc, train_std, test_std, Best_Predict_Test, y_predicttrain, y_predicttest, all_report_train, all_report_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random permutation of indices for shuffling\n",
    "idx = np.random.permutation(len(X))\n",
    "\n",
    "# Initialize lists to store shuffled data\n",
    "x_shuffled = []\n",
    "y_shuffled = []\n",
    "\n",
    "# Iterate through the shuffled indices\n",
    "for i in idx:\n",
    "    # Append shuffled data to the lists\n",
    "    x_shuffled.append(X[i])\n",
    "    y_shuffled.append(Y[i])\n",
    "x_a = np.array(x_shuffled)\n",
    "print(x_a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the shuffled labels to one-hot encoded format for training data\n",
    "Y_hot_encoded_train = np.asarray(to_categorical(y_shuffled))\n",
    "\n",
    "# Convert the test labels to one-hot encoded format\n",
    "Y_hot_encoded_test = np.asarray(to_categorical(test_Y))\n",
    "\n",
    "# Print the shapes of the one-hot encoded training and test labels\n",
    "print(Y_hot_encoded_train.shape, Y_hot_encoded_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_shuffled[0], test_X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Testing on all 4 different kinds of LSTMs\n",
    "models = ['vanilla']\n",
    "\n",
    "\n",
    "# Initialize dictionaries to store predictions, actual values, and metrics for training and testing\n",
    "predictions_train = dict()\n",
    "actual_train = dict()\n",
    "predictions_test = dict()\n",
    "actual_test = dict()\n",
    "metrics_train = dict()\n",
    "metrics_test = dict()\n",
    "test_acc_all = dict()\n",
    "test_stddev = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionaries to store results for each model\n",
    "for j in range(1):\n",
    "\n",
    "  predictions_train_per_step = dict()\n",
    "  actual_train_per_step = dict()\n",
    "  predictions_test_per_step = dict()\n",
    "  actual_test_per_step = dict()\n",
    "  metrics_train_per_step = dict()\n",
    "  metrics_test_per_step = dict()\n",
    "  test_acc_per_step = dict()\n",
    "  test_stddev_per_step = dict()\n",
    "  n_steps_out = j + 1\n",
    "\n",
    "  print('---------------------------------------------------------')\n",
    "  print('Number of steps out:', n_steps_out)\n",
    "\n",
    "  # Loop over different LSTM models\n",
    "  for i in models:\n",
    "    print(\"For \" + i + \":\")\n",
    "\n",
    "    # Reshape data based on the LSTM model type\n",
    "    if i == 'vanilla' or i == 'bidirectional' :\n",
    "        x_train, y_train = np.asarray(x_shuffled), np.asarray(Y_hot_encoded_train)\n",
    "        x_test, y_test = np.asarray(test_X), np.asarray(Y_hot_encoded_test)\n",
    "        x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], n_features_in))\n",
    "        x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], n_features_in))\n",
    "    elif i == 'cnn-lstm':\n",
    "        x_train, y_train = np.asarray(x_shuffled), np.asarray(Y_hot_encoded_train)\n",
    "        x_test, y_test = np.asarray(test_X), np.asarray(Y_hot_encoded_test)\n",
    "        x_train = x_train.reshape((x_train.shape[0], n_seq, int(n_steps_in / n_seq), n_features_in))\n",
    "        x_test = x_test.reshape((x_test.shape[0], n_seq, int(n_steps_in / n_seq), n_features_in))\n",
    "    elif i == 'conv-lstm':\n",
    "        print(\"Original shape of x_shuffled:\", np.array(x_shuffled).shape)\n",
    "        x_train, y_train = np.asarray(x_shuffled), np.asarray(Y_hot_encoded_train)\n",
    "        print(\"Shape after reshape:\", np.array(x_train).shape)\n",
    "        x_test, y_test = np.asarray(test_X), np.asarray(Y_hot_encoded_test)\n",
    "        x_train = x_train.reshape((x_train.shape[0], n_seq, 1, int(n_steps_in/n_seq), n_features_in))\n",
    "        x_test = x_test.reshape(x_test.shape[0], n_seq, 1, int(n_steps_in/n_seq), n_features_in)\n",
    "\n",
    "    # Call the LSTM function and retrieve results\n",
    "    train_acc, test_acc, train_std_dev, test_std_dev, Best_Predict_Test, y_predicttrain, y_predicttest, report_train, report_test = lstm(\n",
    "        i, 'original', univariate, x_train, x_test, y_train, y_test, No_exp, n_steps_in, n_steps_out, epochs, hidden_layers)\n",
    "\n",
    "    # Store results in respective dictionaries\n",
    "    predictions_train_per_step[i] = y_predicttrain\n",
    "    actual_train_per_step[i] = y_train\n",
    "    predictions_test_per_step[i] = Best_Predict_Test\n",
    "    actual_test_per_step[i] = y_test\n",
    "    metrics_train_per_step[i] = report_train\n",
    "    metrics_test_per_step[i] = report_test\n",
    "    test_acc_per_step[i] = test_acc\n",
    "    test_stddev_per_step[i] = test_std_dev\n",
    "\n",
    "# Store results for the current n_steps_out in the overall dictionaries\n",
    "  predictions_train[str(j + 1)] = predictions_train_per_step\n",
    "  actual_train[str(j + 1)] = actual_train_per_step\n",
    "  predictions_test[str(j + 1)] = predictions_test_per_step\n",
    "  actual_test[str(j + 1)] = actual_test_per_step\n",
    "  metrics_train[str(j + 1)] = metrics_train_per_step\n",
    "  metrics_test[str(j + 1)] = metrics_test_per_step\n",
    "  test_acc_all[str(j + 1)] = test_acc_per_step\n",
    "  test_stddev[str(j + 1)] = test_stddev_per_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"predictions_\" + ocean + '_original' + '.pkl', 'wb') as f:\n",
    "    pickle.dump([predictions_train,actual_train,predictions_test,actual_test,metrics_train,metrics_test,test_acc,test_stddev], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def make_confusion_matrix_chart(cf_matrix, name):\n",
    "    \"\"\"\n",
    "    Create and save a well-formatted confusion matrix heatmap.\n",
    "\n",
    "    Parameters:\n",
    "    - cf_matrix: Confusion matrix\n",
    "    - name: Name of the file to save the plot\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "\n",
    "    # Set a Seaborn style with dark grid\n",
    "    sns.set(style=\"darkgrid\", font_scale=1.5)\n",
    "\n",
    "    # Create a figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "    # Customize the appearance of the heatmap with green colors\n",
    "    sns.heatmap(cf_matrix, annot=True, fmt='d', cmap='Greens',\n",
    "                linewidths=.5, square=True, cbar=False,\n",
    "                annot_kws={\"size\": 16}, ax=ax)\n",
    "\n",
    "    # Set labels and title\n",
    "    plt.ylabel(\"Actual\", size=18)\n",
    "    plt.xlabel(\"Predicted\", size=18)\n",
    "    plt.title(\"Confusion Matrix\", size=20)\n",
    "\n",
    "    # Set tick parameters\n",
    "    ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "    # Save the plot as an image\n",
    "    plt.savefig(name + '.png', dpi=300, transparent=False, bbox_inches='tight')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "    # Return None\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [i.argmax() for i in actual_test_per_step['vanilla']]\n",
    "pred = [i.argmax() for i in predictions_test_per_step['vanilla']]\n",
    "cf_matrix_test = confusion_matrix(y, pred)\n",
    "make_confusion_matrix_chart(cf_matrix_test, ocean + 'vanilla_cm_original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store precision, recall, and F1-score metrics for class 0, class 1, overall accuracy,\n",
    "# macro average, and weighted average across multiple experiments\n",
    "precision0 = []\n",
    "precision1 = []\n",
    "precisionacc = []\n",
    "precisionmacavg = []\n",
    "precisionweighavg = []\n",
    "recall0 = []\n",
    "recall1 = []\n",
    "recallacc = []\n",
    "recallmacavg = []\n",
    "recallweighavg = []\n",
    "f10 = []\n",
    "f11 = []\n",
    "f1acc = []\n",
    "f1macavg = []\n",
    "f1weighavg = []\n",
    "\n",
    "# Loop through the results of multiple experiments\n",
    "for i in range(No_exp):\n",
    "    # Append precision, recall, and F1-score metrics for class 0, class 1, overall accuracy,\n",
    "    # macro average, and weighted average from the model results\n",
    "    precision0.append(metrics_test_per_step['vanilla'][i]['0']['precision'])\n",
    "    precision1.append(metrics_test_per_step['vanilla'][i]['1']['precision'])\n",
    "    precisionacc.append(metrics_test_per_step['vanilla'][i]['accuracy'])\n",
    "    precisionmacavg.append(metrics_test_per_step['vanilla'][i]['macro avg']['precision'])\n",
    "    precisionweighavg.append(metrics_test_per_step['vanilla'][i]['weighted avg']['precision'])\n",
    "\n",
    "    recall0.append(metrics_test_per_step['vanilla'][i]['0']['recall'])\n",
    "    recall1.append(metrics_test_per_step['vanilla'][i]['1']['recall'])\n",
    "    recallacc.append(metrics_test_per_step['vanilla'][i]['accuracy'])\n",
    "    recallmacavg.append(metrics_test_per_step['vanilla'][i]['macro avg']['recall'])\n",
    "    recallweighavg.append(metrics_test_per_step['vanilla'][i]['weighted avg']['recall'])\n",
    "\n",
    "    f10.append(metrics_test_per_step['vanilla'][i]['0']['f1-score'])\n",
    "    f11.append(metrics_test_per_step['vanilla'][i]['1']['f1-score'])\n",
    "    f1acc.append(metrics_test_per_step['vanilla'][i]['accuracy'])\n",
    "    f1macavg.append(metrics_test_per_step['vanilla'][i]['macro avg']['f1-score'])\n",
    "    f1weighavg.append(metrics_test_per_step['vanilla'][i]['weighted avg']['f1-score'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(round(np.mean(precision0),4)) + \"±\" + str(round(np.std(precision0),4)),\" & \" + str(round(np.mean(recall0),4)) + \"±\" + str(round(np.std(recall0),4)), \" & \" + str(round(np.mean(f10),4)) + \"±\" + str(round(np.std(f10),4)))\n",
    "print(str(round(np.mean(precision1),4)) + \"±\" + str(round(np.std(precision1),4)),\" & \" + str(round(np.mean(recall1),4)) + \"±\" + str(round(np.std(recall1),4)), \" & \" + str(round(np.mean(f11),4)) + \"±\" + str(round(np.std(f11),4)))\n",
    "print(str(round(np.mean(precisionacc),4)) + \"±\" + str(round(np.std(precisionacc),4)),\" & \" + str(round(np.mean(recallacc),4)) + \"±\" + str(round(np.std(recallacc),4)), \" & \" + str(round(np.mean(f1acc),4)) + \"±\" + str(round(np.std(f1acc),4)))\n",
    "print(str(round(np.mean(precisionmacavg),4)) + \"±\" + str(round(np.std(precisionmacavg),4)),\" & \" + str(round(np.mean(recallmacavg),4)) + \"±\" + str(round(np.std(recallmacavg),4)), \" & \" + str(round(np.mean(f1macavg),4)) + \"±\" + str(round(np.std(f1macavg),4)))\n",
    "print(str(round(np.mean(precisionweighavg),4)) + \"±\" + str(round(np.std(precisionweighavg),4)),\" & \" + str(round(np.mean(recallweighavg),4)) + \"±\" + str(round(np.std(recallweighavg),4)), \" & \" + str(round(np.mean(f1weighavg),4)) + \"±\" + str(round(np.std(f1weighavg),4)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
