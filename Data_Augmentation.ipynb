{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import RepeatVector\n",
    "\n",
    "from keras.layers import TimeDistributed\n",
    "from numpy import hstack\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import datetime\n",
    "import time\n",
    "import joblib\n",
    "from datetime import timedelta, date\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from numpy import array\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns; sns.set_theme()\n",
    "import errno\n",
    "\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import ConvLSTM2D\n",
    "from keras.models import load_model\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc, classification_report, confusion_matrix\n",
    "from scipy import interp\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import imblearn\n",
    "import collections\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_data_south_indian(url):\n",
    "    \"\"\"\n",
    "    Load and preprocess data for South Indian region.\n",
    "\n",
    "    Parameters:\n",
    "    - url (str): URL or file path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Processed DataFrame with a 'category' column.\n",
    "    \"\"\"\n",
    "    # Load data from CSV\n",
    "    df = pd.read_csv(url)\n",
    "\n",
    "    # Add a 'category' column based on speed ranges\n",
    "    df['category'] = df['Speed(knots)'].apply(lambda x:\n",
    "        0 if x <= 33 else\n",
    "        1 if 34 <= x <= 47 else\n",
    "        2 if 48 <= x <= 63 else\n",
    "        3 if 64 <= x <= 89 else\n",
    "        4 if 90 <= x <= 115 else\n",
    "        5\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "def load_data_south_pacific(url):\n",
    "    \"\"\"\n",
    "    Load and preprocess data for South Pacific region.\n",
    "\n",
    "    Parameters:\n",
    "    - url (str): URL or file path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Processed DataFrame with a 'category' column.\n",
    "    \"\"\"\n",
    "    # Load data from CSV\n",
    "    df = pd.read_csv(url)\n",
    "\n",
    "    # Add a 'category' column based on speed ranges\n",
    "    df['category'] = df['Speed(knots)'].apply(lambda x:\n",
    "        0 if x <= 33 else\n",
    "        1 if 34 <= x <= 47 else\n",
    "        2 if 48 <= x <= 63 else\n",
    "        3 if 64 <= x <= 85 else\n",
    "        4 if 86 <= x <= 107 else\n",
    "        5\n",
    "    )\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocean = 'south_indian'\n",
    "\n",
    "# Display the selected ocean\n",
    "print(f\"Selected ocean: {ocean}\")\n",
    "\n",
    "# Set the data URL and function based on the selected ocean\n",
    "if ocean == 'south_indian':\n",
    "    url_data = 'https://raw.githubusercontent.com/sydney-machine-learning/cyclonedatasets/main/SouthIndian-SouthPacific-Ocean/South_indian_hurricane.csv'\n",
    "    data_loading_function = load_data_south_indian\n",
    "    hot_encoded_result_file_name = 'south_indian'\n",
    "    category_result_file_name = 'roc_data_south_indian'\n",
    "else:\n",
    "    url_data = 'https://raw.githubusercontent.com/sydney-machine-learning/cyclonedatasets/main/SouthIndian-SouthPacific-Ocean/South_pacific_hurricane.csv'\n",
    "    data_loading_function = load_data_south_pacific\n",
    "    hot_encoded_result_file_name = 'south_pacific'\n",
    "    category_result_file_name = 'roc_data_south_pacific'\n",
    "\n",
    "# Display the data URL for verification\n",
    "print(f\"Data URL: {url_data}\")\n",
    "\n",
    "# Display the result file names\n",
    "print(f\"Hot-encoded result file name: {hot_encoded_result_file_name}\")\n",
    "print(f\"Category result file name: {category_result_file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data using the specified function and URL\n",
    "df = data_loading_function(url_data)\n",
    "\n",
    "# Latitude and longitude values into int\n",
    "df['Lat'] = df['Lat'].apply(lambda x: -int(x[:-1]) * 0.1 if x.endswith('N') else int(x[:-1]) * 0.1 if x.endswith('S') else 0)\n",
    "df['Lon'] = df['Lon'].apply(lambda x: -int(x[:-1]) * 0.1 if x.endswith('W') else int(x[:-1]) * 0.1 if x.endswith('E') else 0)\n",
    "\n",
    "# Extract 'Speed(knots)' and 'category' columns as lists\n",
    "speed = df['Speed(knots)'].tolist()\n",
    "categories = df['category'].tolist()\n",
    "latitude = df['Lat'].tolist()\n",
    "longitude = df['Lon'].tolist()\n",
    "\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df_head = df.head()\n",
    "print(\"DataFrame Head:\")\n",
    "print(df_head)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def split_sequence(sequences, n_steps_in, n_steps_out):\n",
    "    \"\"\"\n",
    "    Split multivariate sequences into input and output parts.\n",
    "\n",
    "    Parameters:\n",
    "    - sequences (numpy.ndarray): Multivariate time series data.\n",
    "    - n_steps_in (int): Number of input time steps.\n",
    "    - n_steps_out (int): Number of output time steps.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Input sequences (X) and output sequences (y) as numpy arrays.\n",
    "    \"\"\"\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out\n",
    "        if out_end_ix > len(sequences):\n",
    "            break\n",
    "        seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix:out_end_ix, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return X, y\n",
    "\n",
    "def uni_split_sequence(sequence, n_steps):\n",
    "    \"\"\"\n",
    "    Split univariate sequence into input and output parts.\n",
    "\n",
    "    Parameters:\n",
    "    - sequence (list): Univariate time series data.\n",
    "    - n_steps (int): Number of input time steps.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Input sequences (X) and output sequences (y) as numpy arrays.\n",
    "    \"\"\"\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        end_ix = i + n_steps\n",
    "        if end_ix > len(sequence) - 1:\n",
    "            break\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return X , y\n",
    "\n",
    "\n",
    "def multivariate_split_sequence(sequence, n_steps):\n",
    "    \"\"\"\n",
    "    Split multivariate sequence into input and output parts.\n",
    "\n",
    "    Parameters:\n",
    "    - sequence (numpy.ndarray): Multivariate time series data.\n",
    "    - n_steps (int): Number of input time steps.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Input sequences (X) and output sequences (y) as numpy arrays.\n",
    "    \"\"\"\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        end_ix = i + n_steps\n",
    "        if end_ix > len(sequence) - 1:\n",
    "            break\n",
    "        seq_x, seq_y = sequence[i:end_ix, :], sequence[end_ix, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return X, y\n",
    "\n",
    "# Example usage:\n",
    "# Assuming df is your DataFrame with 'Latitude', 'Longitude', 'Speed(knots)', and 'category' columns\n",
    "features_columns = ['Lat', 'Lon', 'Speed(knots)']\n",
    "target_column = 'category'\n",
    "\n",
    "# Extract features and target\n",
    "features = df[features_columns].values\n",
    "target = df[target_column].values\n",
    "\n",
    "n_steps = 5  # Number of input time steps\n",
    "\n",
    "X, y = multivariate_split_sequence(features, n_steps)\n",
    "\n",
    "# Now X contains sequences of 'Latitude', 'Longitude', and 'Speed(knots)' for each time step,\n",
    "# and y contains the corresponding 'category' values.\n",
    "\n",
    "def rmse(pred, actual):\n",
    "    \"\"\"\n",
    "    Calculate the Root Mean Squared Error (RMSE) between two arrays.\n",
    "\n",
    "    Parameters:\n",
    "    - pred (numpy.ndarray): Predicted values.\n",
    "    - actual (numpy.ndarray): Actual values.\n",
    "\n",
    "    Returns:\n",
    "    - float: Root Mean Squared Error.\n",
    "    \"\"\"\n",
    "    return np.sqrt(((pred - actual) ** 2).mean())\n",
    "\n",
    "def categorical(pred, actual):\n",
    "    \"\"\"\n",
    "    Compute classification metrics for categorical values.\n",
    "\n",
    "    Parameters:\n",
    "    - pred (numpy.ndarray): Predicted categorical values.\n",
    "    - actual (numpy.ndarray): Actual categorical values.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Accuracy, AUC, Confusion Matrix, Precision, Recall, F1 Score.\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(pred, actual)\n",
    "    acc = accuracy_score(actual, pred, normalize=True, sample_weight=None)\n",
    "    ps1 = precision_score(pred, actual, average='micro')\n",
    "    rs1 = recall_score(pred, actual, average='micro')\n",
    "    f11 = f1_score(pred, actual, average='micro')\n",
    "    auc = roc_auc_score(actual, pred)\n",
    "    return acc, auc, cm, ps1, rs1, f11\n",
    "\n",
    "def make_confusion_matrix_chart(cf_matrix_test, cmap='Blues', annot_kws=None):\n",
    "    \"\"\"\n",
    "    Generate and display a heatmap-style confusion matrix chart.\n",
    "\n",
    "    Parameters:\n",
    "    - cf_matrix_test (numpy.ndarray): Confusion matrix.\n",
    "    - cmap (str): Colormap for the heatmap.\n",
    "    - annot_kws (dict): Additional keyword arguments for annotation customization.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Set up the figure and axes\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Customize the heatmap using seaborn\n",
    "    sns.heatmap(cf_matrix_test, annot=True, cmap=cmap,\n",
    "                yticklabels=['0', '1'], xticklabels=['0', '1'],\n",
    "                fmt='g', annot_kws=annot_kws)\n",
    "\n",
    "    # Customize axis labels and title\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.title('Confusion Matrix - Test Data')\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define whether it's a univariate or multivariate case\n",
    "univariate_1 = False  # If True, it's a multivariate case\n",
    "\n",
    "# Define sequence and time step parameters\n",
    "n_steps_in_1 = 8 # Adjust based on the number of time steps you want to consider for input\n",
    "n_seq_1= 2  # Adjust based on your specific needs\n",
    "n_steps_out_1 = 1  # Number of output time steps\n",
    "\n",
    "# Define the number of features for input and output\n",
    "n_features_in_1 = 2  # Latitude, Longitude\n",
    "n_features_out_1 = 2  # 'category'\n",
    "\n",
    "# Define whether it's a univariate or multivariate case\n",
    "univariate_2 = True  # If not True, it's a multivariate case\n",
    "\n",
    "# Define sequence and time step parameters\n",
    "n_steps_in_2  = 8  # Adjust based on the number of time steps you want to consider for input\n",
    "n_seq_2 = 2  # Adjust based on your specific needs\n",
    "n_steps_out_2 = 1  # Number of output time steps\n",
    "\n",
    "# Define the number of features for input and output\n",
    "n_features_in_2 = 1  # Latitude, Longitude, Speed\n",
    "n_features_out_2 = 2  # 'category'\n",
    "\n",
    "# Define the number of hidden layers in the model\n",
    "hidden_layers = 50\n",
    "\n",
    "# Define training parameters\n",
    "epochs = 100\n",
    "No_exp = 30 # Number of experiments\n",
    "\n",
    "# Display the configuration\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize variables\n",
    "cyclone_id = df['No. of Cycl'][0]\n",
    "X = []\n",
    "Y = []\n",
    "start_index = 0\n",
    "end_index = 0\n",
    "\n",
    "# Iterate through the DataFrame\n",
    "for i in range(1, df.shape[0]):\n",
    "    # Check if the cyclone ID is the same as the previous row\n",
    "    if df['No. of Cycl'][i] == cyclone_id:\n",
    "        end_index += 1\n",
    "    else:\n",
    "        # Split the sequence and append to X and Y\n",
    "        x, y = multivariate_split_sequence(features[start_index:end_index + 1, :], n_steps_in_1)\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "\n",
    "        # Update start and end indices for the new cyclone\n",
    "        cyclone_id = df['No. of Cycl'][i]\n",
    "        start_index = i\n",
    "        end_index = i\n",
    "\n",
    "    # Check if it's the last row of the DataFrame\n",
    "    if i == df.shape[0] - 1:\n",
    "        # Split the sequence and append to X and Y\n",
    "        x, y = multivariate_split_sequence(features[start_index:end_index + 1, :], n_steps_in_1)\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the initial lengths of X and Y\n",
    "print(len(X), len(Y))\n",
    "# Flattening X and Y\n",
    "X = [item for sublist in X for item in sublist]\n",
    "Y = [item for sublist in Y for item in sublist]\n",
    "# Print lengths of X and Y after flattening\n",
    "print(len(X), len(Y))\n",
    "print(type(X), Y[0])\n",
    "print(speed[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(X)):\n",
    "  X[i] = X[i].tolist()\n",
    "for i in range(len(X)):\n",
    "  print(type(X[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating a list for RI cyclone classification\n",
    "intensify_y = []\n",
    "for i in range(len(X)):\n",
    "  intensify_y.append(0)\n",
    "\n",
    "for i in range(len(X)):\n",
    "    # Iterate through each column in X[i]\n",
    "    for j in range(len(X[0])):\n",
    "        # Iterate through k from 1 to 4\n",
    "        for k in range(1, 5):\n",
    "            # Check if the difference is greater than 30\n",
    "            if (j - k) >= 0 and (X[i][j][2] - X[i][j-k][2]) >= 30:\n",
    "                # Set Y[i] to 1 and break the loop\n",
    "                intensify_y[i] = 1\n",
    "                break\n",
    "\n",
    "\n",
    "print(len(intensify_y))\n",
    "\n",
    "# Use 'intensify_y' as the updated target variable\n",
    "Y = intensify_y\n",
    "\n",
    "print(Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xe = X\n",
    "Ye = Y\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import numpy as np\n",
    "\n",
    "# Assuming X has shape (samples, time_steps, features)\n",
    "# Flatten the time steps and features\n",
    "X1 = np.array(X)\n",
    "X_flat = np.reshape(X1, (X1.shape[0], -1))\n",
    "\n",
    "# Check the class distribution before oversampling\n",
    "print(\"Class distribution before oversampling:\", Counter(Y))\n",
    "\n",
    "# Create the RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "\n",
    "# Fit and apply the random oversampling on your training data\n",
    "X_resampled_flat, Y_resampled = ros.fit_resample(X_flat, Y)\n",
    "\n",
    "# Check the class distribution after oversampling\n",
    "print(\"Class distribution after oversampling:\", Counter(Y_resampled))\n",
    "\n",
    "# Reshape the oversampled data back to its original shape\n",
    "X1 = np.reshape(X_resampled_flat, (X_resampled_flat.shape[0], X1.shape[1], X1.shape[2]))\n",
    "Y = Y_resampled\n",
    "X = X1.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i in range(len(X)):\n",
    "  for j in range(len(X[i])):\n",
    "    X[i][j].append(Y[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = []\n",
    "Y1 = []\n",
    "for i in range(len(X)):\n",
    "  x = []\n",
    "  y = []\n",
    "  for j in range(0,4):\n",
    "    x.append(X[i][j])\n",
    "  for k in range(4, 8):\n",
    "    y.append(X[i][k])\n",
    "  X1.append(x)\n",
    "  Y1.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y2 = []\n",
    "for i in range(len(Y1)):\n",
    "  y = []\n",
    "  for j in range(len(Y1[i])):\n",
    "    y1 = []\n",
    "    for k in range(0,3):\n",
    "      y1.append(Y1[i][j][k])\n",
    "    y.append(y1)\n",
    "  Y2.append(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y2[123])\n",
    "Y1 = Y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert the list to a NumPy array, shuffle it, and then convert it back to a list\n",
    "idx = np.random.permutation(len(X1))\n",
    "\n",
    "# Initialize lists to store shuffled data\n",
    "x_shuffled = []\n",
    "y_shuffled = []\n",
    "\n",
    "# Iterate through the shuffled indices\n",
    "for i in idx:\n",
    "    # Append shuffled data to the lists\n",
    "    x_shuffled.append(X1[i])\n",
    "    y_shuffled.append(Y1[i])\n",
    "\n",
    "X1 = x_shuffled\n",
    "Y1 = y_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the training limit as 75% of the total length of X\n",
    "train_limit = int(len(X1) * 0.75)\n",
    "\n",
    "# Display the calculated training limit\n",
    "print(\"Training set size:\", train_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract test data for evaluation\n",
    "test_X = X1[train_limit + 1:]\n",
    "test_Y = Y1[train_limit + 1:]\n",
    "X1 = X1[:train_limit]\n",
    "Y1 = Y1[:train_limit]\n",
    "# Display the lengths of the datasets\n",
    "print(\"Length of X:\", len(X1))\n",
    "print(\"Length of Y:\", len(Y1))\n",
    "print(\"Length of Test X (for evaluation):\", len(test_X))\n",
    "print(\"Length of Test Y (for evaluation):\", len(test_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X1 = []\n",
    "test_Y1 = []\n",
    "for i in range(len(test_X)):\n",
    "  if(test_X[i][0][3]==1):\n",
    "    test_X1.append(test_X[i])\n",
    "    test_Y1.append(test_Y[i])\n",
    "test_X = test_X1\n",
    "test_Y = test_Y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list to a NumPy array, shuffle it, and then convert it back to a list\n",
    "idx = np.random.permutation(len(test_X1))\n",
    "\n",
    "# Initialize lists to store shuffled data\n",
    "x_shuffled = []\n",
    "y_shuffled = []\n",
    "\n",
    "# Iterate through the shuffled indices\n",
    "for i in idx:\n",
    "    # Append shuffled data to the lists\n",
    "    x_shuffled.append(test_X[i])\n",
    "    y_shuffled.append(test_Y[i])\n",
    "\n",
    "test_X = x_shuffled\n",
    "test_Y = y_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list to a NumPy array, shuffle it, and then convert it back to a list\n",
    "idx = np.random.permutation(len(X1))\n",
    "\n",
    "# Initialize lists to store shuffled data\n",
    "x_shuffled = []\n",
    "y_shuffled = []\n",
    "\n",
    "# Iterate through the shuffled indices\n",
    "for i in idx:\n",
    "    # Append shuffled data to the lists\n",
    "    x_shuffled.append(X1[i])\n",
    "    y_shuffled.append(Y1[i])\n",
    "\n",
    "X1 = x_shuffled\n",
    "Y1 = y_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_timesteps = 4\n",
    "num_features = 4\n",
    "\n",
    "# Number of output time steps and features\n",
    "num_output_timesteps = 4\n",
    "num_output_features = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "def simple_lstm_model(num_timesteps, num_output_timesteps, num_output_features, num_input_features):\n",
    "    \"\"\"\n",
    "    Create and train a simpler LSTM model for predicting latitude and longitude.\n",
    "\n",
    "    Parameters:\n",
    "    - num_timesteps: Number of input time steps.\n",
    "    - num_output_timesteps: Number of output time steps.\n",
    "    - num_output_features: Number of output features (latitude and longitude).\n",
    "    - num_input_features: Number of input features.\n",
    "\n",
    "    Returns:\n",
    "    - LSTM model.\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=50, input_shape=(num_timesteps, num_input_features), return_sequences=False))\n",
    "    model.add(Dense(units=num_output_timesteps * num_output_features))\n",
    "    model.add(tf.keras.layers.Reshape((num_output_timesteps, num_output_features)))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_Y[145])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "def lstm_regression(model_name, x_train, x_test, y_train, y_test, Num_Exp, n_steps_in, n_steps_out, Epochs, Hidden):\n",
    "    # Initialize arrays to store accuracy metrics\n",
    "    train_mse_lat = np.zeros(Num_Exp)\n",
    "    train_mse_lon = np.zeros(Num_Exp)\n",
    "    test_mse_lat = np.zeros(Num_Exp)\n",
    "    test_mse_lon = np.zeros(Num_Exp)\n",
    "    train_mae_lat = np.zeros(Num_Exp)\n",
    "    train_mae_lon = np.zeros(Num_Exp)\n",
    "    test_mae_lat = np.zeros(Num_Exp)\n",
    "    test_mae_lon = np.zeros(Num_Exp)\n",
    "    train_r2_lat = np.zeros(Num_Exp)\n",
    "    train_r2_lon = np.zeros(Num_Exp)\n",
    "    test_r2_lat = np.zeros(Num_Exp)\n",
    "    test_r2_lon = np.zeros(Num_Exp)\n",
    "\n",
    "    model = simple_lstm_model(4, 4, 3, 4)\n",
    "\n",
    "    # Display the model summary\n",
    "    model.summary()\n",
    "\n",
    "    # Initialize arrays to store predictions\n",
    "    Best_mse_lat = float('inf')  # Initialize the best MSE score\n",
    "    Best_mse_lon = float('inf')\n",
    "    Best_mae_lat = float('inf')  # Initialize the best MSE score\n",
    "    Best_mae_lon = float('inf')\n",
    "    Best_r2_lat = 0  # Initialize the best MSE score\n",
    "    Best_r2_lon = 0\n",
    "\n",
    "    # Loop through experiment runs\n",
    "    start_time = time.time()\n",
    "    for run in range(Num_Exp):\n",
    "        print(\"Experiment\", run + 1, \"in progress\")\n",
    "\n",
    "        # Fit the model\n",
    "        model.fit(x_train, y_train, epochs=Epochs, batch_size=10, verbose=0, shuffle=False)\n",
    "        y_predicttrain = model.predict(x_train)\n",
    "        y_predicttest = model.predict(x_test)\n",
    "        print(y_predicttrain[0])\n",
    "        print(y_predicttest[0])\n",
    "        print(len(y_predicttrain))\n",
    "        print(len(y_predicttest))\n",
    "        # Flatten y_true and y_predict arrays\n",
    "        y_train_lat = []\n",
    "        for i in range(len(y_train)):\n",
    "          for j in range(len(y_train[i])):\n",
    "            y_train_lat.append(y_train[i][j][0])\n",
    "        y_predtrain_lat = []\n",
    "        for i in range(len(y_predicttrain)):\n",
    "          for j in range(len(y_predicttrain[i])):\n",
    "            y_predtrain_lat.append(y_predicttrain[i][j][0])\n",
    "        y_train_lon = []\n",
    "        for i in range(len(y_train)):\n",
    "          for j in range(len(y_train[i])):\n",
    "            y_train_lon.append(y_train[i][j][1])\n",
    "        y_predtrain_lon = []\n",
    "        for i in range(len(y_predicttrain)):\n",
    "          for j in range(len(y_predicttrain[i])):\n",
    "            y_predtrain_lon.append(y_predicttrain[i][j][1])\n",
    "        y_test_lat = []\n",
    "        print(type(y_test[0]))\n",
    "        print(len(y_test))\n",
    "        for i in range(len(y_test)):\n",
    "          for j in range(len(y_test[i])):\n",
    "            y_test_lat.append(y_test[i][j][0])\n",
    "        y_predtest_lat = []\n",
    "        for i in range(len(y_predicttest)):\n",
    "          for j in range(len(y_predicttest[i])):\n",
    "            y_predtest_lat.append(y_predicttest[i][j][0])\n",
    "        y_test_lon = []\n",
    "        for i in range(len(y_test)):\n",
    "          for j in range(len(y_test[i])):\n",
    "            y_test_lon.append(y_test[i][j][1])\n",
    "        y_predtest_lon = []\n",
    "        for i in range(len(y_predicttest)):\n",
    "          for j in range(len(y_predicttest[i])):\n",
    "            y_predtest_lon.append(y_predicttest[i][j][1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Calculate MSE for train and test sets\n",
    "        mse_train_lat = np.sqrt(mean_squared_error(y_train_lat, y_predtrain_lat))\n",
    "        mse_train_lon = np.sqrt(mean_squared_error(y_train_lon, y_predtrain_lon))\n",
    "        mse_test_lat = np.sqrt(mean_squared_error(y_test_lat, y_predtest_lat))\n",
    "        mse_test_lon = np.sqrt(mean_squared_error(y_test_lon, y_predtest_lon))\n",
    "        mae_train_lat = mean_absolute_error(y_train_lat, y_predtrain_lat)\n",
    "        mae_train_lon = mean_absolute_error(y_train_lon, y_predtrain_lon)\n",
    "        mae_test_lat = mean_absolute_error(y_test_lat, y_predtest_lat)\n",
    "        mae_test_lon = mean_absolute_error(y_test_lon, y_predtest_lon)\n",
    "        r2_train_lat = r2_score(y_train_lat, y_predtrain_lat)\n",
    "        r2_train_lon = r2_score(y_train_lon, y_predtrain_lon)\n",
    "        r2_test_lat = r2_score(y_test_lat, y_predtest_lat)\n",
    "        r2_test_lon = r2_score(y_test_lon, y_predtest_lon)\n",
    "\n",
    "\n",
    "        # Store MSE in arrays\n",
    "        train_mse_lat[run] = mse_train_lat\n",
    "        train_mse_lon[run] = mse_train_lon\n",
    "        test_mse_lat[run] = mse_test_lat\n",
    "        test_mse_lon[run] = mse_test_lon\n",
    "        train_mae_lat[run] = mae_train_lat\n",
    "        train_mae_lon[run] = mae_train_lon\n",
    "        test_mae_lat[run] = mae_test_lat\n",
    "        test_mae_lon[run] = mae_test_lon\n",
    "        train_r2_lat[run] = r2_train_lat\n",
    "        train_r2_lon[run] = r2_train_lon\n",
    "        test_r2_lat[run] = r2_test_lat\n",
    "        test_r2_lon[run] = r2_test_lon\n",
    "        print(\"train MSE latitude: \", mse_train_lat)\n",
    "        print(\"train MSE longitude: \", mse_train_lon)\n",
    "        print(\"test MSE latitude: \", mse_test_lat)\n",
    "        print(\"test  MSE longitude: \", mse_test_lon)\n",
    "        print(\"train MAE latitude: \", mae_train_lat)\n",
    "        print(\"train MAE longitude: \", mae_train_lon)\n",
    "        print(\"test MAE latitude: \", mae_test_lat)\n",
    "        print(\"test  MAE longitude: \", mae_test_lon)\n",
    "        print(\"train R2 SCORE latitude: \", r2_train_lat)\n",
    "        print(\"train R2 SCORE longitude: \", r2_train_lon)\n",
    "        print(\"test R2 SCORE latitude: \", r2_test_lat)\n",
    "        print(\"test R2 SCORE longitude: \", r2_test_lon)\n",
    "\n",
    "    # Update the best MSE score and associated predictions\n",
    "    if mse_test_lat < Best_mse_lat:\n",
    "        Best_mse_lat = mse_test_lat\n",
    "\n",
    "    if mse_test_lon < Best_mse_lon:\n",
    "        Best_mse_lon = mse_test_lon\n",
    "    if mae_test_lat < Best_mae_lat:\n",
    "        Best_mae_lat = mae_test_lat\n",
    "    if mae_test_lon < Best_mae_lon:\n",
    "        Best_mae_lon = mae_test_lon\n",
    "    if r2_test_lat > Best_r2_lat:\n",
    "        Best_r2_lat = r2_test_lat\n",
    "\n",
    "    if r2_test_lon > Best_r2_lon:\n",
    "        Best_r2_lon = r2_test_lon\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Save the trained model\n",
    "    model.save(\"model_\" + model_name + '.h5')\n",
    "\n",
    "    # Calculate standard deviations of train and test MSEs\n",
    "    train_std1_lat = np.std(train_mse_lat)\n",
    "    train_std1_lon = np.std(train_mse_lon)\n",
    "    test_std1_lat = np.std(test_mse_lat)\n",
    "    test_std1_lon = np.std(test_mse_lon)\n",
    "\n",
    "    train_std2_lat = np.std(train_mae_lat)\n",
    "    train_std2_lon = np.std(train_mae_lon)\n",
    "    test_std2_lat = np.std(test_mae_lat)\n",
    "    test_std2_lon = np.std(test_mae_lon)\n",
    "\n",
    "    train_std3_lat = np.std(train_r2_lat)\n",
    "    train_std3_lon = np.std(train_r2_lon)\n",
    "    test_std3_lat = np.std(test_r2_lat)\n",
    "    test_std3_lon = np.std(test_r2_lon)\n",
    "\n",
    "\n",
    "    # Display experiment summary\n",
    "    print(\"Total time for\", Num_Exp, \"experiments\", time.time() - start_time)\n",
    "    print(\"MSE for latitude test data: \", test_mse_lat)\n",
    "    print(\"MSE for longitude test data: \", test_mse_lon)\n",
    "    print(\"MAE for latitude test data: \", test_mae_lat)\n",
    "    print(\"MAE for longitude test data: \", test_mae_lon)\n",
    "    print(\"R2 SCORE for latitude test data: \", test_r2_lat)\n",
    "    print(\"R2 SCORE for longitude test data: \", test_r2_lon)\n",
    "    print(\"Mean and std MSE latitude: \", np.mean(test_mse_lat), \"Std Dev latitude: \", test_std1_lat)\n",
    "    print(\"Mean and std MSE longitude: \", np.mean(test_mse_lon), \"Std Dev longitude: \", test_std1_lon)\n",
    "    print(\"Mean and std latitude: \", np.mean(test_mae_lat), \"Std Dev latitude: \", test_std2_lat)\n",
    "    print(\"Mean and std longitude: \", np.mean(test_mae_lon), \"Std Dev longitude: \", test_std2_lon)\n",
    "    print(\"Mean and std latitude: \", np.mean(test_r2_lat), \"Std Dev latitude: \", test_std3_lat)\n",
    "    print(\"Mean and std longitude: \", np.mean(test_r2_lon), \"Std Dev longitude: \", test_std3_lon)\n",
    "\n",
    "\n",
    "    # Return relevant information\n",
    "    return train_mse_lat,train_mse_lon,test_mse_lat,  test_mse_lon, train_std1_lat, train_std1_lon, test_std1_lat, test_std1_lon,  y_predicttrain, y_predicttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionaries to store results for each model\n",
    "for j in range(1):\n",
    "\n",
    "    n_steps_out = j + 1\n",
    "\n",
    "    print('---------------------------------------------------------')\n",
    "    print('Number of steps out:', n_steps_out)\n",
    "\n",
    "    # Loop over different LSTM models\n",
    "    x_train, y_train = np.asarray(X1), np.asarray(Y1)\n",
    "    x_test, y_test = np.asarray(test_X), np.asarray(test_Y)\n",
    "    x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 4))\n",
    "    x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 4))\n",
    "\n",
    "    # Call the LSTM regression function and retrieve results\n",
    "    train_mse_lat,train_mse_lon,test_mse_lat,  test_mse_lon, train_std_lat, train_std_lon, test_std_lat, test_std_lon, y_predicttrain, y_predicttest = lstm_regression(\n",
    "        \"gentrack\", x_train, x_test, y_train, y_test, 1, 5, 3, epochs, hidden_layers)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_predicttest[0])\n",
    "print(y_predicttrain[0])\n",
    "print(len(y_predicttest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_X)):\n",
    "    for j in range(len(test_X[i])):\n",
    "        # Remove the last element (1) from each sublist\n",
    "        test_X[i][j] = test_X[i][j][:-1]\n",
    "\n",
    "print(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_X)):\n",
    "  Xe.append(test_X[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_X)):\n",
    "  for j in range(4):\n",
    "    test_X[i].append(y_predicttest[i][j])\n",
    "print(len(test_X[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_X)):\n",
    "  Ye.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_X)):\n",
    "  Xe.append(test_X[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(Xe), len(Ye))\n",
    "print(Xe[0], Xe[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = []\n",
    "for i in range(len(Xe)):\n",
    "  k = []\n",
    "  for j in range(len(Xe[i])):\n",
    "    k.append(Xe[i][j][2])\n",
    "  Xs.append(k)\n",
    "print(len(Xs))\n",
    "print(Xs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list to a NumPy array, shuffle it, and then convert it back to a list\n",
    "idx = np.random.permutation(len(Xe))\n",
    "\n",
    "# Initialize lists to store shuffled data\n",
    "x_shuffled = []\n",
    "y_shuffled = []\n",
    "\n",
    "# Iterate through the shuffled indices\n",
    "for i in idx:\n",
    "    # Append shuffled data to the lists\n",
    "    x_shuffled.append(Xe[i])\n",
    "    y_shuffled.append(Ye[i])\n",
    "\n",
    "Xe = x_shuffled\n",
    "Ye = y_shuffled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Xe1 = Xe\n",
    "Ye1 = Ye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(Ye)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the training limit as 75% of the total length of X\n",
    "train_limit_gen = int(len(Xe) * 0.75)\n",
    "\n",
    "# Display the calculated training limit\n",
    "print(\"Training set size:\", train_limit_gen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract test data for evaluation\n",
    "test_X_org = Xe[train_limit_gen + 1:]\n",
    "test_Y_org = Ye[train_limit_gen + 1:]\n",
    "\n",
    "# Display the lengths of the datasets\n",
    "print(\"Length of X:\", len(Xe))\n",
    "print(\"Length of Y:\", len(Ye))\n",
    "print(\"Length of Test X (for evaluation):\", len(test_X_org))\n",
    "print(\"Length of Test Y (for evaluation):\", len(test_Y_org))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_x = Xe\n",
    "test_Xe = Xe[train_limit_gen+1:]\n",
    "test_Xe = np.asarray(test_Xe).astype(float)\n",
    "test_Ye = Ye[train_limit_gen+1:]\n",
    "Xe = Xe[:train_limit_gen]\n",
    "Xe = np.asarray(Xe).astype(float)\n",
    "Ye = Ye[:train_limit_gen]\n",
    "print(len(test_Xe), len(test_Ye))\n",
    "len(Xe), len(Ye)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define whether it's a univariate or multivariate case\n",
    "univariate = False  # If True, it's a multivariate case\n",
    "\n",
    "# Define sequence and time step parameters\n",
    "n_steps_in = 8 # Adjust based on the number of time steps you want to consider for input\n",
    "n_seq = 3 # Adjust based on your specific needs\n",
    "n_steps_out = 1  # Number of output time steps\n",
    "\n",
    "# Define the number of features for input and output\n",
    "n_features_in = 3  # Latitude, Longitude, Speed\n",
    "n_features_out = 2  # 'category'\n",
    "\n",
    "# Define the number of hidden layers in the model\n",
    "hidden_layers = 50\n",
    "\n",
    "# Define training parameters\n",
    "epochs = 100\n",
    "No_exp = 30 # Number of experiments\n",
    "\n",
    "# Display the configuration\n",
    "\n",
    "print(f\"Univariate: {univariate}\")\n",
    "print(f\"Number of Input Time Steps: {n_steps_in}\")\n",
    "print(f\"Number of Input Sequences: {n_seq}\")\n",
    "print(f\"Number of Output Time Steps: {n_steps_out}\")\n",
    "print(f\"Number of Input Features: {n_features_in}\")\n",
    "print(f\"Number of Output Features: {n_features_out}\")\n",
    "print(f\"Number of Hidden Layers: {hidden_layers}\")\n",
    "print(f\"Epochs: {epochs}\")\n",
    "print(f\"Number of Experiments: {No_exp}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def multivariate_vanilla_lstm(n_steps_in, n_steps_out, n_features_in, n_features_out):\n",
    "    \"\"\"\n",
    "    Create a Multivariate LSTM model for sequence prediction.\n",
    "\n",
    "    Parameters:\n",
    "    - n_steps_in: Number of time steps in the input sequence.\n",
    "    - n_steps_out: Number of time steps in the output sequence.\n",
    "    - n_features_in: Number of input features.\n",
    "    - n_features_out: Number of output features.\n",
    "\n",
    "    Returns:\n",
    "    - model: Compiled Multivariate LSTM model.\n",
    "    \"\"\"\n",
    "    # Initialize a sequential model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add an LSTM layer with 50 units and ReLU activation\n",
    "    model.add(LSTM(50, activation='relu', input_shape=(n_steps_in, n_features_in)))\n",
    "\n",
    "    # Add a Dense layer with a single unit and sigmoid activation for binary classification\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    # Compile the model using Adam optimizer and binary crossentropy loss\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# all models\n",
    "def lstm(model_name, method, univariate, x_train, x_test, y_train, y_test, Num_Exp, n_steps_in, n_steps_out, Epochs, Hidden):\n",
    "    # Initialize arrays to store accuracy metrics\n",
    "    train_acc = np.zeros(Num_Exp)\n",
    "    test_acc = np.zeros(Num_Exp)\n",
    "\n",
    "    # Choose the appropriate model based on the specified model_name\n",
    "    if model_name == 'vanilla':\n",
    "        model = multivariate_vanilla_lstm(n_steps_in, n_steps_out, n_features_in, n_features_out)\n",
    "    elif model_name == 'bidirectional':\n",
    "        model = bidirectional_lstm(n_steps_in, n_steps_out, n_features_in, n_features_out)\n",
    "    elif model_name == 'cnn-lstm':\n",
    "        model = cnn_lstm(n_steps_in, n_steps_out, n_features_in, n_features_out, n_seq)\n",
    "    elif model_name == 'conv-lstm':\n",
    "        model = conv_lstm(n_steps_in, n_steps_out, n_features_in, n_features_out, n_seq)\n",
    "    else:\n",
    "        # Handle the case where model_name is not recognized (add appropriate logic or raise an error)\n",
    "        raise ValueError(f\"Unsupported model_name: {model_name}\")\n",
    "\n",
    "    # Display the model summary\n",
    "    model.summary()\n",
    "\n",
    "    # Initialize arrays to store predictions and classification reports\n",
    "    y_predicttest_allruns = np.zeros([Num_Exp, x_test.shape[0], y_test.shape[1]])\n",
    "    Best_f1 = 0  # Initialize the best F1 score\n",
    "\n",
    "    # Extract the actual classes from one-hot encoded vectors for both test and train sets\n",
    "    act_test = [y_test[i].argmax() for i in range(y_test.shape[0])]\n",
    "    act_train = [y_train[i].argmax() for i in range(y_train.shape[0])]\n",
    "\n",
    "    # Initialize dictionaries to store classification reports\n",
    "    Best_report_train = dict()\n",
    "    Best_report_test = dict()\n",
    "    all_report_train = dict()\n",
    "    all_report_test = dict()\n",
    "\n",
    "    # Loop through experiment runs\n",
    "    start_time = time.time()\n",
    "    for run in range(Num_Exp):\n",
    "        print(\"Experiment\", run + 1, \"in progress\")\n",
    "\n",
    "        # Fit the model\n",
    "        model.fit(x_train, y_train, epochs=Epochs, batch_size=10, verbose=0, shuffle=False)\n",
    "        y_predicttrain = model.predict(x_train)\n",
    "        y_predicttest = model.predict(x_test)\n",
    "\n",
    "        # Extract predicted classes from one-hot encoded vectors\n",
    "        pred_test = [y_predicttest[i].argmax() for i in range(y_predicttest.shape[0])]\n",
    "        pred_train = [y_predicttrain[i].argmax() for i in range(y_predicttrain.shape[0])]\n",
    "\n",
    "        # Generate classification reports\n",
    "        report_train = classification_report(act_train, pred_train, labels=[0, 1], output_dict=True)\n",
    "        report_test = classification_report(act_test, pred_test, labels=[0, 1], output_dict=True)\n",
    "\n",
    "        # Store classification reports in dictionaries\n",
    "        all_report_train[run] = report_train\n",
    "        all_report_test[run] = report_test\n",
    "\n",
    "        # Calculate F1-score for the test set\n",
    "        test_acc[run] = report_test['1']['f1-score']\n",
    "        print(\"train acc: \", report_train['1']['f1-score'])\n",
    "        print(\"test acc: \", test_acc[run])\n",
    "\n",
    "        precision_minority = report_test['1']['precision']\n",
    "        recall_minority = report_test['1']['recall']\n",
    "        print(\"Precision (Minority Class):\", precision_minority)\n",
    "        print(\"Recall (Minority Class):\", recall_minority)\n",
    "\n",
    "        macro_avg_precision = report_test['macro avg']['precision']\n",
    "        macro_avg_recall = report_test['macro avg']['recall']\n",
    "        macro_avg_f1 = report_test['macro avg']['f1-score']\n",
    "\n",
    "        weighted_avg_precision = report_test['weighted avg']['precision']\n",
    "        weighted_avg_recall = report_test['weighted avg']['recall']\n",
    "        weighted_avg_f1 = report_test['weighted avg']['f1-score']\n",
    "\n",
    "        # Print or store Macro Avg and Weighted Avg values\n",
    "        print(\"Macro Avg Precision:\", macro_avg_precision)\n",
    "        print(\"Macro Avg Recall:\", macro_avg_recall)\n",
    "        print(\"Macro Avg F1-Score:\", macro_avg_f1)\n",
    "\n",
    "        print(\"Weighted Avg Precision:\", weighted_avg_precision)\n",
    "        print(\"Weighted Avg Recall:\", weighted_avg_recall)\n",
    "        print(\"Weighted Avg F1-Score:\", weighted_avg_f1)\n",
    "\n",
    "\n",
    "\n",
    "        # Update the best F1 score and associated predictions and reports\n",
    "        if test_acc[run] > Best_f1:\n",
    "            Best_f1 = test_acc[run]\n",
    "            Best_Predict_Test = y_predicttest\n",
    "            Best_report_train, Best_report_test = report_train, report_test\n",
    "\n",
    "    # Save the trained model\n",
    "    model.save(\"model_\" + ocean + \"_\" + model_name + \"_\" + method + '.h5')\n",
    "\n",
    "    # Calculate standard deviations of train and test accuracies\n",
    "    train_std = np.std(train_acc)\n",
    "    test_std = np.std(test_acc)\n",
    "\n",
    "    # Display experiment summary\n",
    "    print(\"Total time for\", Num_Exp, \"experiments\", time.time() - start_time)\n",
    "    print(\"F1 scores for test data: \", test_acc)\n",
    "    print(\"Mean: \", np.mean(test_acc), \"Std Dev: \", test_std)\n",
    "\n",
    "    # Return relevant information\n",
    "    return train_acc, test_acc, train_std, test_std, Best_Predict_Test, y_predicttrain, y_predicttest, all_report_train, all_report_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random permutation of indices for shuffling\n",
    "idx = np.random.permutation(len(Xe))\n",
    "\n",
    "# Initialize lists to store shuffled data\n",
    "x_shuffled = []\n",
    "y_shuffled = []\n",
    "\n",
    "# Iterate through the shuffled indices\n",
    "for i in idx:\n",
    "    # Append shuffled data to the lists\n",
    "    x_shuffled.append(Xe[i])\n",
    "    y_shuffled.append(Ye[i])\n",
    "x_a = np.array(x_shuffled)\n",
    "print(x_a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the shuffled labels to one-hot encoded format for training data\n",
    "Y_hot_encoded_train = np.asarray(to_categorical(y_shuffled))\n",
    "\n",
    "# Convert the test labels to one-hot encoded format\n",
    "Y_hot_encoded_test = np.asarray(to_categorical(test_Ye))\n",
    "\n",
    "# Print the shapes of the one-hot encoded training and test labels\n",
    "print(Y_hot_encoded_train.shape, Y_hot_encoded_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_shuffled[0], test_Xe[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing on all 4 different kinds of LSTMs\n",
    "models = ['vanilla']\n",
    "\n",
    "\n",
    "# Initialize dictionaries to store predictions, actual values, and metrics for training and testing\n",
    "predictions_train = dict()\n",
    "actual_train = dict()\n",
    "predictions_test = dict()\n",
    "actual_test = dict()\n",
    "metrics_train = dict()\n",
    "metrics_test = dict()\n",
    "test_acc_all = dict()\n",
    "test_stddev = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionaries to store results for each model\n",
    "for j in range(1):\n",
    "\n",
    "  predictions_train_per_step = dict()\n",
    "  actual_train_per_step = dict()\n",
    "  predictions_test_per_step = dict()\n",
    "  actual_test_per_step = dict()\n",
    "  metrics_train_per_step = dict()\n",
    "  metrics_test_per_step = dict()\n",
    "  test_acc_per_step = dict()\n",
    "  test_stddev_per_step = dict()\n",
    "  n_steps_out = j + 1\n",
    "\n",
    "  print('---------------------------------------------------------')\n",
    "  print('Number of steps out:', n_steps_out)\n",
    "\n",
    "  # Loop over different LSTM models\n",
    "  for i in models:\n",
    "    print(\"For \" + i + \":\")\n",
    "\n",
    "    # Reshape data based on the LSTM model type\n",
    "    if i == 'vanilla' or i == 'bidirectional' :\n",
    "        x_train, y_train = np.asarray(x_shuffled), np.asarray(Y_hot_encoded_train)\n",
    "        x_test, y_test = np.asarray(test_Xe), np.asarray(Y_hot_encoded_test)\n",
    "        x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], n_features_in))\n",
    "        x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], n_features_in))\n",
    "    elif i == 'cnn-lstm':\n",
    "        x_train, y_train = np.asarray(x_shuffled), np.asarray(Y_hot_encoded_train)\n",
    "        x_test, y_test = np.asarray(test_X), np.asarray(Y_hot_encoded_test)\n",
    "        x_train = x_train.reshape((x_train.shape[0], n_seq, int(n_steps_in / n_seq), n_features_in))\n",
    "        x_test = x_test.reshape((x_test.shape[0], n_seq, int(n_steps_in / n_seq), n_features_in))\n",
    "    elif i == 'conv-lstm':\n",
    "        print(\"Original shape of x_shuffled:\", np.array(x_shuffled).shape)\n",
    "        x_train, y_train = np.asarray(x_shuffled), np.asarray(Y_hot_encoded_train)\n",
    "        print(\"Shape after reshape:\", np.array(x_train).shape)\n",
    "        x_test, y_test = np.asarray(test_X), np.asarray(Y_hot_encoded_test)\n",
    "        x_train = x_train.reshape((x_train.shape[0], n_seq, 1, int(n_steps_in/n_seq), n_features_in))\n",
    "        x_test = x_test.reshape(x_test.shape[0], n_seq, 1, int(n_steps_in/n_seq), n_features_in)\n",
    "\n",
    "    # Call the LSTM function and retrieve results\n",
    "    train_acc, test_acc, train_std_dev, test_std_dev, Best_Predict_Test, y_predicttrain, y_predicttest, report_train, report_test = lstm(\n",
    "        i, 'original', univariate, x_train, x_test, y_train, y_test, No_exp, n_steps_in, n_steps_out, epochs, hidden_layers)\n",
    "\n",
    "    # Store results in respective dictionaries\n",
    "    predictions_train_per_step[i] = y_predicttrain\n",
    "    actual_train_per_step[i] = y_train\n",
    "    predictions_test_per_step[i] = Best_Predict_Test\n",
    "    actual_test_per_step[i] = y_test\n",
    "    metrics_train_per_step[i] = report_train\n",
    "    metrics_test_per_step[i] = report_test\n",
    "    test_acc_per_step[i] = test_acc\n",
    "    test_stddev_per_step[i] = test_std_dev\n",
    "\n",
    "# Store results for the current n_steps_out in the overall dictionaries\n",
    "  predictions_train[str(j + 1)] = predictions_train_per_step\n",
    "  actual_train[str(j + 1)] = actual_train_per_step\n",
    "  predictions_test[str(j + 1)] = predictions_test_per_step\n",
    "  actual_test[str(j + 1)] = actual_test_per_step\n",
    "  metrics_train[str(j + 1)] = metrics_train_per_step\n",
    "  metrics_test[str(j + 1)] = metrics_test_per_step\n",
    "  test_acc_all[str(j + 1)] = test_acc_per_step\n",
    "  test_stddev[str(j + 1)] = test_stddev_per_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"predictions_\" + ocean + '_original' + '.pkl', 'wb') as f:\n",
    "    pickle.dump([predictions_train,actual_train,predictions_test,actual_test,metrics_train,metrics_test,test_acc,test_stddev], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def make_confusion_matrix_chart(cf_matrix, name):\n",
    "    \"\"\"\n",
    "    Create and save a well-formatted confusion matrix heatmap.\n",
    "\n",
    "    Parameters:\n",
    "    - cf_matrix: Confusion matrix\n",
    "    - name: Name of the file to save the plot\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "\n",
    "    # Set a Seaborn style with dark grid\n",
    "    sns.set(style=\"darkgrid\", font_scale=1.5)\n",
    "\n",
    "    # Create a figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "    # Customize the appearance of the heatmap with green colors\n",
    "    sns.heatmap(cf_matrix, annot=True, fmt='d', cmap='Greens',\n",
    "                linewidths=.5, square=True, cbar=False,\n",
    "                annot_kws={\"size\": 16}, ax=ax)\n",
    "\n",
    "    # Set labels and title\n",
    "    plt.ylabel(\"Actual\", size=18)\n",
    "    plt.xlabel(\"Predicted\", size=18)\n",
    "    plt.title(\"Confusion Matrix\", size=20)\n",
    "\n",
    "    # Set tick parameters\n",
    "    ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "    # Save the plot as an image\n",
    "    plt.savefig(name + '.png', dpi=300, transparent=False, bbox_inches='tight')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "    # Return None\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [i.argmax() for i in actual_test_per_step['vanilla']]\n",
    "pred = [i.argmax() for i in predictions_test_per_step['vanilla']]\n",
    "cf_matrix_test = confusion_matrix(y, pred)\n",
    "make_confusion_matrix_chart(cf_matrix_test, ocean + 'vanilla_cm_original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store precision, recall, and F1-score metrics for class 0, class 1, overall accuracy,\n",
    "# macro average, and weighted average across multiple experiments\n",
    "precision0 = []\n",
    "precision1 = []\n",
    "precisionacc = []\n",
    "precisionmacavg = []\n",
    "precisionweighavg = []\n",
    "recall0 = []\n",
    "recall1 = []\n",
    "recallacc = []\n",
    "recallmacavg = []\n",
    "recallweighavg = []\n",
    "f10 = []\n",
    "f11 = []\n",
    "f1acc = []\n",
    "f1macavg = []\n",
    "f1weighavg = []\n",
    "\n",
    "# Loop through the results of multiple experiments\n",
    "for i in range(No_exp):\n",
    "    # Append precision, recall, and F1-score metrics for class 0, class 1, overall accuracy,\n",
    "    # macro average, and weighted average from the model results\n",
    "    precision0.append(metrics_test_per_step['vanilla'][i]['0']['precision'])\n",
    "    precision1.append(metrics_test_per_step['vanilla'][i]['1']['precision'])\n",
    "    precisionacc.append(metrics_test_per_step['vanilla'][i]['accuracy'])\n",
    "    precisionmacavg.append(metrics_test_per_step['vanilla'][i]['macro avg']['precision'])\n",
    "    precisionweighavg.append(metrics_test_per_step['vanilla'][i]['weighted avg']['precision'])\n",
    "\n",
    "    recall0.append(metrics_test_per_step['vanilla'][i]['0']['recall'])\n",
    "    recall1.append(metrics_test_per_step['vanilla'][i]['1']['recall'])\n",
    "    recallacc.append(metrics_test_per_step['vanilla'][i]['accuracy'])\n",
    "    recallmacavg.append(metrics_test_per_step['vanilla'][i]['macro avg']['recall'])\n",
    "    recallweighavg.append(metrics_test_per_step['vanilla'][i]['weighted avg']['recall'])\n",
    "\n",
    "    f10.append(metrics_test_per_step['vanilla'][i]['0']['f1-score'])\n",
    "    f11.append(metrics_test_per_step['vanilla'][i]['1']['f1-score'])\n",
    "    f1acc.append(metrics_test_per_step['vanilla'][i]['accuracy'])\n",
    "    f1macavg.append(metrics_test_per_step['vanilla'][i]['macro avg']['f1-score'])\n",
    "    f1weighavg.append(metrics_test_per_step['vanilla'][i]['weighted avg']['f1-score'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(round(np.mean(precision0),4)) + \"±\" + str(round(np.std(precision0),4)),\" & \" + str(round(np.mean(recall0),4)) + \"±\" + str(round(np.std(recall0),4)), \" & \" + str(round(np.mean(f10),4)) + \"±\" + str(round(np.std(f10),4)))\n",
    "print(str(round(np.mean(precision1),4)) + \"±\" + str(round(np.std(precision1),4)),\" & \" + str(round(np.mean(recall1),4)) + \"±\" + str(round(np.std(recall1),4)), \" & \" + str(round(np.mean(f11),4)) + \"±\" + str(round(np.std(f11),4)))\n",
    "print(str(round(np.mean(precisionacc),4)) + \"±\" + str(round(np.std(precisionacc),4)),\" & \" + str(round(np.mean(recallacc),4)) + \"±\" + str(round(np.std(recallacc),4)), \" & \" + str(round(np.mean(f1acc),4)) + \"±\" + str(round(np.std(f1acc),4)))\n",
    "print(str(round(np.mean(precisionmacavg),4)) + \"±\" + str(round(np.std(precisionmacavg),4)),\" & \" + str(round(np.mean(recallmacavg),4)) + \"±\" + str(round(np.std(recallmacavg),4)), \" & \" + str(round(np.mean(f1macavg),4)) + \"±\" + str(round(np.std(f1macavg),4)))\n",
    "print(str(round(np.mean(precisionweighavg),4)) + \"±\" + str(round(np.std(precisionweighavg),4)),\" & \" + str(round(np.mean(recallweighavg),4)) + \"±\" + str(round(np.std(recallweighavg),4)), \" & \" + str(round(np.mean(f1weighavg),4)) + \"±\" + str(round(np.std(f1weighavg),4)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list to a NumPy array, shuffle it, and then convert it back to a list\n",
    "idx = np.random.permutation(len(Xs))\n",
    "\n",
    "# Initialize lists to store shuffled data\n",
    "x_shuffled = []\n",
    "y_shuffled = []\n",
    "\n",
    "# Iterate through the shuffled indices\n",
    "for i in idx:\n",
    "    # Append shuffled data to the lists\n",
    "    x_shuffled.append(Xs[i])\n",
    "    y_shuffled.append(Ye[i])\n",
    "\n",
    "Xs = x_shuffled\n",
    "Ys = y_shuffled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the training limit as 75% of the total length of X\n",
    "train_limit_gen = int(len(Xs) * 0.75)\n",
    "\n",
    "# Display the calculated training limit\n",
    "print(\"Training set size:\", train_limit_gen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract test data for evaluation\n",
    "test_X_org = Xs[train_limit_gen + 1:]\n",
    "test_Y_org = Ys[train_limit_gen + 1:]\n",
    "\n",
    "# Display the lengths of the datasets\n",
    "print(\"Length of X:\", len(Xs))\n",
    "print(\"Length of Y:\", len(Ys))\n",
    "print(\"Length of Test X (for evaluation):\", len(test_X_org))\n",
    "print(\"Length of Test Y (for evaluation):\", len(test_Y_org))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Xs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the original X (speed) values before splitting\n",
    "speed_x = Xs\n",
    "test_X = Xs[train_limit_gen+1:]\n",
    "test_X = np.asarray(test_X).astype(float)\n",
    "test_Y = Ys[train_limit_gen+1:]\n",
    "X = Xs[:train_limit_gen]\n",
    "X = np.asarray(X).astype(float)\n",
    "Y = Ys[:train_limit_gen]\n",
    "print(len(test_X), len(test_Y))\n",
    "len(X), len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the class distribution for training and test data\n",
    "counter_train = Counter(Ys)\n",
    "counter_test = Counter(test_Y)\n",
    "\n",
    "# Display the class distribution for training data\n",
    "print(\"Class Distribution - Training Data:\")\n",
    "print(counter_train)\n",
    "\n",
    "# Display the class distribution for test data\n",
    "print(\"\\nClass Distribution - Test Data:\")\n",
    "print(counter_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vanilla_lstm(n_steps_in, n_steps_out, n_features_in, n_features_out):\n",
    "    \"\"\"\n",
    "    Create a Vanilla LSTM model for sequence prediction.\n",
    "\n",
    "    Parameters:\n",
    "    - n_steps_in: Number of time steps in the input sequence.\n",
    "    - n_steps_out: Number of time steps in the output sequence.\n",
    "    - n_features_in: Number of input features.\n",
    "    - n_features_out: Number of output features.\n",
    "\n",
    "    Returns:\n",
    "    - model: Compiled Vanilla LSTM model.\n",
    "    \"\"\"\n",
    "    # Initialize a sequential model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add an LSTM layer with 50 units and ReLU activation\n",
    "    model.add(LSTM(50, activation='relu', input_shape=(n_steps_in, n_features_in)))\n",
    "\n",
    "    # Add a Dense output layer with softmax activation for classification\n",
    "    # Adjust the activation based on the nature of the task (e.g., 'linear' for regression)\n",
    "    model.add(Dense(units=n_features_out, activation='softmax'))\n",
    "\n",
    "    # Compile the model using Adam optimizer and binary crossentropy loss\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "univariate = True  # If False, it's a multivariate case\n",
    "\n",
    "# Define sequence and time step parameters\n",
    "n_steps_in = 8\n",
    "n_seq = 2\n",
    "n_steps_out = 1\n",
    "\n",
    "# Define the number of features for input and output\n",
    "n_features_in = 1  # Speed\n",
    "n_features_out = 2  # One-hot encoding of category\n",
    "\n",
    "# Define the number of hidden layers in the model\n",
    "hidden_layers = 50\n",
    "\n",
    "# Define training parameters\n",
    "epochs = 100\n",
    "No_exp = 30 # Number of experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all models\n",
    "def lstm(model_name, method, univariate, x_train, x_test, y_train, y_test, Num_Exp, n_steps_in, n_steps_out, Epochs, Hidden):\n",
    "    # Initialize arrays to store accuracy metrics\n",
    "    train_acc = np.zeros(Num_Exp)\n",
    "    test_acc = np.zeros(Num_Exp)\n",
    "\n",
    "    # Choose the appropriate model based on the specified model_name\n",
    "    if model_name == 'vanilla':\n",
    "        model = vanilla_lstm(n_steps_in, n_steps_out, n_features_in, n_features_out)\n",
    "    elif model_name == 'bidirectional':\n",
    "        model = bidirectional_lstm(n_steps_in, n_steps_out, n_features_in, n_features_out)\n",
    "    elif model_name == 'cnn-lstm':\n",
    "        model = cnn_lstm(n_steps_in, n_steps_out, n_features_in, n_features_out, n_seq)\n",
    "    elif model_name == 'conv-lstm':\n",
    "        model = conv_lstm(n_steps_in, n_steps_out, n_features_in, n_features_out, n_seq)\n",
    "\n",
    "    # Display the model summary\n",
    "    model.summary()\n",
    "\n",
    "    # Initialize arrays to store predictions and classification reports\n",
    "    y_predicttest_allruns = np.zeros([Num_Exp, x_test.shape[0], y_test.shape[1]])\n",
    "    Best_f1 = 0  # Initialize the best F1 score\n",
    "    Best_Predict_Test = 0\n",
    "\n",
    "    # Extract the actual classes from one-hot encoded vectors for both test and train sets\n",
    "    act_test = [y_test[i].argmax() for i in range(y_test.shape[0])]\n",
    "    act_train = [y_train[i].argmax() for i in range(y_train.shape[0])]\n",
    "\n",
    "    # Initialize dictionaries to store classification reports\n",
    "    Best_report_train = dict()\n",
    "    Best_report_test = dict()\n",
    "    all_report_train = dict()\n",
    "    all_report_test = dict()\n",
    "\n",
    "    # Loop through experiment runs\n",
    "    start_time = time.time()\n",
    "    for run in range(Num_Exp):\n",
    "        print(\"Experiment\", run + 1, \"in progress\")\n",
    "\n",
    "        # Fit the model\n",
    "        model.fit(x_train, y_train, epochs=Epochs, batch_size=10, verbose=0, shuffle=False)\n",
    "        y_predicttrain = model.predict(x_train)\n",
    "        y_predicttest = model.predict(x_test)\n",
    "\n",
    "        # Extract predicted classes from one-hot encoded vectors\n",
    "        pred_test = [y_predicttest[i].argmax() for i in range(y_predicttest.shape[0])]\n",
    "        pred_train = [y_predicttrain[i].argmax() for i in range(y_predicttrain.shape[0])]\n",
    "\n",
    "        # Generate classification reports\n",
    "        report_train = classification_report(act_train, pred_train, labels=[0, 1], output_dict=True)\n",
    "        report_test = classification_report(act_test, pred_test, labels=[0, 1], output_dict=True)\n",
    "\n",
    "        # Store classification reports in dictionaries\n",
    "        all_report_train[run] = report_train\n",
    "        all_report_test[run] = report_test\n",
    "\n",
    "        # Calculate F1-score for the test set\n",
    "        test_acc[run] = report_test['1']['f1-score']\n",
    "        print(\"train acc: \", report_train['1']['f1-score'])\n",
    "        print(\"test acc: \", test_acc[run])\n",
    "\n",
    "        # Update the best F1 score and associated predictions and reports\n",
    "        if test_acc[run] > Best_f1:\n",
    "            Best_f1 = test_acc[run]\n",
    "            Best_Predict_Test = y_predicttest\n",
    "            Best_report_train, Best_report_test = report_train, report_test\n",
    "\n",
    "    # Save the trained model\n",
    "    model.save(\"model_\" + ocean + \"_\" + model_name + \"_\" + method + '.h5')\n",
    "\n",
    "    # Calculate standard deviations of train and test accuracies\n",
    "    train_std = np.std(train_acc)\n",
    "    test_std = np.std(test_acc)\n",
    "\n",
    "    # Display experiment summary\n",
    "    print(\"Total time for\", Num_Exp, \"experiments\", time.time() - start_time)\n",
    "    print(\"F1 scores for test data: \", test_acc)\n",
    "    print(\"Mean: \", np.mean(test_acc), \"Std Dev: \", test_std)\n",
    "\n",
    "    # Return relevant information\n",
    "    return train_acc, test_acc, train_std, test_std, Best_Predict_Test, y_predicttrain, y_predicttest, all_report_train, all_report_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random permutation of indices for shuffling\n",
    "# idx = np.random.permutation(len(X_smote))\n",
    "idx = np.random.permutation(len(X))\n",
    "print(len(idx))\n",
    "\n",
    "# Initialize lists to store shuffled data\n",
    "x_shuffled = []\n",
    "y_shuffled = []\n",
    "\n",
    "# Iterate through the shuffled indices\n",
    "for i in idx:\n",
    "    # Append shuffled data to the lists\n",
    "    # x_shuffled.append(X_smote[i])\n",
    "    # y_shuffled.append(Y_smote[i])\n",
    "    x_shuffled.append(X[i])\n",
    "    y_shuffled.append(Y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the shuffled labels to one-hot encoded format for training data\n",
    "Y_hot_encoded_train = np.asarray(to_categorical(y_shuffled))\n",
    "\n",
    "# Convert the test labels to one-hot encoded format\n",
    "Y_hot_encoded_test = np.asarray(to_categorical(test_Y))\n",
    "\n",
    "# Print the shapes of the one-hot encoded training and test labels\n",
    "print(Y_hot_encoded_train.shape, Y_hot_encoded_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_shuffled), len(test_X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing on all 4 different kinds of LSTMs\n",
    "models = ['vanilla']\n",
    "\n",
    "\n",
    "# Initialize dictionaries to store predictions, actual values, and metrics for training and testing\n",
    "predictions_train = dict()\n",
    "actual_train = dict()\n",
    "predictions_test = dict()\n",
    "actual_test = dict()\n",
    "metrics_train = dict()\n",
    "metrics_test = dict()\n",
    "test_acc_all = dict()\n",
    "test_stddev = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over different values of n_steps_out (in this case, only 1)\n",
    "for j in range(1):\n",
    "    # Initialize dictionaries to store results for each model\n",
    "    predictions_train_per_step = dict()\n",
    "    actual_train_per_step = dict()\n",
    "    predictions_test_per_step = dict()\n",
    "    actual_test_per_step = dict()\n",
    "    metrics_train_per_step = dict()\n",
    "    metrics_test_per_step = dict()\n",
    "    test_acc_per_step = dict()\n",
    "    test_stddev_per_step = dict()\n",
    "    n_steps_out = j + 1\n",
    "\n",
    "    print('---------------------------------------------------------')\n",
    "    print('Number of steps out:', n_steps_out)\n",
    "\n",
    "    # Loop over different LSTM models\n",
    "    for i in models:\n",
    "        print(\"For \" + i + \":\")\n",
    "\n",
    "        # Reshape data based on the LSTM model type\n",
    "        if i == 'vanilla' or i == 'bidirectional':\n",
    "            x_train, y_train = np.asarray(x_shuffled), np.asarray(Y_hot_encoded_train)\n",
    "            x_test, y_test = np.asarray(test_X), np.asarray(Y_hot_encoded_test)\n",
    "            x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], n_features_in))\n",
    "            x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], n_features_in))\n",
    "        elif i == 'cnn-lstm':\n",
    "            x_train, y_train = np.asarray(x_shuffled), np.asarray(Y_hot_encoded_train)\n",
    "            x_test, y_test = np.asarray(test_X), np.asarray(Y_hot_encoded_test)\n",
    "            x_train = x_train.reshape((x_train.shape[0], n_seq, int(n_steps_in / n_seq), n_features_in))\n",
    "            x_test = x_test.reshape((x_test.shape[0], n_seq, int(n_steps_in / n_seq), n_features_in))\n",
    "        elif i == 'conv-lstm':\n",
    "            x_train, y_train = np.asarray(x_shuffled), np.asarray(Y_hot_encoded_train)\n",
    "            x_test, y_test = np.asarray(test_X), np.asarray(Y_hot_encoded_test)\n",
    "            x_train = x_train.reshape((x_train.shape[0], n_seq, 1, int(n_steps_in / n_seq), n_features_in))\n",
    "            x_test = x_test.reshape((x_test.shape[0], n_seq, 1, int(n_steps_in / n_seq), n_features_in))\n",
    "\n",
    "        # Call the LSTM function and retrieve results\n",
    "        train_acc, test_acc, train_std_dev, test_std_dev, Best_Predict_Test, y_predicttrain, y_predicttest, report_train, report_test = lstm(\n",
    "            i, 'original', univariate, x_train, x_test, y_train, y_test, No_exp, n_steps_in, n_steps_out, epochs, hidden_layers)\n",
    "\n",
    "        # Store results in respective dictionaries\n",
    "        predictions_train_per_step[i] = y_predicttrain\n",
    "        actual_train_per_step[i] = y_train\n",
    "        predictions_test_per_step[i] = Best_Predict_Test\n",
    "        actual_test_per_step[i] = y_test\n",
    "        metrics_train_per_step[i] = report_train\n",
    "        metrics_test_per_step[i] = report_test\n",
    "        test_acc_per_step[i] = test_acc\n",
    "        test_stddev_per_step[i] = test_std_dev\n",
    "\n",
    "    # Store results for the current n_steps_out in the overall dictionaries\n",
    "    predictions_train[str(j + 1)] = predictions_train_per_step\n",
    "    actual_train[str(j + 1)] = actual_train_per_step\n",
    "    predictions_test[str(j + 1)] = predictions_test_per_step\n",
    "    actual_test[str(j + 1)] = actual_test_per_step\n",
    "    metrics_train[str(j + 1)] = metrics_train_per_step\n",
    "    metrics_test[str(j + 1)] = metrics_test_per_step\n",
    "    test_acc_all[str(j + 1)] = test_acc_per_step\n",
    "    test_stddev[str(j + 1)] = test_stddev_per_step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
