{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "master_slave_GAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOAN2U2x9JrA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b24beda-1caa-4ae6-d40d-05bd6a93b062"
      },
      "source": [
        "import copy\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Activation\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Bidirectional\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from numpy import hstack\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import datetime\n",
        "import time\n",
        "import joblib\n",
        "from datetime import timedelta, date\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from numpy import array\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import seaborn as sns; sns.set_theme() \n",
        "import errno\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import ConvLSTM2D\n",
        "from keras.models import load_model\n",
        "import pickle\n",
        "from sklearn.metrics import accuracy_score, roc_curve, auc, classification_report, confusion_matrix\n",
        "from scipy import interp\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "import imblearn\n",
        "import collections\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import f1_score\n",
        "from matplotlib import pyplot"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPL7ejJ69Qvo"
      },
      "source": [
        "def load_data_south_indian(url):\n",
        "  df = pd.read_csv(url)\n",
        "  #df.columns = ['id','date','longitude','latitude','speed']\n",
        "  #df = df.drop(['date'], axis = 1)\n",
        "  df['category'] = df['Speed(knots)'].apply(lambda x: \n",
        "  0 if x<=33 else 1  if x<=47 and x>=34 else 2 if x<=63 and x>=48 else 3 if x<=89 and x>=64 else 4 if x<=115 and x>=90 else 5 )\n",
        "  return df\n",
        "\n",
        "def load_data_south_pacific(url):\n",
        "  df = pd.read_csv(url)\n",
        "  #df.columns = ['id','date','longitude','latitude','speed']\n",
        "  #df = df.drop(['date'], axis = 1)\n",
        "  df['category'] = df['Speed(knots)'].apply(lambda x: \n",
        "  0 if x<=33 else 1  if x<=47 and x>=34 else 2 if x<=63 and x>=48 else 3 if x<=85 and x>=64 else 4 if x<=107 and x>=86 else 5 )\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EvJumXX9WWC",
        "outputId": "8e698ce3-6b2c-4920-8454-18ebce1461b7"
      },
      "source": [
        "ocean = 'south_indian'  #south_indian or south_pacific\n",
        "print(ocean)\n",
        "\n",
        "if ocean == 'south_indian':\n",
        "    url_data = 'https://raw.githubusercontent.com/sydney-machine-learning/cyclonedatasets/main/SouthIndian-SouthPacific-Ocean/South_indian_hurricane.csv'\n",
        "    function = load_data_south_indian\n",
        "    hot_encoded_result_file_name = 'south_indian'\n",
        "    category_result_file_name = 'roc_data_south_indian'\n",
        "else:\n",
        "    url_data = 'https://raw.githubusercontent.com/sydney-machine-learning/cyclonedatasets/main/SouthIndian-SouthPacific-Ocean/South_pacific_hurricane.csv'\n",
        "    function = load_data_south_pacific\n",
        "    hot_encoded_result_file_name = 'south_pacific'\n",
        "    category_result_file_name = 'roc_data_south_pacific' "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "south_indian\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "P5GxeAk79bKE",
        "outputId": "21bdab8a-daad-4547-ec51-684043f79c51"
      },
      "source": [
        "df = function(url_data)\n",
        "speed = df['Speed(knots)'].tolist()\n",
        "categories = df['category'].tolist()\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Basin</th>\n",
              "      <th>No. of Cycl</th>\n",
              "      <th>Time</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>Lat</th>\n",
              "      <th>Lon</th>\n",
              "      <th>Speed(knots)</th>\n",
              "      <th>lat_tenth</th>\n",
              "      <th>lon_tenth</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>SI</td>\n",
              "      <td>1</td>\n",
              "      <td>1981072512</td>\n",
              "      <td>BEST</td>\n",
              "      <td>0</td>\n",
              "      <td>118S</td>\n",
              "      <td>867E</td>\n",
              "      <td>20</td>\n",
              "      <td>11.8</td>\n",
              "      <td>86.7</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>SI</td>\n",
              "      <td>1</td>\n",
              "      <td>1981072518</td>\n",
              "      <td>BEST</td>\n",
              "      <td>0</td>\n",
              "      <td>116S</td>\n",
              "      <td>864E</td>\n",
              "      <td>25</td>\n",
              "      <td>11.6</td>\n",
              "      <td>86.4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>SI</td>\n",
              "      <td>1</td>\n",
              "      <td>1981072600</td>\n",
              "      <td>BEST</td>\n",
              "      <td>0</td>\n",
              "      <td>113S</td>\n",
              "      <td>862E</td>\n",
              "      <td>25</td>\n",
              "      <td>11.3</td>\n",
              "      <td>86.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>SI</td>\n",
              "      <td>1</td>\n",
              "      <td>1981072606</td>\n",
              "      <td>BEST</td>\n",
              "      <td>0</td>\n",
              "      <td>109S</td>\n",
              "      <td>862E</td>\n",
              "      <td>25</td>\n",
              "      <td>10.9</td>\n",
              "      <td>86.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SI</td>\n",
              "      <td>1</td>\n",
              "      <td>1981072612</td>\n",
              "      <td>BEST</td>\n",
              "      <td>0</td>\n",
              "      <td>104S</td>\n",
              "      <td>861E</td>\n",
              "      <td>25</td>\n",
              "      <td>10.4</td>\n",
              "      <td>86.1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Basin  No. of Cycl        Time  ... lat_tenth  lon_tenth category\n",
              "0    SI            1  1981072512  ...      11.8       86.7        0\n",
              "1    SI            1  1981072518  ...      11.6       86.4        0\n",
              "2    SI            1  1981072600  ...      11.3       86.2        0\n",
              "3    SI            1  1981072606  ...      10.9       86.2        0\n",
              "4    SI            1  1981072612  ...      10.4       86.1        0\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3ifbDar9ecd"
      },
      "source": [
        "def split_sequence(sequences, n_steps_in, n_steps_out):\n",
        "\tX, y = list(), list()\n",
        "\tfor i in range(len(sequences)):\n",
        "\t\t# find the end of this pattern\n",
        "\t\tend_ix = i + n_steps_in\n",
        "\t\tout_end_ix = end_ix + n_steps_out\n",
        "\t\t# check if we are beyond the dataset\n",
        "\t\tif out_end_ix > len(sequences):\n",
        "\t\t\tbreak\n",
        "\t\t# gather input and output parts of the pattern\n",
        "\t\tseq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix:out_end_ix, -1 ]\n",
        "\t\tX.append(seq_x)\n",
        "\t\ty.append(seq_y)\n",
        "\treturn array(X), array(y)\n",
        " \n",
        "# split a univariate sequence into samples\n",
        "def uni_split_sequence(sequence, n_steps):\n",
        "\tX, y = list(), list()\n",
        "\tfor i in range(len(sequence)):\n",
        "\t\t# find the end of this pattern\n",
        "\t\tend_ix = i + n_steps\n",
        "\t\t# check if we are beyond the sequence\n",
        "\t\tif end_ix > len(sequence)-1:\n",
        "\t\t\tbreak\n",
        "\t\t# gather input and output parts of the pattern\n",
        "\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
        "\t\tX.append(seq_x)\n",
        "\t\ty.append(seq_y)\n",
        "\treturn array(X), array(y) \n",
        " \n",
        "def rmse(pred, actual):\n",
        "    return np.sqrt(((pred-actual) ** 2).mean())\n",
        "\n",
        "def categorical(pred, actual):\n",
        "  cm = confusion_matrix(pred,actual)\n",
        "  ps = precision_score(pred,actual,average='micro')\n",
        "  rs = recall_score(pred,actual,average='micro')\n",
        "  f1 = f1_score(pred,actual, average = 'micro')\n",
        "  return cm,ps,rs,f1\n",
        "\n",
        "def make_confusion_matrix_chart2(cf_matrix_test, name):\n",
        "    #plt.figure(figsize=(20,12))\n",
        "    sns.set(font_scale=2.5)\n",
        "    fig, ax = plt.subplots(figsize = (20,12))\n",
        "    sns.heatmap(cf_matrix_test, annot=True, yticklabels=['0','1','2','3','4','5'], \n",
        "                                xticklabels=['0','1','2','3','4','5'], fmt='g')\n",
        "    plt.ylabel(\"Actual\", size=30)\n",
        "    plt.xlabel(\"Pred\", size=30)\n",
        "    ax.tick_params(axis='both', which='major', labelsize=25)\n",
        "    plt.savefig(name + '.png', dpi=300, transparent=False, bbox_inches='tight')\n",
        "    return None "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvKP7IKqymiw"
      },
      "source": [
        "univariate = True # if false, its multivariate case\n",
        "n_steps_in = 4\n",
        "n_seq = 2\n",
        "n_steps_out = 1\n",
        "n_features_in = 1 #speed\n",
        "n_features_out = 6 # one hot encoding of category\n",
        "Hidden = 10\n",
        "Epochs = 100\n",
        "Num_Exp = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-j1-OIjJ9nA8"
      },
      "source": [
        "id = df['No. of Cycl'][0]\n",
        "count = 0\n",
        "X = []\n",
        "Y = []\n",
        "start_index=0\n",
        "end_index=0\n",
        "for i in range(1, df.shape[0]):\n",
        "  if df['No. of Cycl'][i] == id :\n",
        "    end_index+=1\n",
        "  else:\n",
        "    x,y = uni_split_sequence(speed[start_index:end_index+1], n_steps_in)\n",
        "    X.append(x)\n",
        "    Y.append(y)\n",
        "    id = df['No. of Cycl'][i]\n",
        "    start_index=i\n",
        "    end_index=i\n",
        "  if i == df.shape[0]-1:   \n",
        "    x,y = uni_split_sequence(speed[start_index:end_index+1], n_steps_in)\n",
        "    X.append(x)\n",
        "    Y.append(y)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoEReuBL9rCr",
        "outputId": "f78713ae-c869-477a-87f7-25f450bd48fd"
      },
      "source": [
        "print(len(X), len(Y))\n",
        "X = [item for sublist in X for item in sublist]\n",
        "Y = [item for sublist in Y for item in sublist]\n",
        "print(len(X), len(Y))\n",
        "print(X[0], Y[0], X[1], Y[1])\n",
        "print(speed[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "670 670\n",
            "21454 21454\n",
            "[20 25 25 25] 25 [25 25 25 25] 30\n",
            "[20, 25, 25, 25, 25, 30, 30, 30, 25, 25]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abT14GsW9sx7"
      },
      "source": [
        "tmp = pd.DataFrame({'col': Y})\n",
        "if ocean == 'south_pacific':\n",
        "  category = tmp['col'].apply(lambda x: \n",
        "  0 if x<=33 else 1  if x<=47 and x>=34 else 2 if x<=63 and x>=48 else 3 if x<=85 and x>=64 else 4 if x<=107 and x>=86 else 5 )\n",
        "else:\n",
        "  category = tmp['col'].apply(lambda x: \n",
        "  0 if x<=33 else 1  if x<=47 and x>=34 else 2 if x<=63 and x>=48 else 3 if x<=89 and x>=64 else 4 if x<=115 and x>=90 else 5 )\n",
        "\n",
        "Y=category"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpAc1ilO-xWk",
        "outputId": "30b2eca3-1004-4495-ce1b-ece064cac6e9"
      },
      "source": [
        "train_limit = int(len(X)*70/100)\n",
        "train_limit"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15017"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqzfTLPbgnxt",
        "outputId": "672f0194-ea97-4a48-f27d-ce68b6337bb6"
      },
      "source": [
        "test_X_original = X[train_limit+1:]\n",
        "#X_original = X[:train_limit]\n",
        "#X_original = np.asarray(X_original).astype(float)\n",
        "test_Y_original = Y[train_limit+1:]\n",
        "#Y_original = Y[:train_limit]\n",
        "len(X), len(Y), len(test_X_original), len(test_Y_original)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(21454, 21454, 6436, 6436)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aknEoD6dXGde"
      },
      "source": [
        "X = MinMaxScaler().fit_transform(np.asarray(X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNQvG3PZ-3I4",
        "outputId": "fef80161-bc75-4380-ba43-a5910b59ec9f"
      },
      "source": [
        "speed_x = X\n",
        "test_X = X[train_limit+1:]\n",
        "test_X = np.asarray(test_X).astype(float)\n",
        "test_Y = Y[train_limit+1:]\n",
        "X = X[:train_limit]\n",
        "X = np.asarray(X).astype(float)\n",
        "Y = Y[:train_limit]\n",
        "print(len(test_X), len(test_Y))\n",
        "len(X), len(Y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6436 6436\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15017, 15017)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rn2Xw0vDAZ4n",
        "outputId": "f6daedd6-8e3d-4d80-f06e-65398149b4dd"
      },
      "source": [
        "counter_train = collections.OrderedDict(sorted(Counter(Y).items()))\n",
        "counter_test = collections.OrderedDict(sorted(Counter(test_Y).items()))\n",
        "print(\"train data: \", counter_train)\n",
        "print(\"test data: \", counter_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train data:  OrderedDict([(0, 4727), (1, 3819), (2, 2060), (3, 2254), (4, 1743), (5, 414)])\n",
            "test data:  OrderedDict([(0, 1303), (1, 1155), (2, 1082), (3, 1258), (4, 1350), (5, 288)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "id": "Viy_k8szAeRi",
        "outputId": "d1e18151-217c-49a9-effb-5e23f66f1f3e"
      },
      "source": [
        "plt.figure()\n",
        "#fig = plt.subplots(figsize =(20, 12))\n",
        "fig, ax = plt.subplots(figsize = (12,8))\n",
        "plt.bar(range(len(counter_train)), list(counter_train.values()), align='center')\n",
        "plt.xticks(range(len(counter_train)), list(counter_train.keys()))\n",
        "plt.xlabel('Category', size=40)\n",
        "plt.ylabel('Number', size=40)\n",
        "#plt.title('ROC' + ' (' + str(no_of_output_steps) + ' steps ahead, ' + model.capitalize() + ', Time step: ' + str(time_step) + ') - ' + train_or_test.capitalize() + ' (' + ocean + '_ocean)')\n",
        "ax.tick_params(axis='both', which='major', labelsize=30)\n",
        "plt.savefig(ocean + '_class_dist.png', dpi=300, transparent=False, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iw5div8vBVfa"
      },
      "source": [
        "def vanilla(n_steps_in,n_steps_out,n_features_in, n_features_out):\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(Hidden, activation='relu', input_shape=(n_steps_in, n_features_in)))\n",
        "  model.add(Dense(n_features_out, activation = \"softmax\"))\n",
        "  model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "  return model\n",
        " \n",
        "  #model = Sequential()\n",
        "  #model.add(LSTM(Hidden, activation='relu', input_shape=(n_steps_in, n_features_in)))\n",
        "  #model.add(RepeatVector(n_steps_out))\n",
        "  #model.add(LSTM(Hidden, activation='relu', return_sequences=True))\n",
        "  #model.add(TimeDistributed(Dense(n_features_out, activation = \"softmax\")))\n",
        "  #model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "  #return model\n",
        "\n",
        "def bidirectional(n_steps_in,n_steps_out,n_features_in, n_features_out):\n",
        "  model = Sequential()\n",
        "  model.add(Bidirectional(LSTM(Hidden, activation='relu', input_shape=(n_steps_in, n_features_in))))\n",
        "  model.add(Dense(n_features_out, activation = \"softmax\"))\n",
        "  model.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "  return model\n",
        "\n",
        "def cnn_lstm(n_steps_in,n_steps_out,n_features_in, n_features_out, n_seq):\n",
        "  model = Sequential()\n",
        "  model.add(TimeDistributed(Conv1D(filters=64, kernel_size=1, activation='relu'), input_shape=(None, int(n_steps_in/n_seq), n_features_in)))\n",
        "  model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
        "  model.add(TimeDistributed(Flatten()))\n",
        "  model.add(LSTM(Hidden, activation='relu'))\n",
        "  model.add(Dense(n_features_out, activation = \"softmax\"))\n",
        "  model.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "  return model\n",
        "\n",
        "def conv_lstm(n_steps_in,n_steps_out,n_features_in, n_features_out, n_seq):\n",
        "  model = Sequential()\n",
        "  model.add(ConvLSTM2D(filters=64, kernel_size=(1,2), activation='relu', input_shape=(n_seq, 1, int(n_steps_in/n_seq), n_features_in)))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(n_features_out, activation = \"softmax\"))\n",
        "  model.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCcCV5jyFE5A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f97ba3b-9c5a-4ee5-bf18-df2e8c298827"
      },
      "source": [
        "idx = np.random.permutation(len(X))\n",
        "print(len(idx))\n",
        "x_shuffled = []\n",
        "y_shuffled = []\n",
        "for i in idx:\n",
        "  x_shuffled.append(X[i])\n",
        "  y_shuffled.append(Y[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15017\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8uZm8wWIpM1",
        "outputId": "95c1e57c-750b-4d04-946d-488dc6d74746"
      },
      "source": [
        "Y_hot_encoded_train =  np.asarray(to_categorical(y_shuffled))\n",
        "#Y_hot_encoded_train = Y_hot_encoded_train.reshape(len(y_shuffled), n_steps_out, n_features_out)\n",
        "\n",
        "Y_hot_encoded_test =  np.asarray(to_categorical(test_Y))\n",
        "#Y_hot_encoded_test = Y_hot_encoded_test.reshape(len(test_Y), n_steps_out, n_features_out)\n",
        "\n",
        "print(Y_hot_encoded_train.shape, Y_hot_encoded_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(15017, 6) (6436, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUUYK__AVFV5",
        "outputId": "d37be0c1-0c6f-44ca-d7d4-acf8f737bbb8"
      },
      "source": [
        "x_shuffled[0], test_X[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0.10344828, 0.07142857, 0.07142857, 0.10344828]),\n",
              " array([0.10344828, 0.07142857, 0.14285714, 0.17241379]))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctVD6IeccA0r"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from tqdm.auto import tqdm\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dbpNKWBTPg3"
      },
      "source": [
        "fake_y_value = 2 #class id of fake data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXeG9Q4T8sXY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7953713d-34c9-4adc-a01e-38a91302bd15"
      },
      "source": [
        "x_shuffled = X\n",
        "y_shuffled = Y\n",
        "print(len(x_shuffled), len(X), len(y_shuffled)) \n",
        "t2 = np.asarray(x_shuffled).shape\n",
        "X_oversampled = torch.from_numpy(np.asarray(x_shuffled))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15017 15017 15017\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kU8ddMHk8saz"
      },
      "source": [
        "def get_generator_block(input_dim, output_dim):\n",
        "    return nn.Sequential(\n",
        "        nn.Linear(input_dim, output_dim),\n",
        "        nn.BatchNorm1d(output_dim),\n",
        "        nn.ReLU(inplace=True),\n",
        "    )\n",
        "class Generator(nn.Module):\n",
        "\n",
        "    def __init__(self, z_dim=t2[1], im_dim=t2[1], hidden_dim=128):\n",
        "        super(Generator, self).__init__()\n",
        "        self.gen = nn.Sequential(\n",
        "            get_generator_block(z_dim, hidden_dim),\n",
        "            get_generator_block(hidden_dim, hidden_dim * 2),\n",
        "            get_generator_block(hidden_dim * 2, hidden_dim * 4),\n",
        "            get_generator_block(hidden_dim * 4, hidden_dim * 8),\n",
        "            nn.Linear(hidden_dim * 8, im_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, noise):\n",
        "        return self.gen(noise)\n",
        "    \n",
        "    \n",
        "    def get_gen(self):\n",
        "\n",
        "        return self.gen\n",
        "def get_discriminator_block(input_dim, output_dim):\n",
        "    return nn.Sequential(\n",
        "        nn.Linear(input_dim, output_dim),\n",
        "        nn.LeakyReLU(0.2, inplace=True)        \n",
        "    )\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, im_dim=t2[1], hidden_dim=128):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.disc = nn.Sequential(\n",
        "            get_discriminator_block(im_dim, hidden_dim * 4),\n",
        "            get_discriminator_block(hidden_dim * 4, hidden_dim * 2),\n",
        "            get_discriminator_block(hidden_dim * 2, hidden_dim),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, image):\n",
        "\n",
        "        return self.disc(image)\n",
        "    \n",
        "    def get_disc(self):\n",
        "\n",
        "        return self.dis\n",
        "def get_noise(n_samples, z_dim, device='cuda'):\n",
        "\n",
        "    return torch.randn(n_samples,z_dim,device=device) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhhj1feF8sd5"
      },
      "source": [
        "criterion = nn.BCEWithLogitsLoss()\n",
        "n_epochs = 1000\n",
        "z_dim = t2[1]\n",
        "batch_size = 128\n",
        "lr = 0.00001\n",
        "display_step = 1\n",
        "device = 'cuda'\n",
        "gen = Generator(z_dim).to(device)\n",
        "gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)\n",
        "disc = Discriminator().to(device) \n",
        "disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87NodJ5e8shn"
      },
      "source": [
        "def get_disc_loss(gen, disc, criterion, real, num_images, z_dim, device):\n",
        "\n",
        "    fake_noise = get_noise(num_images, z_dim, device=device)\n",
        "    fake = gen(fake_noise)\n",
        "    disc_fake_pred = disc(fake.detach())\n",
        "    disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))\n",
        "    disc_real_pred = disc(real)\n",
        "    disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))\n",
        "    disc_loss = (disc_fake_loss + disc_real_loss) / 2\n",
        "\n",
        "    return disc_loss\n",
        "\n",
        "def get_gen_loss(gen, disc, criterion, num_images, z_dim, device):\n",
        "\n",
        "    fake_noise = get_noise(num_images, z_dim, device=device)\n",
        "    fake = gen(fake_noise)\n",
        "    disc_fake_pred = disc(fake)\n",
        "    gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))\n",
        "    return gen_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTQWHOcG8skX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5b23b40-396c-4385-d24c-2a02cd1382ae"
      },
      "source": [
        "li=[]\n",
        "for i in range(len(y_shuffled)):\n",
        "    if int(y_shuffled[i])==fake_y_value:\n",
        "        li.append(x_shuffled[i])\n",
        "        \n",
        "print(len(y_shuffled), len(li))\n",
        "\n",
        "X_real=np.array(li)\n",
        "t3=X_real.shape\n",
        "li2=[1]*(t3[0])\n",
        "y_real=np.array(li2)\n",
        "y_real.shape\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "tensor_x = torch.Tensor(X_real) \n",
        "tensor_y = torch.Tensor(y_real)\n",
        "my_dataset = TensorDataset(tensor_x,tensor_y)\n",
        "dataloader = DataLoader(\n",
        "    my_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True)\n",
        "\n",
        "cur_step = 0\n",
        "mean_generator_loss = 0\n",
        "mean_discriminator_loss = 0\n",
        "test_generator = True \n",
        "gen_loss = False\n",
        "error = False"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15017 2060\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSs5uGkwKFMK",
        "outputId": "207ebf65-d117-40ac-8107-ccda5990b278"
      },
      "source": [
        "samples_to_generate = int((X_oversampled.shape[0]-X_real.shape[0])/6)\n",
        "print(samples_to_generate)\n",
        "\n",
        "#epochs = [100,500,1000,2000,3000,4000,5000]\n",
        "epochs = [1000]\n",
        "\n",
        "smote_gan_data = dict()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2159\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sljP1gyHMlQ0"
      },
      "source": [
        "def augment_data():\n",
        "  fake_noise = get_noise(samples_to_generate, z_dim, device=device)\n",
        "  res=gen(fake_noise)\n",
        "  fres=res.cpu().detach().numpy()\n",
        "  X_old=X\n",
        "  finX=np.concatenate((X_old, fres), axis=0)\n",
        "  y_fake = np.full(shape=samples_to_generate,fill_value=fake_y_value)\n",
        "  Y_old=np.asarray(Y)\n",
        "  finY = np.append(Y_old, y_fake, axis=0)\n",
        "  idx = np.random.permutation(finX.shape[0])\n",
        "  x_shuffled = []\n",
        "  y_shuffled = []\n",
        "  for i in idx:\n",
        "    x_shuffled.append(finX[i])\n",
        "    y_shuffled.append(finY[i])\n",
        "  x_shuffled = np.asarray(x_shuffled)\n",
        "  x_shuffled = x_shuffled.reshape((x_shuffled.shape[0], x_shuffled.shape[1], n_features_in))\n",
        "  y_shuffled = np.asarray(to_categorical(y_shuffled))\n",
        "  return x_shuffled, y_shuffled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "silHRMRKYLJs"
      },
      "source": [
        "x_test = np.asarray(test_X)\n",
        "x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], n_features_in))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YflxwAmpRPhC"
      },
      "source": [
        "master_slave_results = dict()\n",
        "master_slave_test_acc = dict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iI-RdEy98srF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d1a6b47-24b7-4915-af12-47474455332b"
      },
      "source": [
        "for no_epoch in epochs:\n",
        "  print(\"no of epochs: \", no_epoch)\n",
        "  model = vanilla(n_steps_in,n_steps_out,n_features_in,n_features_out)\n",
        "  for epoch in range(no_epoch):\n",
        "    for real, _ in tqdm(dataloader, disable=True):\n",
        "        cur_batch_size = len(real) \n",
        "        real = real.view(cur_batch_size, -1).to(device)  \n",
        "        disc_opt.zero_grad()\n",
        "        disc_loss = get_disc_loss(gen, disc, criterion, real, cur_batch_size, z_dim, device)\n",
        "        disc_loss.backward(retain_graph=True)\n",
        "        disc_opt.step()\n",
        "        if test_generator:\n",
        "            old_generator_weights = gen.gen[0][0].weight.detach().clone()\n",
        "        gen_opt.zero_grad()\n",
        "        gen_loss = get_gen_loss(gen, disc, criterion, cur_batch_size, z_dim, device)\n",
        "        gen_loss.backward()\n",
        "        gen_opt.step()\n",
        "        if test_generator:\n",
        "            try:\n",
        "                assert lr > 0.0000002 or (gen.gen[0][0].weight.grad.abs().max() < 0.0005 and epoch == 0)\n",
        "                assert torch.any(gen.gen[0][0].weight.detach().clone() != old_generator_weights)\n",
        "            except:\n",
        "                error = True\n",
        "                print(\"Runtime tests have failed\")\n",
        "        mean_discriminator_loss += disc_loss.item() / display_step\n",
        "        mean_generator_loss += gen_loss.item() / display_step\n",
        "        #calling classifier after every 10 epochs of GAN\n",
        "        if(epoch%10==0):\n",
        "          print(f\"Epoch {epoch}: Step {cur_step}: Generator loss: {mean_generator_loss},discriminator loss: {mean_discriminator_loss}\")\n",
        "        if cur_step % display_step == 0 and cur_step > 0:\n",
        "            mean_generator_loss = 0\n",
        "            mean_discriminator_loss = 0\n",
        "        cur_step += 1\n",
        "\n",
        "    if epoch%10==0 and epoch>0:\n",
        "      print(f\"Epoch {epoch}\")\n",
        "      if(epoch!=10 and epoch>0):\n",
        "        model.load_weights('my_model_weights.h5')\n",
        "      x_train, y_train = augment_data()\n",
        "      model.fit(x_train, y_train, epochs=10, batch_size=10, verbose=0, shuffle=False)\n",
        "      y_predicttest = model.predict(x_test)\n",
        "      pred_test = [y_predicttest[i].argmax() for i in range(y_predicttest.shape[0])]\n",
        "      report_test = classification_report(test_Y, pred_test, labels=[0,1,2,3,4,5], output_dict=True)\n",
        "      print(\"accuracy: \", report_test[str(fake_y_value)]['f1-score'])\n",
        "      master_slave_results[epoch]=report_test\n",
        "      master_slave_test_acc[epoch]=report_test[str(fake_y_value)]['f1-score']\n",
        "      model.save_weights('my_model_weights.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no of epochs:  1000\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Epoch 0: Step 0: Generator loss: 0.681206226348877,discriminator loss: 0.6955541968345642\n",
            "Epoch 0: Step 1: Generator loss: 1.363266110420227,discriminator loss: 1.390764594078064\n",
            "Epoch 0: Step 2: Generator loss: 0.6829104423522949,discriminator loss: 0.6949605941772461\n",
            "Epoch 0: Step 3: Generator loss: 0.6836707592010498,discriminator loss: 0.6946296691894531\n",
            "Epoch 0: Step 4: Generator loss: 0.6845188140869141,discriminator loss: 0.6944134831428528\n",
            "Epoch 0: Step 5: Generator loss: 0.6853058338165283,discriminator loss: 0.6941161751747131\n",
            "Epoch 0: Step 6: Generator loss: 0.686143159866333,discriminator loss: 0.6939059495925903\n",
            "Epoch 0: Step 7: Generator loss: 0.6869192719459534,discriminator loss: 0.693584680557251\n",
            "Epoch 0: Step 8: Generator loss: 0.687742292881012,discriminator loss: 0.6933552622795105\n",
            "Epoch 0: Step 9: Generator loss: 0.688555121421814,discriminator loss: 0.6931544542312622\n",
            "Epoch 0: Step 10: Generator loss: 0.6892883777618408,discriminator loss: 0.6928278207778931\n",
            "Epoch 0: Step 11: Generator loss: 0.6900759339332581,discriminator loss: 0.6925780177116394\n",
            "Epoch 0: Step 12: Generator loss: 0.6908645033836365,discriminator loss: 0.6922718286514282\n",
            "Epoch 0: Step 13: Generator loss: 0.691663920879364,discriminator loss: 0.6919963359832764\n",
            "Epoch 0: Step 14: Generator loss: 0.6923822164535522,discriminator loss: 0.6918447017669678\n",
            "Epoch 0: Step 15: Generator loss: 0.6931552290916443,discriminator loss: 0.6914275884628296\n",
            "Epoch 0: Step 16: Generator loss: 0.6938744783401489,discriminator loss: 0.6917470693588257\n",
            "Epoch 10: Step 170: Generator loss: 0.7070708870887756,discriminator loss: 0.6912164092063904\n",
            "Epoch 10: Step 171: Generator loss: 0.7063189744949341,discriminator loss: 0.6929855346679688\n",
            "Epoch 10: Step 172: Generator loss: 0.7061086893081665,discriminator loss: 0.6914079189300537\n",
            "Epoch 10: Step 173: Generator loss: 0.7063730359077454,discriminator loss: 0.6899663209915161\n",
            "Epoch 10: Step 174: Generator loss: 0.7059346437454224,discriminator loss: 0.6946597099304199\n",
            "Epoch 10: Step 175: Generator loss: 0.7053554654121399,discriminator loss: 0.694412887096405\n",
            "Epoch 10: Step 176: Generator loss: 0.7048113346099854,discriminator loss: 0.6933286190032959\n",
            "Epoch 10: Step 177: Generator loss: 0.7047876119613647,discriminator loss: 0.692920446395874\n",
            "Epoch 10: Step 178: Generator loss: 0.7039117813110352,discriminator loss: 0.6942999958992004\n",
            "Epoch 10: Step 179: Generator loss: 0.7038633227348328,discriminator loss: 0.6927787065505981\n",
            "Epoch 10: Step 180: Generator loss: 0.7034309506416321,discriminator loss: 0.6933374404907227\n",
            "Epoch 10: Step 181: Generator loss: 0.7036269307136536,discriminator loss: 0.6946573853492737\n",
            "Epoch 10: Step 182: Generator loss: 0.7028449177742004,discriminator loss: 0.6948745250701904\n",
            "Epoch 10: Step 183: Generator loss: 0.7023991346359253,discriminator loss: 0.6940391063690186\n",
            "Epoch 10: Step 184: Generator loss: 0.7020877599716187,discriminator loss: 0.6935080885887146\n",
            "Epoch 10: Step 185: Generator loss: 0.7016063928604126,discriminator loss: 0.6951808929443359\n",
            "Epoch 10: Step 186: Generator loss: 0.7034527063369751,discriminator loss: 0.6886765956878662\n",
            "Epoch 10\n",
            "accuracy:  0.7272727272727274\n",
            "Epoch 20: Step 340: Generator loss: 0.6800243854522705,discriminator loss: 0.6926690936088562\n",
            "Epoch 20: Step 341: Generator loss: 0.6804901361465454,discriminator loss: 0.6925371885299683\n",
            "Epoch 20: Step 342: Generator loss: 0.6802974939346313,discriminator loss: 0.6924611330032349\n",
            "Epoch 20: Step 343: Generator loss: 0.6804860830307007,discriminator loss: 0.692527174949646\n",
            "Epoch 20: Step 344: Generator loss: 0.6805647611618042,discriminator loss: 0.6920884251594543\n",
            "Epoch 20: Step 345: Generator loss: 0.6806306838989258,discriminator loss: 0.6921648979187012\n",
            "Epoch 20: Step 346: Generator loss: 0.6806744337081909,discriminator loss: 0.6922624111175537\n",
            "Epoch 20: Step 347: Generator loss: 0.6806350350379944,discriminator loss: 0.6922143697738647\n",
            "Epoch 20: Step 348: Generator loss: 0.6806312799453735,discriminator loss: 0.6924420595169067\n",
            "Epoch 20: Step 349: Generator loss: 0.6810010671615601,discriminator loss: 0.6919751167297363\n",
            "Epoch 20: Step 350: Generator loss: 0.6809331178665161,discriminator loss: 0.6920336484909058\n",
            "Epoch 20: Step 351: Generator loss: 0.6810476779937744,discriminator loss: 0.691839337348938\n",
            "Epoch 20: Step 352: Generator loss: 0.6813153028488159,discriminator loss: 0.6917130351066589\n",
            "Epoch 20: Step 353: Generator loss: 0.6812368631362915,discriminator loss: 0.6916760802268982\n",
            "Epoch 20: Step 354: Generator loss: 0.6809880137443542,discriminator loss: 0.6919345855712891\n",
            "Epoch 20: Step 355: Generator loss: 0.6811854839324951,discriminator loss: 0.6915624737739563\n",
            "Epoch 20: Step 356: Generator loss: 0.6813958883285522,discriminator loss: 0.6914803981781006\n",
            "Epoch 20\n",
            "accuracy:  0.7511618081960287\n",
            "Epoch 30: Step 510: Generator loss: 0.7052631378173828,discriminator loss: 0.690421462059021\n",
            "Epoch 30: Step 511: Generator loss: 0.7052232623100281,discriminator loss: 0.6903179287910461\n",
            "Epoch 30: Step 512: Generator loss: 0.705596923828125,discriminator loss: 0.6906329393386841\n",
            "Epoch 30: Step 513: Generator loss: 0.7062320709228516,discriminator loss: 0.690490186214447\n",
            "Epoch 30: Step 514: Generator loss: 0.7051489353179932,discriminator loss: 0.6900135278701782\n",
            "Epoch 30: Step 515: Generator loss: 0.7050905227661133,discriminator loss: 0.6904699802398682\n",
            "Epoch 30: Step 516: Generator loss: 0.7059798240661621,discriminator loss: 0.6907196044921875\n",
            "Epoch 30: Step 517: Generator loss: 0.7064899206161499,discriminator loss: 0.6901694536209106\n",
            "Epoch 30: Step 518: Generator loss: 0.7054959535598755,discriminator loss: 0.6902639865875244\n",
            "Epoch 30: Step 519: Generator loss: 0.7065141201019287,discriminator loss: 0.6907744407653809\n",
            "Epoch 30: Step 520: Generator loss: 0.7064377665519714,discriminator loss: 0.6904670000076294\n",
            "Epoch 30: Step 521: Generator loss: 0.7062995433807373,discriminator loss: 0.6902962923049927\n",
            "Epoch 30: Step 522: Generator loss: 0.7071088552474976,discriminator loss: 0.6902830600738525\n",
            "Epoch 30: Step 523: Generator loss: 0.7070522308349609,discriminator loss: 0.6897305250167847\n",
            "Epoch 30: Step 524: Generator loss: 0.7066311836242676,discriminator loss: 0.6899076700210571\n",
            "Epoch 30: Step 525: Generator loss: 0.7078936100006104,discriminator loss: 0.6900650262832642\n",
            "Epoch 30: Step 526: Generator loss: 0.7093411684036255,discriminator loss: 0.6886353492736816\n",
            "Epoch 30\n",
            "accuracy:  0.7367119901112484\n",
            "Epoch 40: Step 680: Generator loss: 0.686028003692627,discriminator loss: 0.692682147026062\n",
            "Epoch 40: Step 681: Generator loss: 0.6857452392578125,discriminator loss: 0.6928988695144653\n",
            "Epoch 40: Step 682: Generator loss: 0.6855913996696472,discriminator loss: 0.6922251582145691\n",
            "Epoch 40: Step 683: Generator loss: 0.6861608624458313,discriminator loss: 0.6925628185272217\n",
            "Epoch 40: Step 684: Generator loss: 0.6854631900787354,discriminator loss: 0.6924055814743042\n",
            "Epoch 40: Step 685: Generator loss: 0.6858976483345032,discriminator loss: 0.6917997598648071\n",
            "Epoch 40: Step 686: Generator loss: 0.6856833100318909,discriminator loss: 0.6922848224639893\n",
            "Epoch 40: Step 687: Generator loss: 0.685215175151825,discriminator loss: 0.6923153400421143\n",
            "Epoch 40: Step 688: Generator loss: 0.6858916878700256,discriminator loss: 0.6929510831832886\n",
            "Epoch 40: Step 689: Generator loss: 0.6855014562606812,discriminator loss: 0.6930820941925049\n",
            "Epoch 40: Step 690: Generator loss: 0.6851723790168762,discriminator loss: 0.6924445629119873\n",
            "Epoch 40: Step 691: Generator loss: 0.6858339309692383,discriminator loss: 0.692401647567749\n",
            "Epoch 40: Step 692: Generator loss: 0.6855759024620056,discriminator loss: 0.6929267644882202\n",
            "Epoch 40: Step 693: Generator loss: 0.6858758926391602,discriminator loss: 0.6928969621658325\n",
            "Epoch 40: Step 694: Generator loss: 0.6857993602752686,discriminator loss: 0.6935722827911377\n",
            "Epoch 40: Step 695: Generator loss: 0.6858727931976318,discriminator loss: 0.6928898096084595\n",
            "Epoch 40: Step 696: Generator loss: 0.6867648959159851,discriminator loss: 0.6922295093536377\n",
            "Epoch 40\n",
            "accuracy:  0.7895460797799174\n",
            "Epoch 50: Step 850: Generator loss: 0.7054746150970459,discriminator loss: 0.6931882500648499\n",
            "Epoch 50: Step 851: Generator loss: 0.7051058411598206,discriminator loss: 0.6931329965591431\n",
            "Epoch 50: Step 852: Generator loss: 0.7048680186271667,discriminator loss: 0.6926960945129395\n",
            "Epoch 50: Step 853: Generator loss: 0.7043381929397583,discriminator loss: 0.6931561231613159\n",
            "Epoch 50: Step 854: Generator loss: 0.7038895487785339,discriminator loss: 0.6927082538604736\n",
            "Epoch 50: Step 855: Generator loss: 0.7037651538848877,discriminator loss: 0.6927597522735596\n",
            "Epoch 50: Step 856: Generator loss: 0.7033885717391968,discriminator loss: 0.693293571472168\n",
            "Epoch 50: Step 857: Generator loss: 0.7029535174369812,discriminator loss: 0.693244457244873\n",
            "Epoch 50: Step 858: Generator loss: 0.7027845978736877,discriminator loss: 0.6930431723594666\n",
            "Epoch 50: Step 859: Generator loss: 0.7022333145141602,discriminator loss: 0.6932535767555237\n",
            "Epoch 50: Step 860: Generator loss: 0.7018392086029053,discriminator loss: 0.6933373808860779\n",
            "Epoch 50: Step 861: Generator loss: 0.7015781402587891,discriminator loss: 0.6931171417236328\n",
            "Epoch 50: Step 862: Generator loss: 0.7010641098022461,discriminator loss: 0.6933870911598206\n",
            "Epoch 50: Step 863: Generator loss: 0.7008423805236816,discriminator loss: 0.6930229663848877\n",
            "Epoch 50: Step 864: Generator loss: 0.7004572153091431,discriminator loss: 0.6929054260253906\n",
            "Epoch 50: Step 865: Generator loss: 0.6999937295913696,discriminator loss: 0.6934720873832703\n",
            "Epoch 50: Step 866: Generator loss: 0.7000207901000977,discriminator loss: 0.6928800940513611\n",
            "Epoch 50\n",
            "accuracy:  0.8100840336134455\n",
            "Epoch 60: Step 1020: Generator loss: 0.6852008104324341,discriminator loss: 0.6928274631500244\n",
            "Epoch 60: Step 1021: Generator loss: 0.6854552626609802,discriminator loss: 0.692584753036499\n",
            "Epoch 60: Step 1022: Generator loss: 0.685480535030365,discriminator loss: 0.6927231550216675\n",
            "Epoch 60: Step 1023: Generator loss: 0.6854420900344849,discriminator loss: 0.6928637027740479\n",
            "Epoch 60: Step 1024: Generator loss: 0.6858733892440796,discriminator loss: 0.6927773952484131\n",
            "Epoch 60: Step 1025: Generator loss: 0.6861437559127808,discriminator loss: 0.6928407549858093\n",
            "Epoch 60: Step 1026: Generator loss: 0.6863001585006714,discriminator loss: 0.6928126811981201\n",
            "Epoch 60: Step 1027: Generator loss: 0.6862257719039917,discriminator loss: 0.6930650472640991\n",
            "Epoch 60: Step 1028: Generator loss: 0.6860930919647217,discriminator loss: 0.6935516595840454\n",
            "Epoch 60: Step 1029: Generator loss: 0.6869754791259766,discriminator loss: 0.6931819915771484\n",
            "Epoch 60: Step 1030: Generator loss: 0.6869078874588013,discriminator loss: 0.6933319568634033\n",
            "Epoch 60: Step 1031: Generator loss: 0.6869655251502991,discriminator loss: 0.6938701868057251\n",
            "Epoch 60: Step 1032: Generator loss: 0.6876571178436279,discriminator loss: 0.6933889985084534\n",
            "Epoch 60: Step 1033: Generator loss: 0.6876433491706848,discriminator loss: 0.693169116973877\n",
            "Epoch 60: Step 1034: Generator loss: 0.6876951456069946,discriminator loss: 0.6938447952270508\n",
            "Epoch 60: Step 1035: Generator loss: 0.688105583190918,discriminator loss: 0.6929959058761597\n",
            "Epoch 60: Step 1036: Generator loss: 0.6894634962081909,discriminator loss: 0.6934719085693359\n",
            "Epoch 60\n",
            "accuracy:  0.7972508591065293\n",
            "Epoch 70: Step 1190: Generator loss: 0.693398118019104,discriminator loss: 0.6939773559570312\n",
            "Epoch 70: Step 1191: Generator loss: 0.6929068565368652,discriminator loss: 0.694362998008728\n",
            "Epoch 70: Step 1192: Generator loss: 0.6928094625473022,discriminator loss: 0.6945403814315796\n",
            "Epoch 70: Step 1193: Generator loss: 0.6923649311065674,discriminator loss: 0.6945280432701111\n",
            "Epoch 70: Step 1194: Generator loss: 0.6923344731330872,discriminator loss: 0.6947374939918518\n",
            "Epoch 70: Step 1195: Generator loss: 0.6920167207717896,discriminator loss: 0.694145143032074\n",
            "Epoch 70: Step 1196: Generator loss: 0.6920526027679443,discriminator loss: 0.6947504281997681\n",
            "Epoch 70: Step 1197: Generator loss: 0.6918188333511353,discriminator loss: 0.6943281888961792\n",
            "Epoch 70: Step 1198: Generator loss: 0.6912428736686707,discriminator loss: 0.6943602561950684\n",
            "Epoch 70: Step 1199: Generator loss: 0.6908735036849976,discriminator loss: 0.6947197318077087\n",
            "Epoch 70: Step 1200: Generator loss: 0.6907342672348022,discriminator loss: 0.6943128108978271\n",
            "Epoch 70: Step 1201: Generator loss: 0.6903778314590454,discriminator loss: 0.6943087577819824\n",
            "Epoch 70: Step 1202: Generator loss: 0.690598726272583,discriminator loss: 0.6937671899795532\n",
            "Epoch 70: Step 1203: Generator loss: 0.6901117563247681,discriminator loss: 0.694125771522522\n",
            "Epoch 70: Step 1204: Generator loss: 0.6897526979446411,discriminator loss: 0.6940467357635498\n",
            "Epoch 70: Step 1205: Generator loss: 0.6894528269767761,discriminator loss: 0.6947619915008545\n",
            "Epoch 70: Step 1206: Generator loss: 0.689586341381073,discriminator loss: 0.6930839419364929\n",
            "Epoch 70\n",
            "accuracy:  0.7556283258288989\n",
            "Epoch 80: Step 1360: Generator loss: 0.700621485710144,discriminator loss: 0.6944215297698975\n",
            "Epoch 80: Step 1361: Generator loss: 0.7011758089065552,discriminator loss: 0.6937332153320312\n",
            "Epoch 80: Step 1362: Generator loss: 0.7015641331672668,discriminator loss: 0.69414222240448\n",
            "Epoch 80: Step 1363: Generator loss: 0.702572762966156,discriminator loss: 0.6942991018295288\n",
            "Epoch 80: Step 1364: Generator loss: 0.7027202844619751,discriminator loss: 0.6942511200904846\n",
            "Epoch 80: Step 1365: Generator loss: 0.7032151222229004,discriminator loss: 0.6942494511604309\n",
            "Epoch 80: Step 1366: Generator loss: 0.7039701342582703,discriminator loss: 0.6941694617271423\n",
            "Epoch 80: Step 1367: Generator loss: 0.703762412071228,discriminator loss: 0.6940773725509644\n",
            "Epoch 80: Step 1368: Generator loss: 0.7046899795532227,discriminator loss: 0.6937218904495239\n",
            "Epoch 80: Step 1369: Generator loss: 0.7054198384284973,discriminator loss: 0.6933284401893616\n",
            "Epoch 80: Step 1370: Generator loss: 0.7061809301376343,discriminator loss: 0.6935812830924988\n",
            "Epoch 80: Step 1371: Generator loss: 0.7068607807159424,discriminator loss: 0.6936227083206177\n",
            "Epoch 80: Step 1372: Generator loss: 0.7072968482971191,discriminator loss: 0.6935732364654541\n",
            "Epoch 80: Step 1373: Generator loss: 0.7080172300338745,discriminator loss: 0.6934136152267456\n",
            "Epoch 80: Step 1374: Generator loss: 0.7084751725196838,discriminator loss: 0.6932482719421387\n",
            "Epoch 80: Step 1375: Generator loss: 0.7095060348510742,discriminator loss: 0.6933107376098633\n",
            "Epoch 80: Step 1376: Generator loss: 0.7107254862785339,discriminator loss: 0.6923905611038208\n",
            "Epoch 80\n",
            "accuracy:  0.8051612903225807\n",
            "Epoch 90: Step 1530: Generator loss: 0.6751829981803894,discriminator loss: 0.6928408741950989\n",
            "Epoch 90: Step 1531: Generator loss: 0.6751195192337036,discriminator loss: 0.6927557587623596\n",
            "Epoch 90: Step 1532: Generator loss: 0.6749953627586365,discriminator loss: 0.6928658485412598\n",
            "Epoch 90: Step 1533: Generator loss: 0.674953818321228,discriminator loss: 0.6927425265312195\n",
            "Epoch 90: Step 1534: Generator loss: 0.6749305129051208,discriminator loss: 0.6927199363708496\n",
            "Epoch 90: Step 1535: Generator loss: 0.674875020980835,discriminator loss: 0.692528486251831\n",
            "Epoch 90: Step 1536: Generator loss: 0.6747080683708191,discriminator loss: 0.6926193237304688\n",
            "Epoch 90: Step 1537: Generator loss: 0.6747016906738281,discriminator loss: 0.6925152540206909\n",
            "Epoch 90: Step 1538: Generator loss: 0.6747092008590698,discriminator loss: 0.6923267841339111\n",
            "Epoch 90: Step 1539: Generator loss: 0.6747841835021973,discriminator loss: 0.6923074722290039\n",
            "Epoch 90: Step 1540: Generator loss: 0.6746029853820801,discriminator loss: 0.6924058198928833\n",
            "Epoch 90: Step 1541: Generator loss: 0.6746846437454224,discriminator loss: 0.6920214891433716\n",
            "Epoch 90: Step 1542: Generator loss: 0.6746681928634644,discriminator loss: 0.6920771598815918\n",
            "Epoch 90: Step 1543: Generator loss: 0.6746572256088257,discriminator loss: 0.6917968392372131\n",
            "Epoch 90: Step 1544: Generator loss: 0.6746145486831665,discriminator loss: 0.6918739080429077\n",
            "Epoch 90: Step 1545: Generator loss: 0.6744617223739624,discriminator loss: 0.692081093788147\n",
            "Epoch 90: Step 1546: Generator loss: 0.6743232607841492,discriminator loss: 0.6933091282844543\n",
            "Epoch 90\n",
            "accuracy:  0.7923844061650046\n",
            "Epoch 100: Step 1700: Generator loss: 0.7220292091369629,discriminator loss: 0.692184567451477\n",
            "Epoch 100: Step 1701: Generator loss: 0.7223563194274902,discriminator loss: 0.6920502185821533\n",
            "Epoch 100: Step 1702: Generator loss: 0.7228075265884399,discriminator loss: 0.6922142505645752\n",
            "Epoch 100: Step 1703: Generator loss: 0.7229907512664795,discriminator loss: 0.6919161081314087\n",
            "Epoch 100: Step 1704: Generator loss: 0.7232226729393005,discriminator loss: 0.6920261979103088\n",
            "Epoch 100: Step 1705: Generator loss: 0.7235430479049683,discriminator loss: 0.6917919516563416\n",
            "Epoch 100: Step 1706: Generator loss: 0.7236964106559753,discriminator loss: 0.691399335861206\n",
            "Epoch 100: Step 1707: Generator loss: 0.7238119840621948,discriminator loss: 0.6916547417640686\n",
            "Epoch 100: Step 1708: Generator loss: 0.7240372896194458,discriminator loss: 0.6914102435112\n",
            "Epoch 100: Step 1709: Generator loss: 0.7244771718978882,discriminator loss: 0.6915781497955322\n",
            "Epoch 100: Step 1710: Generator loss: 0.7244777679443359,discriminator loss: 0.6908612251281738\n",
            "Epoch 100: Step 1711: Generator loss: 0.724655270576477,discriminator loss: 0.691737174987793\n",
            "Epoch 100: Step 1712: Generator loss: 0.7248139977455139,discriminator loss: 0.6910510063171387\n",
            "Epoch 100: Step 1713: Generator loss: 0.7249096632003784,discriminator loss: 0.6915400624275208\n",
            "Epoch 100: Step 1714: Generator loss: 0.7250761985778809,discriminator loss: 0.6909242868423462\n",
            "Epoch 100: Step 1715: Generator loss: 0.7250673770904541,discriminator loss: 0.6908824443817139\n",
            "Epoch 100: Step 1716: Generator loss: 0.7252120971679688,discriminator loss: 0.6902759075164795\n",
            "Epoch 100\n",
            "accuracy:  0.7827664399092972\n",
            "Epoch 110: Step 1870: Generator loss: 0.6731894016265869,discriminator loss: 0.6930596828460693\n",
            "Epoch 110: Step 1871: Generator loss: 0.673125684261322,discriminator loss: 0.6929053068161011\n",
            "Epoch 110: Step 1872: Generator loss: 0.6731478571891785,discriminator loss: 0.6928829550743103\n",
            "Epoch 110: Step 1873: Generator loss: 0.6731502413749695,discriminator loss: 0.692476212978363\n",
            "Epoch 110: Step 1874: Generator loss: 0.6730093955993652,discriminator loss: 0.6925803422927856\n",
            "Epoch 110: Step 1875: Generator loss: 0.6731447577476501,discriminator loss: 0.692562460899353\n",
            "Epoch 110: Step 1876: Generator loss: 0.6731096506118774,discriminator loss: 0.6922975778579712\n",
            "Epoch 110: Step 1877: Generator loss: 0.6730301380157471,discriminator loss: 0.6922646760940552\n",
            "Epoch 110: Step 1878: Generator loss: 0.6731170415878296,discriminator loss: 0.6921464204788208\n",
            "Epoch 110: Step 1879: Generator loss: 0.673050045967102,discriminator loss: 0.692221999168396\n",
            "Epoch 110: Step 1880: Generator loss: 0.6730445027351379,discriminator loss: 0.6920284628868103\n",
            "Epoch 110: Step 1881: Generator loss: 0.6730395555496216,discriminator loss: 0.6918766498565674\n",
            "Epoch 110: Step 1882: Generator loss: 0.6731249690055847,discriminator loss: 0.691815972328186\n",
            "Epoch 110: Step 1883: Generator loss: 0.6730318069458008,discriminator loss: 0.6913553476333618\n",
            "Epoch 110: Step 1884: Generator loss: 0.6728949546813965,discriminator loss: 0.6918361186981201\n",
            "Epoch 110: Step 1885: Generator loss: 0.6728096008300781,discriminator loss: 0.691842257976532\n",
            "Epoch 110: Step 1886: Generator loss: 0.6731154322624207,discriminator loss: 0.6913973093032837\n",
            "Epoch 110\n",
            "accuracy:  0.7527675276752768\n",
            "Epoch 120: Step 2040: Generator loss: 0.7396604418754578,discriminator loss: 0.6893614530563354\n",
            "Epoch 120: Step 2041: Generator loss: 0.7394251823425293,discriminator loss: 0.6899710297584534\n",
            "Epoch 120: Step 2042: Generator loss: 0.738716721534729,discriminator loss: 0.689812183380127\n",
            "Epoch 120: Step 2043: Generator loss: 0.7398103475570679,discriminator loss: 0.6892783641815186\n",
            "Epoch 120: Step 2044: Generator loss: 0.7401456832885742,discriminator loss: 0.6902896165847778\n",
            "Epoch 120: Step 2045: Generator loss: 0.7393136620521545,discriminator loss: 0.6887215971946716\n",
            "Epoch 120: Step 2046: Generator loss: 0.7398790717124939,discriminator loss: 0.6884287595748901\n",
            "Epoch 120: Step 2047: Generator loss: 0.7393466830253601,discriminator loss: 0.6895420551300049\n",
            "Epoch 120: Step 2048: Generator loss: 0.7394323348999023,discriminator loss: 0.688808262348175\n",
            "Epoch 120: Step 2049: Generator loss: 0.7390960454940796,discriminator loss: 0.6891884803771973\n",
            "Epoch 120: Step 2050: Generator loss: 0.7398244738578796,discriminator loss: 0.6884193420410156\n",
            "Epoch 120: Step 2051: Generator loss: 0.7383970022201538,discriminator loss: 0.6889179944992065\n",
            "Epoch 120: Step 2052: Generator loss: 0.7401261925697327,discriminator loss: 0.6891212463378906\n",
            "Epoch 120: Step 2053: Generator loss: 0.7393563389778137,discriminator loss: 0.6888584494590759\n",
            "Epoch 120: Step 2054: Generator loss: 0.7400562167167664,discriminator loss: 0.6876826286315918\n",
            "Epoch 120: Step 2055: Generator loss: 0.7382399439811707,discriminator loss: 0.6884399056434631\n",
            "Epoch 120: Step 2056: Generator loss: 0.735198974609375,discriminator loss: 0.6899511218070984\n",
            "Epoch 120\n",
            "accuracy:  0.7601121345614738\n",
            "Epoch 130: Step 2210: Generator loss: 0.671342134475708,discriminator loss: 0.6918132305145264\n",
            "Epoch 130: Step 2211: Generator loss: 0.6718605160713196,discriminator loss: 0.6907802224159241\n",
            "Epoch 130: Step 2212: Generator loss: 0.6717896461486816,discriminator loss: 0.6912902593612671\n",
            "Epoch 130: Step 2213: Generator loss: 0.6719188690185547,discriminator loss: 0.6912418603897095\n",
            "Epoch 130: Step 2214: Generator loss: 0.6719843745231628,discriminator loss: 0.6916309595108032\n",
            "Epoch 130: Step 2215: Generator loss: 0.6718124747276306,discriminator loss: 0.6909976005554199\n",
            "Epoch 130: Step 2216: Generator loss: 0.6721735000610352,discriminator loss: 0.691146731376648\n",
            "Epoch 130: Step 2217: Generator loss: 0.672240138053894,discriminator loss: 0.6914647817611694\n",
            "Epoch 130: Step 2218: Generator loss: 0.6721206903457642,discriminator loss: 0.6907034516334534\n",
            "Epoch 130: Step 2219: Generator loss: 0.6719878911972046,discriminator loss: 0.6912073493003845\n",
            "Epoch 130: Step 2220: Generator loss: 0.6722612977027893,discriminator loss: 0.6908861398696899\n",
            "Epoch 130: Step 2221: Generator loss: 0.6724266409873962,discriminator loss: 0.6911683082580566\n",
            "Epoch 130: Step 2222: Generator loss: 0.6723040342330933,discriminator loss: 0.6911879777908325\n",
            "Epoch 130: Step 2223: Generator loss: 0.6723232269287109,discriminator loss: 0.6910626292228699\n",
            "Epoch 130: Step 2224: Generator loss: 0.6724645495414734,discriminator loss: 0.6910048723220825\n",
            "Epoch 130: Step 2225: Generator loss: 0.6726152896881104,discriminator loss: 0.690822958946228\n",
            "Epoch 130: Step 2226: Generator loss: 0.6727737784385681,discriminator loss: 0.6909483671188354\n",
            "Epoch 130\n",
            "accuracy:  0.7716666666666667\n",
            "Epoch 140: Step 2380: Generator loss: 0.7224719524383545,discriminator loss: 0.6915497779846191\n",
            "Epoch 140: Step 2381: Generator loss: 0.7223418951034546,discriminator loss: 0.692205548286438\n",
            "Epoch 140: Step 2382: Generator loss: 0.7221900820732117,discriminator loss: 0.6915034055709839\n",
            "Epoch 140: Step 2383: Generator loss: 0.7219986915588379,discriminator loss: 0.6916753053665161\n",
            "Epoch 140: Step 2384: Generator loss: 0.7219015955924988,discriminator loss: 0.6917417049407959\n",
            "Epoch 140: Step 2385: Generator loss: 0.7218679785728455,discriminator loss: 0.6915667057037354\n",
            "Epoch 140: Step 2386: Generator loss: 0.7216951847076416,discriminator loss: 0.6919726729393005\n",
            "Epoch 140: Step 2387: Generator loss: 0.7214938402175903,discriminator loss: 0.6920760869979858\n",
            "Epoch 140: Step 2388: Generator loss: 0.7212332487106323,discriminator loss: 0.6914076209068298\n",
            "Epoch 140: Step 2389: Generator loss: 0.7211310863494873,discriminator loss: 0.6912480592727661\n",
            "Epoch 140: Step 2390: Generator loss: 0.7209029197692871,discriminator loss: 0.6914043426513672\n",
            "Epoch 140: Step 2391: Generator loss: 0.7208427786827087,discriminator loss: 0.6921108961105347\n",
            "Epoch 140: Step 2392: Generator loss: 0.7203420996665955,discriminator loss: 0.6920244693756104\n",
            "Epoch 140: Step 2393: Generator loss: 0.7203454971313477,discriminator loss: 0.6913838386535645\n",
            "Epoch 140: Step 2394: Generator loss: 0.7199516296386719,discriminator loss: 0.6916739344596863\n",
            "Epoch 140: Step 2395: Generator loss: 0.7195766568183899,discriminator loss: 0.6910850405693054\n",
            "Epoch 140: Step 2396: Generator loss: 0.7200252413749695,discriminator loss: 0.69065260887146\n",
            "Epoch 140\n",
            "accuracy:  0.7244995233555767\n",
            "Epoch 150: Step 2550: Generator loss: 0.6740935444831848,discriminator loss: 0.6924448013305664\n",
            "Epoch 150: Step 2551: Generator loss: 0.6741774678230286,discriminator loss: 0.6923999786376953\n",
            "Epoch 150: Step 2552: Generator loss: 0.6740003228187561,discriminator loss: 0.692049503326416\n",
            "Epoch 150: Step 2553: Generator loss: 0.6740514636039734,discriminator loss: 0.6924430727958679\n",
            "Epoch 150: Step 2554: Generator loss: 0.674031138420105,discriminator loss: 0.692156195640564\n",
            "Epoch 150: Step 2555: Generator loss: 0.6740443706512451,discriminator loss: 0.6919981241226196\n",
            "Epoch 150: Step 2556: Generator loss: 0.6740350127220154,discriminator loss: 0.6923428773880005\n",
            "Epoch 150: Step 2557: Generator loss: 0.6739996671676636,discriminator loss: 0.6921533346176147\n",
            "Epoch 150: Step 2558: Generator loss: 0.674049973487854,discriminator loss: 0.6922950148582458\n",
            "Epoch 150: Step 2559: Generator loss: 0.6741341948509216,discriminator loss: 0.6923545002937317\n",
            "Epoch 150: Step 2560: Generator loss: 0.674201250076294,discriminator loss: 0.6922767162322998\n",
            "Epoch 150: Step 2561: Generator loss: 0.6740316152572632,discriminator loss: 0.6921241879463196\n",
            "Epoch 150: Step 2562: Generator loss: 0.6741682291030884,discriminator loss: 0.6920732259750366\n",
            "Epoch 150: Step 2563: Generator loss: 0.6742082238197327,discriminator loss: 0.6918904185295105\n",
            "Epoch 150: Step 2564: Generator loss: 0.6742699146270752,discriminator loss: 0.6920441389083862\n",
            "Epoch 150: Step 2565: Generator loss: 0.6744323968887329,discriminator loss: 0.6923683285713196\n",
            "Epoch 150: Step 2566: Generator loss: 0.6743927001953125,discriminator loss: 0.6929357051849365\n",
            "Epoch 150\n",
            "accuracy:  0.7695895522388059\n",
            "Epoch 160: Step 2720: Generator loss: 0.7211267948150635,discriminator loss: 0.6916400194168091\n",
            "Epoch 160: Step 2721: Generator loss: 0.7217602729797363,discriminator loss: 0.6913561820983887\n",
            "Epoch 160: Step 2722: Generator loss: 0.7218805551528931,discriminator loss: 0.6912311911582947\n",
            "Epoch 160: Step 2723: Generator loss: 0.7227208614349365,discriminator loss: 0.6913546919822693\n",
            "Epoch 160: Step 2724: Generator loss: 0.722585141658783,discriminator loss: 0.6908419728279114\n",
            "Epoch 160: Step 2725: Generator loss: 0.7224932909011841,discriminator loss: 0.6915155053138733\n",
            "Epoch 160: Step 2726: Generator loss: 0.7236071228981018,discriminator loss: 0.6910843253135681\n",
            "Epoch 160: Step 2727: Generator loss: 0.7234283685684204,discriminator loss: 0.6911063194274902\n",
            "Epoch 160: Step 2728: Generator loss: 0.7231283187866211,discriminator loss: 0.6909234523773193\n",
            "Epoch 160: Step 2729: Generator loss: 0.7231009602546692,discriminator loss: 0.6905987858772278\n",
            "Epoch 160: Step 2730: Generator loss: 0.7231568098068237,discriminator loss: 0.6903550624847412\n",
            "Epoch 160: Step 2731: Generator loss: 0.7233219146728516,discriminator loss: 0.6905484199523926\n",
            "Epoch 160: Step 2732: Generator loss: 0.7234517931938171,discriminator loss: 0.6907199621200562\n",
            "Epoch 160: Step 2733: Generator loss: 0.7234729528427124,discriminator loss: 0.6902872323989868\n",
            "Epoch 160: Step 2734: Generator loss: 0.7238489389419556,discriminator loss: 0.690800666809082\n",
            "Epoch 160: Step 2735: Generator loss: 0.7240931987762451,discriminator loss: 0.6896803379058838\n",
            "Epoch 160: Step 2736: Generator loss: 0.7247170805931091,discriminator loss: 0.6912070512771606\n",
            "Epoch 160\n",
            "accuracy:  0.7917228969860548\n",
            "Epoch 170: Step 2890: Generator loss: 0.6768280267715454,discriminator loss: 0.6929405927658081\n",
            "Epoch 170: Step 2891: Generator loss: 0.6767599582672119,discriminator loss: 0.6928467750549316\n",
            "Epoch 170: Step 2892: Generator loss: 0.676703691482544,discriminator loss: 0.6926446557044983\n",
            "Epoch 170: Step 2893: Generator loss: 0.676654577255249,discriminator loss: 0.6926767826080322\n",
            "Epoch 170: Step 2894: Generator loss: 0.6765847206115723,discriminator loss: 0.6927926540374756\n",
            "Epoch 170: Step 2895: Generator loss: 0.6765903234481812,discriminator loss: 0.692571222782135\n",
            "Epoch 170: Step 2896: Generator loss: 0.6764943599700928,discriminator loss: 0.692131519317627\n",
            "Epoch 170: Step 2897: Generator loss: 0.67645663022995,discriminator loss: 0.6924415826797485\n",
            "Epoch 170: Step 2898: Generator loss: 0.6764309406280518,discriminator loss: 0.6921056509017944\n",
            "Epoch 170: Step 2899: Generator loss: 0.6764478087425232,discriminator loss: 0.6922340989112854\n",
            "Epoch 170: Step 2900: Generator loss: 0.6764117479324341,discriminator loss: 0.69217848777771\n",
            "Epoch 170: Step 2901: Generator loss: 0.6764040589332581,discriminator loss: 0.6922984719276428\n",
            "Epoch 170: Step 2902: Generator loss: 0.676327645778656,discriminator loss: 0.6920053958892822\n",
            "Epoch 170: Step 2903: Generator loss: 0.6763079762458801,discriminator loss: 0.6921411156654358\n",
            "Epoch 170: Step 2904: Generator loss: 0.676243782043457,discriminator loss: 0.6918147802352905\n",
            "Epoch 170: Step 2905: Generator loss: 0.6763830780982971,discriminator loss: 0.6917188167572021\n",
            "Epoch 170: Step 2906: Generator loss: 0.6763027906417847,discriminator loss: 0.6911525726318359\n",
            "Epoch 170\n",
            "accuracy:  0.7987857762359063\n",
            "Epoch 180: Step 3060: Generator loss: 0.7169687151908875,discriminator loss: 0.6927626729011536\n",
            "Epoch 180: Step 3061: Generator loss: 0.7172315716743469,discriminator loss: 0.692596435546875\n",
            "Epoch 180: Step 3062: Generator loss: 0.7173707485198975,discriminator loss: 0.6924829483032227\n",
            "Epoch 180: Step 3063: Generator loss: 0.7176912426948547,discriminator loss: 0.6922802925109863\n",
            "Epoch 180: Step 3064: Generator loss: 0.71812903881073,discriminator loss: 0.6920664310455322\n",
            "Epoch 180: Step 3065: Generator loss: 0.7184988260269165,discriminator loss: 0.692150354385376\n",
            "Epoch 180: Step 3066: Generator loss: 0.7187990546226501,discriminator loss: 0.6920984983444214\n",
            "Epoch 180: Step 3067: Generator loss: 0.7191531658172607,discriminator loss: 0.6919726729393005\n",
            "Epoch 180: Step 3068: Generator loss: 0.7191588878631592,discriminator loss: 0.6917955279350281\n",
            "Epoch 180: Step 3069: Generator loss: 0.7195695638656616,discriminator loss: 0.6916544437408447\n",
            "Epoch 180: Step 3070: Generator loss: 0.7198400497436523,discriminator loss: 0.6912373900413513\n",
            "Epoch 180: Step 3071: Generator loss: 0.7204270958900452,discriminator loss: 0.69157874584198\n",
            "Epoch 180: Step 3072: Generator loss: 0.7207987308502197,discriminator loss: 0.6916431784629822\n",
            "Epoch 180: Step 3073: Generator loss: 0.7210913896560669,discriminator loss: 0.6911196708679199\n",
            "Epoch 180: Step 3074: Generator loss: 0.7213059663772583,discriminator loss: 0.6911989450454712\n",
            "Epoch 180: Step 3075: Generator loss: 0.7217385768890381,discriminator loss: 0.6910606622695923\n",
            "Epoch 180: Step 3076: Generator loss: 0.7214857339859009,discriminator loss: 0.6899628043174744\n",
            "Epoch 180\n",
            "accuracy:  0.7450236966824643\n",
            "Epoch 190: Step 3230: Generator loss: 0.6722127795219421,discriminator loss: 0.6936951875686646\n",
            "Epoch 190: Step 3231: Generator loss: 0.6720084547996521,discriminator loss: 0.6935888528823853\n",
            "Epoch 190: Step 3232: Generator loss: 0.6717730164527893,discriminator loss: 0.6935992240905762\n",
            "Epoch 190: Step 3233: Generator loss: 0.6716079711914062,discriminator loss: 0.6933678388595581\n",
            "Epoch 190: Step 3234: Generator loss: 0.671421468257904,discriminator loss: 0.6933760643005371\n",
            "Epoch 190: Step 3235: Generator loss: 0.6712454557418823,discriminator loss: 0.6931777000427246\n",
            "Epoch 190: Step 3236: Generator loss: 0.6710511445999146,discriminator loss: 0.6931889653205872\n",
            "Epoch 190: Step 3237: Generator loss: 0.6709528565406799,discriminator loss: 0.6930748224258423\n",
            "Epoch 190: Step 3238: Generator loss: 0.6707140803337097,discriminator loss: 0.6930376291275024\n",
            "Epoch 190: Step 3239: Generator loss: 0.6705996990203857,discriminator loss: 0.6929337978363037\n",
            "Epoch 190: Step 3240: Generator loss: 0.6704413890838623,discriminator loss: 0.6928084492683411\n",
            "Epoch 190: Step 3241: Generator loss: 0.6703394055366516,discriminator loss: 0.6927838921546936\n",
            "Epoch 190: Step 3242: Generator loss: 0.670259952545166,discriminator loss: 0.6926383972167969\n",
            "Epoch 190: Step 3243: Generator loss: 0.670132577419281,discriminator loss: 0.6923730969429016\n",
            "Epoch 190: Step 3244: Generator loss: 0.6700747013092041,discriminator loss: 0.6925562024116516\n",
            "Epoch 190: Step 3245: Generator loss: 0.6700180768966675,discriminator loss: 0.6924999952316284\n",
            "Epoch 190: Step 3246: Generator loss: 0.669853150844574,discriminator loss: 0.6925070285797119\n",
            "Epoch 190\n",
            "accuracy:  0.7815989280928987\n",
            "Epoch 200: Step 3400: Generator loss: 0.7164196968078613,discriminator loss: 0.6944334506988525\n",
            "Epoch 200: Step 3401: Generator loss: 0.7168949246406555,discriminator loss: 0.6941641569137573\n",
            "Epoch 200: Step 3402: Generator loss: 0.7173984050750732,discriminator loss: 0.6942139863967896\n",
            "Epoch 200: Step 3403: Generator loss: 0.7182602286338806,discriminator loss: 0.6941181421279907\n",
            "Epoch 200: Step 3404: Generator loss: 0.7187391519546509,discriminator loss: 0.6938905715942383\n",
            "Epoch 200: Step 3405: Generator loss: 0.7195541262626648,discriminator loss: 0.6935641169548035\n",
            "Epoch 200: Step 3406: Generator loss: 0.7203168869018555,discriminator loss: 0.6937710046768188\n",
            "Epoch 200: Step 3407: Generator loss: 0.7209309339523315,discriminator loss: 0.6933079957962036\n",
            "Epoch 200: Step 3408: Generator loss: 0.7215549945831299,discriminator loss: 0.6930192708969116\n",
            "Epoch 200: Step 3409: Generator loss: 0.7221314311027527,discriminator loss: 0.6932779550552368\n",
            "Epoch 200: Step 3410: Generator loss: 0.7226825952529907,discriminator loss: 0.6929739713668823\n",
            "Epoch 200: Step 3411: Generator loss: 0.7233235836029053,discriminator loss: 0.6929366588592529\n",
            "Epoch 200: Step 3412: Generator loss: 0.7238156795501709,discriminator loss: 0.6927143335342407\n",
            "Epoch 200: Step 3413: Generator loss: 0.724400520324707,discriminator loss: 0.6926566362380981\n",
            "Epoch 200: Step 3414: Generator loss: 0.7246842384338379,discriminator loss: 0.6925340890884399\n",
            "Epoch 200: Step 3415: Generator loss: 0.7250514030456543,discriminator loss: 0.6923871040344238\n",
            "Epoch 200: Step 3416: Generator loss: 0.7242130637168884,discriminator loss: 0.6921508312225342\n",
            "Epoch 200\n",
            "accuracy:  0.7501064282673479\n",
            "Epoch 210: Step 3570: Generator loss: 0.677803635597229,discriminator loss: 0.6939990520477295\n",
            "Epoch 210: Step 3571: Generator loss: 0.6774558424949646,discriminator loss: 0.6937928199768066\n",
            "Epoch 210: Step 3572: Generator loss: 0.6771224737167358,discriminator loss: 0.6937713027000427\n",
            "Epoch 210: Step 3573: Generator loss: 0.6769092679023743,discriminator loss: 0.6936781406402588\n",
            "Epoch 210: Step 3574: Generator loss: 0.6766701936721802,discriminator loss: 0.6938016414642334\n",
            "Epoch 210: Step 3575: Generator loss: 0.6763331890106201,discriminator loss: 0.6936521530151367\n",
            "Epoch 210: Step 3576: Generator loss: 0.6760299205780029,discriminator loss: 0.6934820413589478\n",
            "Epoch 210: Step 3577: Generator loss: 0.6756172776222229,discriminator loss: 0.693514347076416\n",
            "Epoch 210: Step 3578: Generator loss: 0.67542964220047,discriminator loss: 0.693386435508728\n",
            "Epoch 210: Step 3579: Generator loss: 0.6752126216888428,discriminator loss: 0.6934008598327637\n",
            "Epoch 210: Step 3580: Generator loss: 0.67487633228302,discriminator loss: 0.6934788823127747\n",
            "Epoch 210: Step 3581: Generator loss: 0.6746805906295776,discriminator loss: 0.6932681798934937\n",
            "Epoch 210: Step 3582: Generator loss: 0.674346923828125,discriminator loss: 0.6932059526443481\n",
            "Epoch 210: Step 3583: Generator loss: 0.6741860508918762,discriminator loss: 0.6932156085968018\n",
            "Epoch 210: Step 3584: Generator loss: 0.6739982962608337,discriminator loss: 0.6931413412094116\n",
            "Epoch 210: Step 3585: Generator loss: 0.6735762357711792,discriminator loss: 0.6931569576263428\n",
            "Epoch 210: Step 3586: Generator loss: 0.67369145154953,discriminator loss: 0.6932411789894104\n",
            "Epoch 210\n",
            "accuracy:  0.7877984084880636\n",
            "Epoch 220: Step 3740: Generator loss: 0.7068841457366943,discriminator loss: 0.6954569220542908\n",
            "Epoch 220: Step 3741: Generator loss: 0.70697420835495,discriminator loss: 0.6951329708099365\n",
            "Epoch 220: Step 3742: Generator loss: 0.7075448036193848,discriminator loss: 0.6950054168701172\n",
            "Epoch 220: Step 3743: Generator loss: 0.7075170874595642,discriminator loss: 0.6950550079345703\n",
            "Epoch 220: Step 3744: Generator loss: 0.7073855400085449,discriminator loss: 0.6951213479042053\n",
            "Epoch 220: Step 3745: Generator loss: 0.7078566551208496,discriminator loss: 0.6950522661209106\n",
            "Epoch 220: Step 3746: Generator loss: 0.707785964012146,discriminator loss: 0.6947242617607117\n",
            "Epoch 220: Step 3747: Generator loss: 0.7081421613693237,discriminator loss: 0.694783091545105\n",
            "Epoch 220: Step 3748: Generator loss: 0.7078149318695068,discriminator loss: 0.6947758793830872\n",
            "Epoch 220: Step 3749: Generator loss: 0.7082749605178833,discriminator loss: 0.6951221823692322\n",
            "Epoch 220: Step 3750: Generator loss: 0.708422064781189,discriminator loss: 0.6948297023773193\n",
            "Epoch 220: Step 3751: Generator loss: 0.7084980607032776,discriminator loss: 0.6947999000549316\n",
            "Epoch 220: Step 3752: Generator loss: 0.7089111804962158,discriminator loss: 0.6947282552719116\n",
            "Epoch 220: Step 3753: Generator loss: 0.708759605884552,discriminator loss: 0.6947033405303955\n",
            "Epoch 220: Step 3754: Generator loss: 0.7091214656829834,discriminator loss: 0.6945156455039978\n",
            "Epoch 220: Step 3755: Generator loss: 0.7091094255447388,discriminator loss: 0.6945632696151733\n",
            "Epoch 220: Step 3756: Generator loss: 0.7100974917411804,discriminator loss: 0.6945021152496338\n",
            "Epoch 220\n",
            "accuracy:  0.7697974217311234\n",
            "Epoch 230: Step 3910: Generator loss: 0.68743896484375,discriminator loss: 0.6954438090324402\n",
            "Epoch 230: Step 3911: Generator loss: 0.6875661611557007,discriminator loss: 0.6954171657562256\n",
            "Epoch 230: Step 3912: Generator loss: 0.687807559967041,discriminator loss: 0.6953994035720825\n",
            "Epoch 230: Step 3913: Generator loss: 0.687945544719696,discriminator loss: 0.6950112581253052\n",
            "Epoch 230: Step 3914: Generator loss: 0.6880643963813782,discriminator loss: 0.6954655051231384\n",
            "Epoch 230: Step 3915: Generator loss: 0.6884127259254456,discriminator loss: 0.6950731873512268\n",
            "Epoch 230: Step 3916: Generator loss: 0.6885744333267212,discriminator loss: 0.6949818134307861\n",
            "Epoch 230: Step 3917: Generator loss: 0.6885492205619812,discriminator loss: 0.6955559253692627\n",
            "Epoch 230: Step 3918: Generator loss: 0.6886764764785767,discriminator loss: 0.6946656107902527\n",
            "Epoch 230: Step 3919: Generator loss: 0.6889627575874329,discriminator loss: 0.6948720812797546\n",
            "Epoch 230: Step 3920: Generator loss: 0.6890352964401245,discriminator loss: 0.6943684816360474\n",
            "Epoch 230: Step 3921: Generator loss: 0.6893447041511536,discriminator loss: 0.6947015523910522\n",
            "Epoch 230: Step 3922: Generator loss: 0.6893604397773743,discriminator loss: 0.6946202516555786\n",
            "Epoch 230: Step 3923: Generator loss: 0.6894198656082153,discriminator loss: 0.6946829557418823\n",
            "Epoch 230: Step 3924: Generator loss: 0.689602792263031,discriminator loss: 0.694420337677002\n",
            "Epoch 230: Step 3925: Generator loss: 0.6897265911102295,discriminator loss: 0.6935534477233887\n",
            "Epoch 230: Step 3926: Generator loss: 0.6901443004608154,discriminator loss: 0.6931771039962769\n",
            "Epoch 230\n",
            "accuracy:  0.7757731958762886\n",
            "Epoch 240: Step 4080: Generator loss: 0.6846145391464233,discriminator loss: 0.6939313411712646\n",
            "Epoch 240: Step 4081: Generator loss: 0.6853733062744141,discriminator loss: 0.6955145597457886\n",
            "Epoch 240: Step 4082: Generator loss: 0.685794472694397,discriminator loss: 0.6955458521842957\n",
            "Epoch 240: Step 4083: Generator loss: 0.686130166053772,discriminator loss: 0.6939896941184998\n",
            "Epoch 240: Step 4084: Generator loss: 0.6859375238418579,discriminator loss: 0.6951196789741516\n",
            "Epoch 240: Step 4085: Generator loss: 0.6877908110618591,discriminator loss: 0.6937863826751709\n",
            "Epoch 240: Step 4086: Generator loss: 0.6871628761291504,discriminator loss: 0.6952722668647766\n",
            "Epoch 240: Step 4087: Generator loss: 0.6885346174240112,discriminator loss: 0.6943458914756775\n",
            "Epoch 240: Step 4088: Generator loss: 0.6893417835235596,discriminator loss: 0.6940625905990601\n",
            "Epoch 240: Step 4089: Generator loss: 0.6895194053649902,discriminator loss: 0.6945057511329651\n",
            "Epoch 240: Step 4090: Generator loss: 0.6893346309661865,discriminator loss: 0.6946548223495483\n",
            "Epoch 240: Step 4091: Generator loss: 0.6900808811187744,discriminator loss: 0.6942761540412903\n",
            "Epoch 240: Step 4092: Generator loss: 0.690227746963501,discriminator loss: 0.6944241523742676\n",
            "Epoch 240: Step 4093: Generator loss: 0.6912434697151184,discriminator loss: 0.6948074102401733\n",
            "Epoch 240: Step 4094: Generator loss: 0.691724419593811,discriminator loss: 0.694958508014679\n",
            "Epoch 240: Step 4095: Generator loss: 0.6919814348220825,discriminator loss: 0.6945048570632935\n",
            "Epoch 240: Step 4096: Generator loss: 0.6938862800598145,discriminator loss: 0.6949291229248047\n",
            "Epoch 240\n",
            "accuracy:  0.7921967769296013\n",
            "Epoch 250: Step 4250: Generator loss: 0.696549654006958,discriminator loss: 0.6943315863609314\n",
            "Epoch 250: Step 4251: Generator loss: 0.6958708763122559,discriminator loss: 0.6954820156097412\n",
            "Epoch 250: Step 4252: Generator loss: 0.6951883435249329,discriminator loss: 0.6948828101158142\n",
            "Epoch 250: Step 4253: Generator loss: 0.6946655511856079,discriminator loss: 0.6941171288490295\n",
            "Epoch 250: Step 4254: Generator loss: 0.6937868595123291,discriminator loss: 0.694835901260376\n",
            "Epoch 250: Step 4255: Generator loss: 0.6931405067443848,discriminator loss: 0.694383442401886\n",
            "Epoch 250: Step 4256: Generator loss: 0.692700207233429,discriminator loss: 0.6944271326065063\n",
            "Epoch 250: Step 4257: Generator loss: 0.6921840906143188,discriminator loss: 0.6945489645004272\n",
            "Epoch 250: Step 4258: Generator loss: 0.6914816498756409,discriminator loss: 0.6952670812606812\n",
            "Epoch 250: Step 4259: Generator loss: 0.6909726858139038,discriminator loss: 0.6944904327392578\n",
            "Epoch 250: Step 4260: Generator loss: 0.6902360916137695,discriminator loss: 0.6942682266235352\n",
            "Epoch 250: Step 4261: Generator loss: 0.6899861097335815,discriminator loss: 0.6954028010368347\n",
            "Epoch 250: Step 4262: Generator loss: 0.6896710991859436,discriminator loss: 0.694815993309021\n",
            "Epoch 250: Step 4263: Generator loss: 0.6888366341590881,discriminator loss: 0.6951688528060913\n",
            "Epoch 250: Step 4264: Generator loss: 0.688212513923645,discriminator loss: 0.6945852041244507\n",
            "Epoch 250: Step 4265: Generator loss: 0.6876135468482971,discriminator loss: 0.6954634189605713\n",
            "Epoch 250: Step 4266: Generator loss: 0.6875988841056824,discriminator loss: 0.6950551867485046\n",
            "Epoch 250\n",
            "accuracy:  0.7613861386138614\n",
            "Epoch 260: Step 4420: Generator loss: 0.6827807426452637,discriminator loss: 0.6927937865257263\n",
            "Epoch 260: Step 4421: Generator loss: 0.6830577850341797,discriminator loss: 0.6940308809280396\n",
            "Epoch 260: Step 4422: Generator loss: 0.6836433410644531,discriminator loss: 0.6933799982070923\n",
            "Epoch 260: Step 4423: Generator loss: 0.6843905448913574,discriminator loss: 0.6938585042953491\n",
            "Epoch 260: Step 4424: Generator loss: 0.6840627193450928,discriminator loss: 0.6946655511856079\n",
            "Epoch 260: Step 4425: Generator loss: 0.6849574446678162,discriminator loss: 0.6936302781105042\n",
            "Epoch 260: Step 4426: Generator loss: 0.685498833656311,discriminator loss: 0.6944975852966309\n",
            "Epoch 260: Step 4427: Generator loss: 0.6859213709831238,discriminator loss: 0.6943744421005249\n",
            "Epoch 260: Step 4428: Generator loss: 0.6862214207649231,discriminator loss: 0.6930547952651978\n",
            "Epoch 260: Step 4429: Generator loss: 0.6864989995956421,discriminator loss: 0.6945738196372986\n",
            "Epoch 260: Step 4430: Generator loss: 0.6870802640914917,discriminator loss: 0.6933228969573975\n",
            "Epoch 260: Step 4431: Generator loss: 0.6875177621841431,discriminator loss: 0.694570779800415\n",
            "Epoch 260: Step 4432: Generator loss: 0.6880576610565186,discriminator loss: 0.694011390209198\n",
            "Epoch 260: Step 4433: Generator loss: 0.6886343359947205,discriminator loss: 0.6939073801040649\n",
            "Epoch 260: Step 4434: Generator loss: 0.688288152217865,discriminator loss: 0.6946893930435181\n",
            "Epoch 260: Step 4435: Generator loss: 0.6892415881156921,discriminator loss: 0.693577229976654\n",
            "Epoch 260: Step 4436: Generator loss: 0.6895219683647156,discriminator loss: 0.69390869140625\n",
            "Epoch 260\n",
            "accuracy:  0.7510955302366346\n",
            "Epoch 270: Step 4590: Generator loss: 0.705391526222229,discriminator loss: 0.6930147409439087\n",
            "Epoch 270: Step 4591: Generator loss: 0.7053719758987427,discriminator loss: 0.6925954818725586\n",
            "Epoch 270: Step 4592: Generator loss: 0.7049727439880371,discriminator loss: 0.6921400427818298\n",
            "Epoch 270: Step 4593: Generator loss: 0.7046680450439453,discriminator loss: 0.6941885948181152\n",
            "Epoch 270: Step 4594: Generator loss: 0.7041769027709961,discriminator loss: 0.6925435066223145\n",
            "Epoch 270: Step 4595: Generator loss: 0.7039471864700317,discriminator loss: 0.692949652671814\n",
            "Epoch 270: Step 4596: Generator loss: 0.703498363494873,discriminator loss: 0.6925016641616821\n",
            "Epoch 270: Step 4597: Generator loss: 0.7033518552780151,discriminator loss: 0.693932056427002\n",
            "Epoch 270: Step 4598: Generator loss: 0.703170657157898,discriminator loss: 0.6924229860305786\n",
            "Epoch 270: Step 4599: Generator loss: 0.7027777433395386,discriminator loss: 0.6929162740707397\n",
            "Epoch 270: Step 4600: Generator loss: 0.7020707130432129,discriminator loss: 0.692479133605957\n",
            "Epoch 270: Step 4601: Generator loss: 0.701715350151062,discriminator loss: 0.6922732591629028\n",
            "Epoch 270: Step 4602: Generator loss: 0.7017892599105835,discriminator loss: 0.6925748586654663\n",
            "Epoch 270: Step 4603: Generator loss: 0.7012046575546265,discriminator loss: 0.6925751566886902\n",
            "Epoch 270: Step 4604: Generator loss: 0.7008476853370667,discriminator loss: 0.6927770376205444\n",
            "Epoch 270: Step 4605: Generator loss: 0.7006667256355286,discriminator loss: 0.6929024457931519\n",
            "Epoch 270: Step 4606: Generator loss: 0.7002695798873901,discriminator loss: 0.6937478184700012\n",
            "Epoch 270\n",
            "accuracy:  0.791895314478683\n",
            "Epoch 280: Step 4760: Generator loss: 0.6856698989868164,discriminator loss: 0.6926126480102539\n",
            "Epoch 280: Step 4761: Generator loss: 0.6859573125839233,discriminator loss: 0.6927285194396973\n",
            "Epoch 280: Step 4762: Generator loss: 0.6861454844474792,discriminator loss: 0.6930170059204102\n",
            "Epoch 280: Step 4763: Generator loss: 0.6861885786056519,discriminator loss: 0.6924524307250977\n",
            "Epoch 280: Step 4764: Generator loss: 0.6864897012710571,discriminator loss: 0.6925554275512695\n",
            "Epoch 280: Step 4765: Generator loss: 0.6867071390151978,discriminator loss: 0.6927351951599121\n",
            "Epoch 280: Step 4766: Generator loss: 0.686974048614502,discriminator loss: 0.6924111843109131\n",
            "Epoch 280: Step 4767: Generator loss: 0.687079668045044,discriminator loss: 0.692682683467865\n",
            "Epoch 280: Step 4768: Generator loss: 0.687429666519165,discriminator loss: 0.6919708251953125\n",
            "Epoch 280: Step 4769: Generator loss: 0.6877831816673279,discriminator loss: 0.6923398971557617\n",
            "Epoch 280: Step 4770: Generator loss: 0.6878314018249512,discriminator loss: 0.6923084259033203\n",
            "Epoch 280: Step 4771: Generator loss: 0.6882151365280151,discriminator loss: 0.6924961805343628\n",
            "Epoch 280: Step 4772: Generator loss: 0.6885315775871277,discriminator loss: 0.6922879219055176\n",
            "Epoch 280: Step 4773: Generator loss: 0.6886599063873291,discriminator loss: 0.6925989985466003\n",
            "Epoch 280: Step 4774: Generator loss: 0.6890243291854858,discriminator loss: 0.6922370791435242\n",
            "Epoch 280: Step 4775: Generator loss: 0.6889350414276123,discriminator loss: 0.6926524639129639\n",
            "Epoch 280: Step 4776: Generator loss: 0.6891526579856873,discriminator loss: 0.6923072338104248\n",
            "Epoch 280\n",
            "accuracy:  0.7723316536875858\n",
            "Epoch 290: Step 4930: Generator loss: 0.7000930309295654,discriminator loss: 0.6926257610321045\n",
            "Epoch 290: Step 4931: Generator loss: 0.7002060413360596,discriminator loss: 0.6922320127487183\n",
            "Epoch 290: Step 4932: Generator loss: 0.6998576521873474,discriminator loss: 0.6926112174987793\n",
            "Epoch 290: Step 4933: Generator loss: 0.6997159719467163,discriminator loss: 0.6923787593841553\n",
            "Epoch 290: Step 4934: Generator loss: 0.6999698877334595,discriminator loss: 0.6923943758010864\n",
            "Epoch 290: Step 4935: Generator loss: 0.6996893882751465,discriminator loss: 0.692356288433075\n",
            "Epoch 290: Step 4936: Generator loss: 0.6995059847831726,discriminator loss: 0.6920289993286133\n",
            "Epoch 290: Step 4937: Generator loss: 0.6994942426681519,discriminator loss: 0.6922215223312378\n",
            "Epoch 290: Step 4938: Generator loss: 0.6995366811752319,discriminator loss: 0.6927680969238281\n",
            "Epoch 290: Step 4939: Generator loss: 0.6992530822753906,discriminator loss: 0.692314863204956\n",
            "Epoch 290: Step 4940: Generator loss: 0.6992341876029968,discriminator loss: 0.6924129724502563\n",
            "Epoch 290: Step 4941: Generator loss: 0.699182391166687,discriminator loss: 0.6928128004074097\n",
            "Epoch 290: Step 4942: Generator loss: 0.6989830732345581,discriminator loss: 0.6923985481262207\n",
            "Epoch 290: Step 4943: Generator loss: 0.6989108324050903,discriminator loss: 0.6925111413002014\n",
            "Epoch 290: Step 4944: Generator loss: 0.6986714601516724,discriminator loss: 0.6925854682922363\n",
            "Epoch 290: Step 4945: Generator loss: 0.6984769105911255,discriminator loss: 0.6925550699234009\n",
            "Epoch 290: Step 4946: Generator loss: 0.6986140012741089,discriminator loss: 0.691794753074646\n",
            "Epoch 290\n",
            "accuracy:  0.7589367552703941\n",
            "Epoch 300: Step 5100: Generator loss: 0.6877856254577637,discriminator loss: 0.6930292844772339\n",
            "Epoch 300: Step 5101: Generator loss: 0.6877497434616089,discriminator loss: 0.692893385887146\n",
            "Epoch 300: Step 5102: Generator loss: 0.6875576972961426,discriminator loss: 0.6928712129592896\n",
            "Epoch 300: Step 5103: Generator loss: 0.6873286366462708,discriminator loss: 0.6922091841697693\n",
            "Epoch 300: Step 5104: Generator loss: 0.6872071623802185,discriminator loss: 0.6929335594177246\n",
            "Epoch 300: Step 5105: Generator loss: 0.6871029734611511,discriminator loss: 0.6931260824203491\n",
            "Epoch 300: Step 5106: Generator loss: 0.687365710735321,discriminator loss: 0.6928936243057251\n",
            "Epoch 300: Step 5107: Generator loss: 0.6871939301490784,discriminator loss: 0.6927384734153748\n",
            "Epoch 300: Step 5108: Generator loss: 0.687056303024292,discriminator loss: 0.6929406523704529\n",
            "Epoch 300: Step 5109: Generator loss: 0.6871802806854248,discriminator loss: 0.6927538514137268\n",
            "Epoch 300: Step 5110: Generator loss: 0.6869972944259644,discriminator loss: 0.6928973197937012\n",
            "Epoch 300: Step 5111: Generator loss: 0.6870967149734497,discriminator loss: 0.6927657127380371\n",
            "Epoch 300: Step 5112: Generator loss: 0.6872897744178772,discriminator loss: 0.6932868957519531\n",
            "Epoch 300: Step 5113: Generator loss: 0.6872539520263672,discriminator loss: 0.6927990913391113\n",
            "Epoch 300: Step 5114: Generator loss: 0.6870157718658447,discriminator loss: 0.693385899066925\n",
            "Epoch 300: Step 5115: Generator loss: 0.687232255935669,discriminator loss: 0.6933130621910095\n",
            "Epoch 300: Step 5116: Generator loss: 0.6871509552001953,discriminator loss: 0.6932039260864258\n",
            "Epoch 300\n",
            "accuracy:  0.7718491260349587\n",
            "Epoch 310: Step 5270: Generator loss: 0.7020373344421387,discriminator loss: 0.693408727645874\n",
            "Epoch 310: Step 5271: Generator loss: 0.7020783424377441,discriminator loss: 0.6930427551269531\n",
            "Epoch 310: Step 5272: Generator loss: 0.7020360827445984,discriminator loss: 0.6930360794067383\n",
            "Epoch 310: Step 5273: Generator loss: 0.7018042802810669,discriminator loss: 0.6931173205375671\n",
            "Epoch 310: Step 5274: Generator loss: 0.701846718788147,discriminator loss: 0.6929121613502502\n",
            "Epoch 310: Step 5275: Generator loss: 0.7019034624099731,discriminator loss: 0.6932650208473206\n",
            "Epoch 310: Step 5276: Generator loss: 0.7019978761672974,discriminator loss: 0.6934777498245239\n",
            "Epoch 310: Step 5277: Generator loss: 0.7018458843231201,discriminator loss: 0.6929786205291748\n",
            "Epoch 310: Step 5278: Generator loss: 0.701783299446106,discriminator loss: 0.6936404705047607\n",
            "Epoch 310: Step 5279: Generator loss: 0.7016124725341797,discriminator loss: 0.6938897371292114\n",
            "Epoch 310: Step 5280: Generator loss: 0.7014329433441162,discriminator loss: 0.6930418014526367\n",
            "Epoch 310: Step 5281: Generator loss: 0.7012521028518677,discriminator loss: 0.6935840845108032\n",
            "Epoch 310: Step 5282: Generator loss: 0.7009896039962769,discriminator loss: 0.6928855776786804\n",
            "Epoch 310: Step 5283: Generator loss: 0.7007764577865601,discriminator loss: 0.6930133104324341\n",
            "Epoch 310: Step 5284: Generator loss: 0.7007416486740112,discriminator loss: 0.6926563382148743\n",
            "Epoch 310: Step 5285: Generator loss: 0.7007299065589905,discriminator loss: 0.6931610107421875\n",
            "Epoch 310: Step 5286: Generator loss: 0.7003395557403564,discriminator loss: 0.6921060085296631\n",
            "Epoch 310\n",
            "accuracy:  0.8010586678429643\n",
            "Epoch 320: Step 5440: Generator loss: 0.6827138066291809,discriminator loss: 0.6920134425163269\n",
            "Epoch 320: Step 5441: Generator loss: 0.6837978363037109,discriminator loss: 0.6932373642921448\n",
            "Epoch 320: Step 5442: Generator loss: 0.6840163469314575,discriminator loss: 0.6936780214309692\n",
            "Epoch 320: Step 5443: Generator loss: 0.6845439672470093,discriminator loss: 0.6933267712593079\n",
            "Epoch 320: Step 5444: Generator loss: 0.6849174499511719,discriminator loss: 0.6934710741043091\n",
            "Epoch 320: Step 5445: Generator loss: 0.6857547760009766,discriminator loss: 0.6926825046539307\n",
            "Epoch 320: Step 5446: Generator loss: 0.6857876777648926,discriminator loss: 0.6920366287231445\n",
            "Epoch 320: Step 5447: Generator loss: 0.6865749359130859,discriminator loss: 0.6929511427879333\n",
            "Epoch 320: Step 5448: Generator loss: 0.6871891617774963,discriminator loss: 0.6923792362213135\n",
            "Epoch 320: Step 5449: Generator loss: 0.6877014636993408,discriminator loss: 0.6933465003967285\n",
            "Epoch 320: Step 5450: Generator loss: 0.6879255771636963,discriminator loss: 0.6926945447921753\n",
            "Epoch 320: Step 5451: Generator loss: 0.6887359619140625,discriminator loss: 0.6928548812866211\n",
            "Epoch 320: Step 5452: Generator loss: 0.6893297433853149,discriminator loss: 0.6923692226409912\n",
            "Epoch 320: Step 5453: Generator loss: 0.6897203922271729,discriminator loss: 0.6925628185272217\n",
            "Epoch 320: Step 5454: Generator loss: 0.689983606338501,discriminator loss: 0.6933448314666748\n",
            "Epoch 320: Step 5455: Generator loss: 0.6907059550285339,discriminator loss: 0.6930302381515503\n",
            "Epoch 320: Step 5456: Generator loss: 0.6912597417831421,discriminator loss: 0.6933096647262573\n",
            "Epoch 320\n",
            "accuracy:  0.8052287581699346\n",
            "Epoch 330: Step 5610: Generator loss: 0.6827923059463501,discriminator loss: 0.6942889094352722\n",
            "Epoch 330: Step 5611: Generator loss: 0.6824392080307007,discriminator loss: 0.6947999596595764\n",
            "Epoch 330: Step 5612: Generator loss: 0.6821117997169495,discriminator loss: 0.694827139377594\n",
            "Epoch 330: Step 5613: Generator loss: 0.6819348335266113,discriminator loss: 0.6943298578262329\n",
            "Epoch 330: Step 5614: Generator loss: 0.6814119815826416,discriminator loss: 0.6947900056838989\n",
            "Epoch 330: Step 5615: Generator loss: 0.6811505556106567,discriminator loss: 0.694453239440918\n",
            "Epoch 330: Step 5616: Generator loss: 0.6808031797409058,discriminator loss: 0.6944417357444763\n",
            "Epoch 330: Step 5617: Generator loss: 0.6804766654968262,discriminator loss: 0.6943418979644775\n",
            "Epoch 330: Step 5618: Generator loss: 0.6801354885101318,discriminator loss: 0.6945801377296448\n",
            "Epoch 330: Step 5619: Generator loss: 0.6798525452613831,discriminator loss: 0.6945036053657532\n",
            "Epoch 330: Step 5620: Generator loss: 0.6794849634170532,discriminator loss: 0.6942835450172424\n",
            "Epoch 330: Step 5621: Generator loss: 0.6791645288467407,discriminator loss: 0.6944701671600342\n",
            "Epoch 330: Step 5622: Generator loss: 0.6788215637207031,discriminator loss: 0.6942117214202881\n",
            "Epoch 330: Step 5623: Generator loss: 0.6785470843315125,discriminator loss: 0.694128155708313\n",
            "Epoch 330: Step 5624: Generator loss: 0.6783440709114075,discriminator loss: 0.6941714882850647\n",
            "Epoch 330: Step 5625: Generator loss: 0.6780458092689514,discriminator loss: 0.6941113471984863\n",
            "Epoch 330: Step 5626: Generator loss: 0.6779317855834961,discriminator loss: 0.6939418315887451\n",
            "Epoch 330\n",
            "accuracy:  0.7636199909950473\n",
            "Epoch 340: Step 5780: Generator loss: 0.719870924949646,discriminator loss: 0.6934878826141357\n",
            "Epoch 340: Step 5781: Generator loss: 0.7202290296554565,discriminator loss: 0.6933596134185791\n",
            "Epoch 340: Step 5782: Generator loss: 0.720808744430542,discriminator loss: 0.6934359669685364\n",
            "Epoch 340: Step 5783: Generator loss: 0.7216635942459106,discriminator loss: 0.693148672580719\n",
            "Epoch 340: Step 5784: Generator loss: 0.7220815420150757,discriminator loss: 0.6931391358375549\n",
            "Epoch 340: Step 5785: Generator loss: 0.722698986530304,discriminator loss: 0.6929776072502136\n",
            "Epoch 340: Step 5786: Generator loss: 0.7233433723449707,discriminator loss: 0.692807137966156\n",
            "Epoch 340: Step 5787: Generator loss: 0.7237024307250977,discriminator loss: 0.6927566528320312\n",
            "Epoch 340: Step 5788: Generator loss: 0.7245055437088013,discriminator loss: 0.6927294731140137\n",
            "Epoch 340: Step 5789: Generator loss: 0.7249249815940857,discriminator loss: 0.6926901936531067\n",
            "Epoch 340: Step 5790: Generator loss: 0.725350558757782,discriminator loss: 0.6924360990524292\n",
            "Epoch 340: Step 5791: Generator loss: 0.7260364294052124,discriminator loss: 0.6925168037414551\n",
            "Epoch 340: Step 5792: Generator loss: 0.7260436415672302,discriminator loss: 0.6923301219940186\n",
            "Epoch 340: Step 5793: Generator loss: 0.7267415523529053,discriminator loss: 0.6922376751899719\n",
            "Epoch 340: Step 5794: Generator loss: 0.7272449731826782,discriminator loss: 0.6922869682312012\n",
            "Epoch 340: Step 5795: Generator loss: 0.7270708680152893,discriminator loss: 0.6920545697212219\n",
            "Epoch 340: Step 5796: Generator loss: 0.7272751331329346,discriminator loss: 0.6919692158699036\n",
            "Epoch 340\n",
            "accuracy:  0.8092592592592593\n",
            "Epoch 350: Step 5950: Generator loss: 0.6700286865234375,discriminator loss: 0.6929027438163757\n",
            "Epoch 350: Step 5951: Generator loss: 0.6702156662940979,discriminator loss: 0.6930186748504639\n",
            "Epoch 350: Step 5952: Generator loss: 0.6702382564544678,discriminator loss: 0.6927928924560547\n",
            "Epoch 350: Step 5953: Generator loss: 0.6702755689620972,discriminator loss: 0.692787766456604\n",
            "Epoch 350: Step 5954: Generator loss: 0.6703851222991943,discriminator loss: 0.6926015615463257\n",
            "Epoch 350: Step 5955: Generator loss: 0.6704012751579285,discriminator loss: 0.6928203105926514\n",
            "Epoch 350: Step 5956: Generator loss: 0.6705513596534729,discriminator loss: 0.692470908164978\n",
            "Epoch 350: Step 5957: Generator loss: 0.6705435514450073,discriminator loss: 0.6925584077835083\n",
            "Epoch 350: Step 5958: Generator loss: 0.6706434488296509,discriminator loss: 0.6925948858261108\n",
            "Epoch 350: Step 5959: Generator loss: 0.6706826686859131,discriminator loss: 0.69236159324646\n",
            "Epoch 350: Step 5960: Generator loss: 0.6709994673728943,discriminator loss: 0.6926248073577881\n",
            "Epoch 350: Step 5961: Generator loss: 0.6710423231124878,discriminator loss: 0.6923912763595581\n",
            "Epoch 350: Step 5962: Generator loss: 0.6712332963943481,discriminator loss: 0.6923264265060425\n",
            "Epoch 350: Step 5963: Generator loss: 0.6713335514068604,discriminator loss: 0.6925734281539917\n",
            "Epoch 350: Step 5964: Generator loss: 0.6715835332870483,discriminator loss: 0.6924257874488831\n",
            "Epoch 350: Step 5965: Generator loss: 0.6716277003288269,discriminator loss: 0.6920634508132935\n",
            "Epoch 350: Step 5966: Generator loss: 0.6721615791320801,discriminator loss: 0.6919360160827637\n",
            "Epoch 350\n",
            "accuracy:  0.7773620205799813\n",
            "Epoch 360: Step 6120: Generator loss: 0.7214376330375671,discriminator loss: 0.6913660764694214\n",
            "Epoch 360: Step 6121: Generator loss: 0.7220644950866699,discriminator loss: 0.6909863948822021\n",
            "Epoch 360: Step 6122: Generator loss: 0.722275972366333,discriminator loss: 0.6910502910614014\n",
            "Epoch 360: Step 6123: Generator loss: 0.7220453023910522,discriminator loss: 0.6910942196846008\n",
            "Epoch 360: Step 6124: Generator loss: 0.7219970226287842,discriminator loss: 0.6905734539031982\n",
            "Epoch 360: Step 6125: Generator loss: 0.7223566770553589,discriminator loss: 0.6907464265823364\n",
            "Epoch 360: Step 6126: Generator loss: 0.7226899266242981,discriminator loss: 0.6906533241271973\n",
            "Epoch 360: Step 6127: Generator loss: 0.7222050428390503,discriminator loss: 0.6907535791397095\n",
            "Epoch 360: Step 6128: Generator loss: 0.722890317440033,discriminator loss: 0.690121591091156\n",
            "Epoch 360: Step 6129: Generator loss: 0.7224010229110718,discriminator loss: 0.6906505823135376\n",
            "Epoch 360: Step 6130: Generator loss: 0.7233477830886841,discriminator loss: 0.6903644800186157\n",
            "Epoch 360: Step 6131: Generator loss: 0.7229283452033997,discriminator loss: 0.6899923086166382\n",
            "Epoch 360: Step 6132: Generator loss: 0.7227541208267212,discriminator loss: 0.6902694702148438\n",
            "Epoch 360: Step 6133: Generator loss: 0.7223436832427979,discriminator loss: 0.6904406547546387\n",
            "Epoch 360: Step 6134: Generator loss: 0.7227163910865784,discriminator loss: 0.6899234652519226\n",
            "Epoch 360: Step 6135: Generator loss: 0.7227113246917725,discriminator loss: 0.6898847818374634\n",
            "Epoch 360: Step 6136: Generator loss: 0.7217921614646912,discriminator loss: 0.6897556781768799\n",
            "Epoch 360\n",
            "accuracy:  0.7933304080737167\n",
            "Epoch 370: Step 6290: Generator loss: 0.6756930351257324,discriminator loss: 0.6912376880645752\n",
            "Epoch 370: Step 6291: Generator loss: 0.675544023513794,discriminator loss: 0.691540002822876\n",
            "Epoch 370: Step 6292: Generator loss: 0.6756443381309509,discriminator loss: 0.6919407844543457\n",
            "Epoch 370: Step 6293: Generator loss: 0.6756308078765869,discriminator loss: 0.6914653778076172\n",
            "Epoch 370: Step 6294: Generator loss: 0.6758105754852295,discriminator loss: 0.6920562982559204\n",
            "Epoch 370: Step 6295: Generator loss: 0.6757347583770752,discriminator loss: 0.6914281845092773\n",
            "Epoch 370: Step 6296: Generator loss: 0.6759281754493713,discriminator loss: 0.6911537647247314\n",
            "Epoch 370: Step 6297: Generator loss: 0.6758389472961426,discriminator loss: 0.6910675168037415\n",
            "Epoch 370: Step 6298: Generator loss: 0.675872266292572,discriminator loss: 0.6909365057945251\n",
            "Epoch 370: Step 6299: Generator loss: 0.675987958908081,discriminator loss: 0.6913432478904724\n",
            "Epoch 370: Step 6300: Generator loss: 0.6760604381561279,discriminator loss: 0.6910707950592041\n",
            "Epoch 370: Step 6301: Generator loss: 0.6758779287338257,discriminator loss: 0.6909179091453552\n",
            "Epoch 370: Step 6302: Generator loss: 0.6759911775588989,discriminator loss: 0.6915444135665894\n",
            "Epoch 370: Step 6303: Generator loss: 0.6761792898178101,discriminator loss: 0.6916406154632568\n",
            "Epoch 370: Step 6304: Generator loss: 0.6763545274734497,discriminator loss: 0.6913889646530151\n",
            "Epoch 370: Step 6305: Generator loss: 0.6762161254882812,discriminator loss: 0.6906751990318298\n",
            "Epoch 370: Step 6306: Generator loss: 0.6766945123672485,discriminator loss: 0.6933532357215881\n",
            "Epoch 370\n",
            "accuracy:  0.7762756214566071\n",
            "Epoch 380: Step 6460: Generator loss: 0.7200844883918762,discriminator loss: 0.6911313533782959\n",
            "Epoch 380: Step 6461: Generator loss: 0.7201007604598999,discriminator loss: 0.6913506388664246\n",
            "Epoch 380: Step 6462: Generator loss: 0.7200291156768799,discriminator loss: 0.6911685466766357\n",
            "Epoch 380: Step 6463: Generator loss: 0.7203460931777954,discriminator loss: 0.6911461353302002\n",
            "Epoch 380: Step 6464: Generator loss: 0.7207658290863037,discriminator loss: 0.6910632252693176\n",
            "Epoch 380: Step 6465: Generator loss: 0.7206206321716309,discriminator loss: 0.6913717985153198\n",
            "Epoch 380: Step 6466: Generator loss: 0.7206395268440247,discriminator loss: 0.691462516784668\n",
            "Epoch 380: Step 6467: Generator loss: 0.7211166620254517,discriminator loss: 0.6909113526344299\n",
            "Epoch 380: Step 6468: Generator loss: 0.7209432125091553,discriminator loss: 0.6914828419685364\n",
            "Epoch 380: Step 6469: Generator loss: 0.7209444046020508,discriminator loss: 0.6915189027786255\n",
            "Epoch 380: Step 6470: Generator loss: 0.7208039164543152,discriminator loss: 0.6916860342025757\n",
            "Epoch 380: Step 6471: Generator loss: 0.7210645079612732,discriminator loss: 0.6912408471107483\n",
            "Epoch 380: Step 6472: Generator loss: 0.7204992771148682,discriminator loss: 0.6909012794494629\n",
            "Epoch 380: Step 6473: Generator loss: 0.7208060026168823,discriminator loss: 0.6918141841888428\n",
            "Epoch 380: Step 6474: Generator loss: 0.7209974527359009,discriminator loss: 0.6914882659912109\n",
            "Epoch 380: Step 6475: Generator loss: 0.7206588983535767,discriminator loss: 0.6912333369255066\n",
            "Epoch 380: Step 6476: Generator loss: 0.7208737730979919,discriminator loss: 0.6906707286834717\n",
            "Epoch 380\n",
            "accuracy:  0.7999999999999999\n",
            "Epoch 390: Step 6630: Generator loss: 0.6685498952865601,discriminator loss: 0.6927193999290466\n",
            "Epoch 390: Step 6631: Generator loss: 0.6689063310623169,discriminator loss: 0.6925765872001648\n",
            "Epoch 390: Step 6632: Generator loss: 0.6689454913139343,discriminator loss: 0.6927334070205688\n",
            "Epoch 390: Step 6633: Generator loss: 0.6691031455993652,discriminator loss: 0.6925680637359619\n",
            "Epoch 390: Step 6634: Generator loss: 0.6690424084663391,discriminator loss: 0.6925621628761292\n",
            "Epoch 390: Step 6635: Generator loss: 0.6689856648445129,discriminator loss: 0.6924416422843933\n",
            "Epoch 390: Step 6636: Generator loss: 0.6689718961715698,discriminator loss: 0.6926805377006531\n",
            "Epoch 390: Step 6637: Generator loss: 0.6691477298736572,discriminator loss: 0.6925448179244995\n",
            "Epoch 390: Step 6638: Generator loss: 0.669456958770752,discriminator loss: 0.6928825974464417\n",
            "Epoch 390: Step 6639: Generator loss: 0.6695370078086853,discriminator loss: 0.6921278238296509\n",
            "Epoch 390: Step 6640: Generator loss: 0.6696367263793945,discriminator loss: 0.6922801733016968\n",
            "Epoch 390: Step 6641: Generator loss: 0.6697069406509399,discriminator loss: 0.6928554177284241\n",
            "Epoch 390: Step 6642: Generator loss: 0.669761061668396,discriminator loss: 0.6923990249633789\n",
            "Epoch 390: Step 6643: Generator loss: 0.67024827003479,discriminator loss: 0.6924405097961426\n",
            "Epoch 390: Step 6644: Generator loss: 0.6703022122383118,discriminator loss: 0.69163978099823\n",
            "Epoch 390: Step 6645: Generator loss: 0.6702880859375,discriminator loss: 0.6919583082199097\n",
            "Epoch 390: Step 6646: Generator loss: 0.6710370779037476,discriminator loss: 0.69134521484375\n",
            "Epoch 390\n",
            "accuracy:  0.7828054298642534\n",
            "Epoch 400: Step 6800: Generator loss: 0.7087985277175903,discriminator loss: 0.6943744421005249\n",
            "Epoch 400: Step 6801: Generator loss: 0.7090616226196289,discriminator loss: 0.6946722269058228\n",
            "Epoch 400: Step 6802: Generator loss: 0.7092849612236023,discriminator loss: 0.6943626999855042\n",
            "Epoch 400: Step 6803: Generator loss: 0.7093589901924133,discriminator loss: 0.6943919658660889\n",
            "Epoch 400: Step 6804: Generator loss: 0.7096105217933655,discriminator loss: 0.694317102432251\n",
            "Epoch 400: Step 6805: Generator loss: 0.7100101113319397,discriminator loss: 0.694068431854248\n",
            "Epoch 400: Step 6806: Generator loss: 0.7100609540939331,discriminator loss: 0.6939802169799805\n",
            "Epoch 400: Step 6807: Generator loss: 0.7103368043899536,discriminator loss: 0.694022536277771\n",
            "Epoch 400: Step 6808: Generator loss: 0.7104619741439819,discriminator loss: 0.6940257549285889\n",
            "Epoch 400: Step 6809: Generator loss: 0.7106620669364929,discriminator loss: 0.6937379240989685\n",
            "Epoch 400: Step 6810: Generator loss: 0.7109495997428894,discriminator loss: 0.6938318014144897\n",
            "Epoch 400: Step 6811: Generator loss: 0.7110522985458374,discriminator loss: 0.69387286901474\n",
            "Epoch 400: Step 6812: Generator loss: 0.7111438512802124,discriminator loss: 0.6938883066177368\n",
            "Epoch 400: Step 6813: Generator loss: 0.7113295197486877,discriminator loss: 0.694096565246582\n",
            "Epoch 400: Step 6814: Generator loss: 0.7115298509597778,discriminator loss: 0.6939756870269775\n",
            "Epoch 400: Step 6815: Generator loss: 0.711617112159729,discriminator loss: 0.6935038566589355\n",
            "Epoch 400: Step 6816: Generator loss: 0.7116152048110962,discriminator loss: 0.6938073039054871\n",
            "Epoch 400\n",
            "accuracy:  0.7671497584541064\n",
            "Epoch 410: Step 6970: Generator loss: 0.6862186193466187,discriminator loss: 0.6944217085838318\n",
            "Epoch 410: Step 6971: Generator loss: 0.6862381100654602,discriminator loss: 0.6945158243179321\n",
            "Epoch 410: Step 6972: Generator loss: 0.6862204670906067,discriminator loss: 0.6944756507873535\n",
            "Epoch 410: Step 6973: Generator loss: 0.6862585544586182,discriminator loss: 0.6944818496704102\n",
            "Epoch 410: Step 6974: Generator loss: 0.6863248348236084,discriminator loss: 0.6944519281387329\n",
            "Epoch 410: Step 6975: Generator loss: 0.6863096952438354,discriminator loss: 0.6945080161094666\n",
            "Epoch 410: Step 6976: Generator loss: 0.6862879395484924,discriminator loss: 0.6942205429077148\n",
            "Epoch 410: Step 6977: Generator loss: 0.6862326860427856,discriminator loss: 0.6940231323242188\n",
            "Epoch 410: Step 6978: Generator loss: 0.6862009763717651,discriminator loss: 0.6942909359931946\n",
            "Epoch 410: Step 6979: Generator loss: 0.6862346529960632,discriminator loss: 0.694266140460968\n",
            "Epoch 410: Step 6980: Generator loss: 0.6861904859542847,discriminator loss: 0.6941711902618408\n",
            "Epoch 410: Step 6981: Generator loss: 0.6861293911933899,discriminator loss: 0.6941392421722412\n",
            "Epoch 410: Step 6982: Generator loss: 0.6861703991889954,discriminator loss: 0.6939103007316589\n",
            "Epoch 410: Step 6983: Generator loss: 0.6862329840660095,discriminator loss: 0.6941766738891602\n",
            "Epoch 410: Step 6984: Generator loss: 0.6861755847930908,discriminator loss: 0.6940289735794067\n",
            "Epoch 410: Step 6985: Generator loss: 0.6861366033554077,discriminator loss: 0.6938412189483643\n",
            "Epoch 410: Step 6986: Generator loss: 0.6862642168998718,discriminator loss: 0.6944001913070679\n",
            "Epoch 410\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcI_TkxOpDwv"
      },
      "source": [
        "for k in master_slave_test_acc:\n",
        "  if(k%100==0):\n",
        "    print(\"epoch: \",k,master_slave_test_acc[k])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-bfXBMiiTKG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}